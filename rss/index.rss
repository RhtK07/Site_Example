<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Sebastian Ruder</title><description>I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</description><link>http://ruder.io/</link><generator>Ghost 0.7</generator><lastBuildDate>Tue, 02 Oct 2018 10:06:05 GMT</lastBuildDate><atom:link href="http://ruder.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>A Review of the Neural History of Natural Language Processing</title><description>This blog post discusses the eight biggest milestones in the last ~15 years of Natural Language Processing.</description><link>http://ruder.io/a-review-of-the-recent-history-of-nlp/</link><guid isPermaLink="false">e29edc89-6069-459e-8092-3371711da5c1</guid><category>natural language processing</category><category>nlp</category><category>deep learning</category><category>word embeddings</category><category>multi-task learning</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 01 Oct 2018 12:50:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/10/review_recent_history_image.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2018/10/review_recent_history_image.png" alt="A Review of the Neural History of Natural Language Processing"&gt;&lt;p&gt;&lt;link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot-number.min.css"&gt;  &lt;/p&gt;

&lt;script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot.min.js"&gt;&lt;/script&gt;  
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"&gt;&lt;/script&gt;  
&lt;script type="text/javascript"&gt;  
    $.bigfoot({useFootnoteOnlyOnce: false});
&lt;/script&gt;  

&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is the first blog post in a two-part series. The series expands on the Frontiers of Natural Language Processing session organized by &lt;a href="http://www.kamperh.com/"&gt;Herman Kamper&lt;/a&gt; and me at the &lt;a href="http://www.deeplearningindaba.com/"&gt;Deep Learning Indaba 2018&lt;/a&gt;. Slides of the entire session can be found &lt;a href="https://www.slideshare.net/SebastianRuder/frontiers-of-natural-language-processing"&gt;here&lt;/a&gt;. This post will discuss major recent advances in NLP focusing on neural network-based methods. The second post will discuss open problems in NLP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt; &amp;nbsp; This post tries to condense ~15 years' worth of work into eight milestones that are the most relevant today and thus omits many relevant and important developments. In particular, it is heavily skewed towards current neural approaches, which may give the false impression that no other methods were influential during this period. More importantly, many of the neural network models presented in this post build on non-neural milestones of the same era. In the &lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#nonneuralmilestones"&gt;final section of this post&lt;/a&gt;, we highlight such influential work that laid the foundations for later methods.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2001neurallanguagemodels"&gt;2001 - Neural language models&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#20018multitasklearning"&gt;2008 - Multi-task learning&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2013wordembeddings"&gt;2013 - Word embeddings&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2013neuralnetworksfornlp"&gt;2013 - Neural networks for NLP&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2014sequencetosequencemodels"&gt;2014 - Sequence-to-sequence models&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2015memorybasednetworks"&gt;2015 - Memory-based networks&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#2018pretrainedlanguagemodels"&gt;2018 - Pretrained language models&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#othermilestones"&gt;Other milestones&lt;/a&gt;&lt;/li&gt;  
&lt;li&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#nonneuralmilestones"&gt;Non-neural milestones&lt;/a&gt;&lt;/li&gt;  
&lt;/ul&gt;

&lt;h1 id="2001neurallanguagemodels"&gt;2001 - Neural language models&lt;/h1&gt;

&lt;p&gt;Language modelling is the task of predicting the next word in a text given the previous words. It is probably the simplest language processing task with concrete practical applications such as &lt;a href="https://en.wikipedia.org/wiki/SwiftKey"&gt;intelligent keyboards&lt;/a&gt; and email response suggestion (Kannan et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn2"&gt;&lt;/a&gt;. Unsurprisingly, language modelling has a rich history. Classic approaches are based on &lt;a href="https://en.wikipedia.org/wiki/N-gram"&gt;n-grams&lt;/a&gt; and employ smoothing to deal with unseen n-grams (Kneser &amp;amp; Ney, 1995)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn1"&gt;&lt;/a&gt;. &lt;br&gt;
The first neural language model, a feed-forward neural network was proposed in 2001 by Bengio et al.&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn3"&gt;&lt;/a&gt;, shown in Figure 1 below.  &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/lm_bengio_2003.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 1: A feed-forward neural network language model (Bengio et al., 2001; 2003)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;This model takes as input vector representations of the \(n\) previous words, which are looked up in a table \(C\). Nowadays, such vectors are known as word embeddings. These word embeddings are concatenated and fed into a hidden layer, whose output is then provided to a softmax layer. For more information about the model, have a look at &lt;a href="http://ruder.io/word-embeddings-1/index.html#classicneurallanguagemodel"&gt;this post&lt;/a&gt;. &lt;br&gt;
More recently, feed-forward neural networks have been replaced with recurrent neural networks (RNNs; Mikolov et al., 2010)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn4"&gt;&lt;/a&gt; and long short-term memory networks (LSTMs; Graves, 2013)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn5"&gt;&lt;/a&gt; for language modelling. Many new language models that extend the classic LSTM have been proposed in recent years (have a look at &lt;a href="http://nlpprogress.com/language_modeling.html"&gt;this page&lt;/a&gt; for an overview). Despite these developments, the classic LSTM remains a strong baseline (Melis et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn6"&gt;&lt;/a&gt;. Even Bengio et al.'s classic feed-forward neural network is in some settings competitive with more sophisticated models as these typically only learn to consider the most recent words (Daniluk et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn7"&gt;&lt;/a&gt;. Understanding better what information such language models capture consequently is an active research area (Kuncoro et al., 2018; Blevins et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn87"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn88"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Language modelling is typically the training ground of choice when applying RNNs and has succeeded at capturing the imagination, with many getting their first exposure via &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;Andrej's blog post&lt;/a&gt;. Language modelling is a form of unsupervised learning, which Yann LeCun also calls predictive learning and cites as a prerequisite to acquiring common sense (see &lt;a href="http://ruder.io/highlights-nips-2016/#generalartificialintelligence"&gt;here&lt;/a&gt; for his Cake slide from NIPS 2016). Probably the most remarkable aspect about language modelling is that despite its simplicity, it is core to many of the later advances discussed in this post:&lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt;Word embeddings: The objective of word2vec is a simplification of language modelling.&lt;/li&gt;  
&lt;li&gt;Sequence-to-sequence models: Such models generate an output sequence by predicting one word at a time.&lt;/li&gt;  
&lt;li&gt;Pretrained language models: These methods use representations from language models for transfer learning.&lt;/li&gt;  
&lt;/ul&gt;

&lt;p&gt;This conversely means that many of the most important recent advances in NLP reduce to a form of language modelling. In order to do "real" natural language understanding, just learning from the raw form of text likely will not be enough and we will need new methods and models.&lt;/p&gt;

&lt;h1 id="2008multitasklearning"&gt;2008 - Multi-task learning&lt;/h1&gt;

&lt;p&gt;Multi-task learning is a general method for sharing parameters between models that are trained on multiple tasks. In neural networks, this can be done easily by tying the weights of different layers. The idea of multi-task learning was first proposed in 1993 by Rich Caruana&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn8"&gt;&lt;/a&gt; and was applied to road-following and pneumonia prediction (Caruana, 1998)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn10"&gt;&lt;/a&gt;. Intuitively, multi-task learning encourages the models to learn representations that are useful for many tasks. This is particularly useful for learning general, low-level representations, to focus a model's attention or in settings with limited amounts of training data. For a more comprehensive overview of multi-task learning, have a look at &lt;a href="http://ruder.io/multi-task/"&gt;this post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Multi-task learning was first applied to neural networks for NLP in 2008 by Collobert and Weston&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn9"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn11"&gt;&lt;/a&gt;. In their model, the look-up tables (or word embedding matrices) are shared between two models trained on different tasks, as depicted in Figure 2 below.  &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/collobert_icml2008.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 2: Sharing of word embedding matrices (Collobert &amp; Weston, 2008; Collobert et al., 2011)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;Sharing the word embeddings enables the models to collaborate and share general low-level information in the word embedding matrix, which typically makes up the largest number of parameters in a model. The 2008 paper by Collobert and Weston proved influential beyond its use of multi-task learning. It spearheaded ideas such as pretraining word embeddings and using convolutional neural networks (CNNs) for text that have only been widely adopted in the last years. It won the &lt;a href="https://research.fb.com/facebook-researchers-win-test-of-time-award-at-icml-2018/"&gt;test-of-time award at ICML 2018&lt;/a&gt; (see the test-of-time award talk contextualizing the paper &lt;a href="https://www.facebook.com/722677142/posts/10155393881507143/"&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Multi-task learning is now used across a wide range of NLP tasks and leveraging existing or "artificial" tasks has become a useful tool in the NLP repertoire. For an overview of different auxiliary tasks, have a look at &lt;a href="http://ruder.io/multi-task-learning-nlp/"&gt;this post&lt;/a&gt;. While the sharing of parameters is typically predefined, different sharing patterns can also be learned during the optimization process (Ruder et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn12"&gt;&lt;/a&gt;. As models are being increasingly evaluated on multiple tasks to gauge their generalization ability, multi-task learning is gaining in importance and dedicated benchmarks for multi-task learning have been proposed recently (Wang et al., 2018; McCann et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn89"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn90"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="2013wordembeddings"&gt;2013 - Word embeddings&lt;/h1&gt;

&lt;p&gt;Sparse vector representations of text, the so-called &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model"&gt;bag-of-words model&lt;/a&gt; have a long history in NLP. Dense vector representations of words or word embeddings have been used as early as 2001 as we have seen above. The main innovation that was proposed in 2013 by Mikolov et al.&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn14"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn13"&gt;&lt;/a&gt; was to make the training of these word embeddings more efficient by removing the hidden layer and approximating the objective. While these changes were simple in nature, they enabled---together with the efficient word2vec implementation---large-scale training of word embeddings.&lt;/p&gt;

&lt;p&gt;Word2vec comes in two flavours that can be seen in Figure 3 below: continuous bag-of-words (CBOW) and skip-gram. They differ in their objective: one predicts the centre word based based on the surrounding words, while the other does the opposite.  &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/cbow_skipgram_mikolov_2013.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 3: Continuous bag-of-words and skip-gram architectures (Mikolov et al., 2013a; 2013b)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;While these embeddings are no different conceptually than the ones learned with a feed-forward neural network, training on a very large corpus enables them to capture certain relation between words such as gender, verb tense, and country-capital relations, which can be seen in Figure 4 below.  &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/word2vec_relations.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 4: Relations captured by word2vec (Mikolov et al., 2013a; 2013b)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;These relations and the meaning behind them sparked initial interest in word embeddings and many studies have investigated the origin of these linear relationships (Arora et al., 2016; Mimno &amp;amp; Thompson, 2017; Antoniak &amp;amp; Mimno, 2018; Wendlandt et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn15"&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn16"&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn17"&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn18"&gt;&lt;/a&gt;. What cemented word embeddings as a mainstay in current NLP, however, was that using pretrained embeddings as initialization was shown to improve performance across a wide range of downstream tasks&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn19"&gt;&lt;/a&gt;.&lt;/a&gt;&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While the relations word2vec captured had an intuitive and almost magical quality to them, later studies showed that there is nothing inherently special about word2vec: Word embeddings can also be learned via matrix factorization (Pennington et al, 2014; Levy &amp;amp; Goldberg, 2014)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn20"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn21"&gt;&lt;/a&gt; and with proper tuning, classic matrix factorization approaches like SVD and LSA achieve similar results (Levy et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn22"&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Since then, a lot of work has gone into exploring different facets of word embeddings (as indicated by the &lt;a href="https://scholar.google.de/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=distributed+representations+of+words+and+phrases&amp;amp;btnG="&gt;staggering number of citations of the original paper&lt;/a&gt;). Have a look at &lt;a href="http://ruder.io/word-embeddings-2017/"&gt;this post&lt;/a&gt; for some trends and future directions. Despite many developments, word2vec is still a popular choice and widely used today. Word2vec's reach has even extended beyond the word level: skip-gram with negative sampling, a convenient objective for learning embeddings based on local context, has been applied to learn representations for sentences (Mikolov &amp;amp; Le, 2014; Kiros et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn23"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn24"&gt;&lt;/a&gt;---and even going beyond NLP---to networks (Grover &amp;amp; Leskovec, 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn25"&gt;&lt;/a&gt; and biological sequences (Asgari &amp;amp; Mofrad, 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn26"&gt;&lt;/a&gt;, among others.&lt;/p&gt;

&lt;p&gt;One particularly exciting direction is to project word embeddings of different languages into the same space to enable (zero-shot) cross-lingual transfer. It is becoming increasingly possible to learn a good projection in a completely unsupervised way (at least for similar languages) (Conneau et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn27"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn28"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn29"&gt;&lt;/a&gt;, which opens applications for low-resource languages and unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn91"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn92"&gt;&lt;/a&gt;. Have a look at (Ruder et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn30"&gt;&lt;/a&gt; for an overview.&lt;/p&gt;

&lt;h1 id="2013neuralnetworksfornlp"&gt;2013 - Neural networks for NLP&lt;/h1&gt;

&lt;p&gt;2013 and 2014 marked the time when neural network models started to get adopted in NLP. Three main types of neural networks became the most widely used: recurrent neural networks, convolutional neural networks, and recursive neural networks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recurrent neural networks&lt;/strong&gt; &amp;nbsp; Recurrent neural networks (RNNs) are an obvious choice to deal with the dynamic input sequences ubiquitous in NLP. Vanilla RNNs (Elman, 1990)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn31"&gt;&lt;/a&gt; were quickly replaced with the classic long-short term memory networks (Hochreiter &amp;amp; Schmidhuber, 1997)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn32"&gt;&lt;/a&gt;, which proved more resilient to the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing and exploding gradient problem&lt;/a&gt;. Before 2013, RNNs were still thought to be difficult to train; &lt;a href="https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf"&gt;Ilya Sutskever's PhD thesis&lt;/a&gt; was a key example on the way to changing this reputation. A visualization of an LSTM cell can be seen in Figure 5 below. A bidirectional LSTM (Graves et al., 2013)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn93"&gt;&lt;/a&gt; is typically used to deal with both left and right context.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/lstm_colah_2015.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 5: An LSTM network (Source: &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Chris Olah&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Convolutional neural networks&lt;/strong&gt; &amp;nbsp; With convolutional neural networks (CNNs) being widely used in computer vision, they also started to get applied to language (Kalchbrenner et al., 2014; Kim et al., 2014)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn33"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn34"&gt;&lt;/a&gt;. A convolutional neural network for text only operates in two dimensions, with the filters only needing to be moved along the temporal dimension. Figure 6 below shows a typical CNN as used in NLP. &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/kim_emnlp2014.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 6: A convolutional neural network for text (Kim, 2014)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;An advantage of convolutional neural networks is that they are more parallelizable than RNNs, as the state at every timestep only depends on the local context (via the convolution operation) rather than all past states as in the RNN. CNNs can be extended with wider receptive fields using dilated convolutions to capture a wider context (Kalchbrenner et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn35"&gt;&lt;/a&gt;. CNNs and LSTMs can also be combined and stacked (Wang et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn36"&gt;&lt;/a&gt; and convolutions can be used to speed up an LSTM (Bradbury et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn37"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recursive neural networks&lt;/strong&gt; &amp;nbsp; RNNs and CNNs both treat the language as a sequence. From a linguistic perspective, however, language is &lt;a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy"&gt;inherently hierarchical&lt;/a&gt;: Words are composed into higher-order phrases and clauses, which can themselves be recursively combined according to a set of production rules. The linguistically inspired idea of treating sentences as trees rather than as a sequence gives rise to recursive neural networks (Socher et al., 2013)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn38"&gt;&lt;/a&gt;, which can be seen in Figure 7 below. &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/socher_emnlp2013.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 7: A recursive neural network (Socher et al., 2013)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Recursive neural networks build the representation of a sequence from the bottom up in contrast to RNNs who process the sentence left-to-right or right-to-left. At every node of the tree, a new representation is computed by composing the representations of the child nodes. As a tree can also be seen as imposing a different processing order on an RNN, LSTMs have naturally been extended to trees (Tai et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn39"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Not only RNNs and LSTMs can be extended to work with hierarchical structures. Word embeddings can be learned based not only on local but on grammatical context (Levy &amp;amp; Goldberg, 2014)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn40"&gt;&lt;/a&gt;; language models can generate words based on a syntactic stack (Dyer et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn41"&gt;&lt;/a&gt;; and graph-convolutional neural networks can operate over a tree (Bastings et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn42"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="2014sequencetosequencemodels"&gt;2014 - Sequence-to-sequence models&lt;/h1&gt;

&lt;p&gt;In 2014, Sutskever et al.&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn63"&gt;&lt;/a&gt; proposed sequence-to-sequence learning, a general framework for mapping one sequence to another one using a neural network. In the framework, an encoder neural network processes a sentence symbol by symbol and compresses it into a vector representation; a decoder neural network then predicts the output symbol by symbol based on the encoder state, taking as input at every step the previously predicted symbol as can be seen in Figure 8 below.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/sutskever_nips2014.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 8: A sequence-to-sequence model (Sutskever et al., 2014)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Machine translation turned out to be the killer application of this framework. In 2016, Google announced that it was starting to replace its monolithic phrase-based MT models with neural MT models (Wu et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn48"&gt;&lt;/a&gt;. &lt;a href="https://www.oreilly.com/ideas/what-machine-learning-means-for-software-development"&gt;According to Jeff Dean&lt;/a&gt;, this meant replacing 500,000 lines of phrase-based MT code with a 500-line neural network model.&lt;/p&gt;

&lt;p&gt;This framework due to its flexibility is now the go-to framework for natural language generation tasks, with different models taking on the role of the encoder and the decoder. Importantly, the decoder model can not only be conditioned on a sequence, but on arbitrary representations. This enables for instance generating a caption based on an image (Vinyals et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn43"&gt;&lt;/a&gt; (as can be seen in Figure 9 below), text based on a table (Lebret et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn44"&gt;&lt;/a&gt;, and a description based on source code changes (Loyola et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn45"&gt;&lt;/a&gt;, among many other applications.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/vinyals_cvpr2015.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 9: Generating a caption based on an image (Vinyals et al., 2015)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Sequence-to-sequence learning can even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as can be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn46"&gt;&lt;/a&gt;, and named entity recognition (Gillick et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn47"&gt;&lt;/a&gt;, among others.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/vinyals_nips2015.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 10: Linearizing a constituency parse tree (Vinyals et al., 2015)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Encoders for sequences and decoders are typically based on RNNs but other model types can be used. New architectures mainly emerge from work in MT, which acts as a Petri dish for sequence-to-sequence architectures. Recent models are deep LSTMs (Wu et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn48"&gt;&lt;/a&gt;, convolutional encoders (Kalchbrenner et al., 2016; Gehring et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn35"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn49"&gt;&lt;/a&gt;, the Transformer (Vaswani et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn50"&gt;&lt;/a&gt;, which will be discussed in the next section, and a combination of an LSTM and a Transformer (Chen et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn51"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="2015attention"&gt;2015 - Attention&lt;/h1&gt;

&lt;p&gt;Attention (Bahdanau et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn52"&gt;&lt;/a&gt; is one of the core innovations in neural MT (NMT) and the key idea that enabled NMT models to outperform classic phrase-based MT systems. The main bottleneck of sequence-to-sequence learning is that it requires to compress the entire content of the source sequence into a fixed-size vector. Attention alleviates this by allowing the decoder to look back at the source sequence hidden states, which are then provided as a weighted average as additional input to the decoder as can be seen in Figure 11 below.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/attention_bahdanau_iclr2015.png" width="250" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 11: Attention (Bahdanau et al., 2015)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Different forms of attention are available (Luong et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn53"&gt;&lt;/a&gt;. Have a look &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#attention"&gt;here&lt;/a&gt; for a brief overview. Attention is widely applicable and potentially useful for any task that requires making decisions based on certain parts of the input. It has been applied to consituency parsing (Vinyals et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn46"&gt;&lt;/a&gt;, reading comprehension (Hermann et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn54"&gt;&lt;/a&gt;, and one-shot learning (Vinyals et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn56"&gt;&lt;/a&gt;, among many others. The input does not even need to be a sequence, but can consist of other representations as in the case of image captioning (Xu et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn55"&gt;&lt;/a&gt;, which can be seen in Figure 12 below. A useful side-effect of attention is that it provides a rare---if only superficial---glimpse into the inner workings of the model by inspecting which parts of the input are relevant for a particular output based on the attention weights.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/xu_icml2015.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 12: Visual attention in an image captioning model indicating what the model is attending to when generating the word "frisbee". (Xu et al., 2015)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Attention is also not restricted to just looking at the input sequence; self-attention can be used to look at the surrounding words in a sentence or document to obtain more contextually sensitive word representations. Multiple layers of self-attention are at the core of the Transformer architecture (Vaswani et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn50"&gt;&lt;/a&gt;, the current state-of-the-art model for NMT.&lt;/p&gt;

&lt;h1 id="2015memorybasednetworks"&gt;2015 - Memory-based networks&lt;/h1&gt;

&lt;p&gt;Attention can be seen as a form of fuzzy memory where the memory consists of the past hidden states of the model, with the model choosing what to retrieve from memory. For a more detailed overview of attention and its connection to memory, have a look at &lt;a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"&gt;this post&lt;/a&gt;. Many models with a more explicit memory have been proposed. They come in different variants such as Neural Turing Machines (Graves et al., 2014)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn57"&gt;&lt;/a&gt;, Memory Networks (Weston et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn58"&gt;&lt;/a&gt; and End-to-end Memory Networks (Sukhbaatar et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn59"&gt;&lt;/a&gt;, Dynamic Memory Networks (Kumar et al., 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn60"&gt;&lt;/a&gt;, the Neural Differentiable Computer (Graves et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn61"&gt;&lt;/a&gt;, and the Recurrent Entity Network (Henaff et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn62"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Memory is often accessed based on similarity to the current state similar to attention and can typically be written to and read from. Models differ in how they implement and leverage the memory. For instance, End-to-end Memory Networks process the input multiple times and update the memory to enable multiple steps of inference. Neural Turing Machines also have a location-based addressing, which allows them to learn simple computer programs like sorting. Memory-based models are typically applied to tasks, where retaining information over longer time spans should be useful such as language modelling and reading comprehension. The concept of a memory is very versatile: A knowledge base or table can function as a memory, while a memory can also be populated based on the entire input or particular parts of it.&lt;/p&gt;

&lt;h1 id="2018pretrainedlanguagemodels"&gt;2018 - Pretrained language models&lt;/h1&gt;

&lt;p&gt;Pretrained word embeddings are context-agnostic and only used to initialize the first layer in our models. In recent months, a range of supervised tasks has been used to pretrain neural networks (Conneau et al., 2017; McCann et al., 2017; Subramanian et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn65"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn64"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn66"&gt;&lt;/a&gt;. In contrast, language models only require unlabelled text; training can thus scale to billions of tokens, new domains, and new languages. Pretrained language models were first proposed in 2015 (Dai &amp;amp; Le, 2015)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn67"&gt;&lt;/a&gt;; only recently were they shown to be beneficial across a diverse range of tasks. Language model embeddings can be used as features in a target model (Peters et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn68"&gt;&lt;/a&gt; or a language model can be fine-tuned on target task data (Ramachandran et al., 2017; Howard &amp;amp; Ruder, 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn94"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn69"&gt;&lt;/a&gt;. Adding language model embeddings gives a large improvement over the state-of-the-art across many different tasks as can be seen in Figure 13 below.&lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/09/elmo_peters_2018.png" width="450" alt="A Review of the Neural History of Natural Language Processing"&gt;  
    &lt;figcaption&gt;Figure 13: Improvements with language model embeddings over the state-of-the-art (Peters et al., 2018)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Pretrained language models have been shown enable learning with significantly less data. As language models only require unlabelled data, they are particularly beneficial for low-resource languages where labelled data is scarce. For more information about the potential of pretrained language models, refer to &lt;a href="https://thegradient.pub/nlp-imagenet/"&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="othermilestones"&gt;Other milestones&lt;/h1&gt;

&lt;p&gt;Some other developments are less pervasive than the ones mentioned above, but still have wide-ranging impact.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Character-based representations&lt;/strong&gt; &amp;nbsp; Using a CNN or an LSTM over characters to obtain a character-based word representation is now fairly common, particularly for morphologically rich languages and tasks where morphological information is important or that have many unknown words. As far as I am aware, character-based representations were first used for sequence labelling (Lample et al., 2016; Plank et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn70"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn71"&gt;&lt;/a&gt;. Character-based representations alleviate the need of having to deal with a fixed vocabulary at increased computational cost and enable applications such as fully character-based NMT (Ling et al., 2016; Lee et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn72"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn73"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adversarial learning&lt;/strong&gt; &amp;nbsp; Adversarial methods have taken the field of ML by storm and have also been used in different forms in NLP. Adversarial examples are becoming increasingly widely used not only as a tool to probe models and understand their failure cases, but also to make them more robust (Jia &amp;amp; Liang, 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn74"&gt;&lt;/a&gt;. (Virtual) adversarial training, that is, worst-case perturbations (Miyato et al., 2017; Yasunaga et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn75"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn76"&gt;&lt;/a&gt; and domain-adversarial losses (Ganin et al., 2016; Kim et al., 2017)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn77"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn78"&gt;&lt;/a&gt; are useful forms of regularization that can equally make models more robust. Generative adversarial networks (GANs) are not yet too effective for natural language generation (Semeniuta et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn79"&gt;&lt;/a&gt;, but are useful for instance when matching distributions (Conneau et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn27"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt; &amp;nbsp; Reinforcement learning has been shown to be useful for tasks with a temporal dependency  such as selecting data during training (Fang et al., 2017; Wu et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn80"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn81"&gt;&lt;/a&gt; and modelling dialogue (Liu et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn86"&gt;&lt;/a&gt;. RL is also effective for directly optimizing a non-differentiable end metric such as ROUGE or BLEU instead of optimizing a surrogate loss such as cross-entropy in summarization (Paulus et al, 2018; Celikyilmaz et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn82"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn83"&gt;&lt;/a&gt; and machine translation (Ranzato et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn84"&gt;&lt;/a&gt;. Similarly, inverse reinforcement learning can be useful in settings where the reward is too complex to be specified such as visual storytelling (Wang et al., 2018)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn85"&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;h1 id="nonneuralmilestones"&gt;Non-neural milestones&lt;/h1&gt;

&lt;p&gt;In 1998 and over the following years, the &lt;a href="https://framenet.icsi.berkeley.edu/fndrupal/"&gt;FrameNet&lt;/a&gt; project was introduced (Baker et al., 1998)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn95"&gt;&lt;/a&gt;, which led to the task of &lt;a href="https://en.wikipedia.org/wiki/Semantic_role_labeling"&gt;semantic role labelling&lt;/a&gt;, a form of shallow semantic parsing that is still actively researched today. In the early 2000s, the shared tasks organized together with the Conference on Natural Language Learning (CoNLL) catalyzed research in core NLP tasks such as chunking (Tjong Kim Sang et al., 2000)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn96"&gt;&lt;/a&gt;, named entity recognition (Tjong Kim Sang et al., 2003)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn97"&gt;&lt;/a&gt;, and dependency parsing (Buchholz et al., 2006)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn98"&gt;&lt;/a&gt;, among others. Many of the CoNLL shared task datasets are still the standard for evaluation today.&lt;/p&gt;

&lt;p&gt;In 2001, conditional random fields (CRF; Lafferty et al., 2001)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn99"&gt;&lt;/a&gt;, one of the most influential classes of sequence labelling methods were introduced, which won the &lt;a href="https://www.ml.cmu.edu/news/news-archive/2011-2015/2011/june/icml-test-time-award-2011.html"&gt;Test-of-time award at ICML 2011&lt;/a&gt;. A CRF layer is a core part of current state-of-the-art models for sequence labelling problems with label interdependencies such as named entity recognition (Lample et al., 2016)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn70"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In 2002, the bilingual evaluation understudy (BLEU; Papineni et al., 2002)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn100"&gt;&lt;/a&gt; metric was proposed, which enabled MT systems to scale up and is still the standard metric for MT evaluation these days. In the same year, the structured preceptron (Collins, 2002)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn101"&gt;&lt;/a&gt; was introduced, which laid the foundation for work in structured perception. At the same conference, sentiment analysis, one of the most popular and widely studied NLP tasks, was introduced (Pang et al., 2002)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn102"&gt;&lt;/a&gt;. All three papers won the &lt;a href="https://naacl2018.wordpress.com/2018/03/22/test-of-time-award-papers/"&gt;Test-of-time award at NAACL 2018&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2003 saw the introduction of latent dirichlet allocation (LDA; Blei et al., 2003)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn103"&gt;&lt;/a&gt;, one of the most widely used techniques in machine learning, which is still the standard way to do topic modelling. In 2004, novel max-margin models were proposed that are better suited for capturing correlations in structured data than SVMs (Taskar et al., 2004a; 2004b)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn104"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn105"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In 2006, OntoNotes (Hovy et al., 2006)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn106"&gt;&lt;/a&gt;, a large multilingual corpus with multiple annotations and high interannotator agreement was introduced. OntoNotes has been used for the training and evaluation of a variety of tasks such as dependency parsing and coreference resolution. Milne and Witten (2008)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn107"&gt;&lt;/a&gt; described in 2008 how Wikipedia can be used to enrich machine learning methods. To this date, Wikipedia is one of the most useful resources for training ML methods, whether for entity linking and disambiguation, language modelling, as a knowledge base, or a variety of other tasks.&lt;/p&gt;

&lt;p&gt;In 2009, the idea of distant supervision (Mintz et al., 2009)&lt;a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/#fn108"&gt;&lt;/a&gt; was proposed. Distant supervision leverages information from heuristics or existing knowledge bases to generate noisy patterns that can be used to automatically extract examples from large corpora. Distant supervision has been used extensively and is a common technique in relation extraction, information extraction, and sentiment analysis, among other tasks.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Djamé Seddah, Daniel Khashabi, Shyam Upadhyay, Chris Dyer, and Michael Roth for providing pointers (see &lt;a href="https://twitter.com/seb_ruder/status/1046791863896887302"&gt;the Twitter thread&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;ol class="footnotes-list"&gt;  
&lt;li id="fn1" class="footnote-item"&gt;Kneser, R., &amp; Ney, H. (1995, May). Improved backing-off for m-gram language modeling. In icassp (Vol. 1, p. 181e4).&lt;/li&gt;  
&lt;li id="fn2" class="footnote-item"&gt;Kannan, A., Kurach, K., Ravi, S., Kaufmann, T., Tomkins, A., Miklos, B., ... &amp; Ramavajjala, V. (2016, August). Smart reply: Automated response suggestion for email. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 955-964). ACM.&lt;/li&gt;  
&lt;li id="fn3" class="footnote-item"&gt;Bengio, Y., Ducharme, R., &amp; Vincent, P. (2001). Proceedings of NIPS.&lt;/li&gt;  
&lt;li id="fn4" class="footnote-item"&gt;Mikolov, T., Karafiát, M., Burget, L., Černocký, J., &amp; Khudanpur, S. (2010). Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association.&lt;/li&gt;  
&lt;li id="fn5" class="footnote-item"&gt;Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.&lt;/li&gt;  
&lt;li id="fn6" class="footnote-item"&gt;Melis, G., Dyer, C., &amp; Blunsom, P. (2018). On the State of the Art of Evaluation in Neural Language Models. In Proceedings of ICLR 2018.&lt;/li&gt;  
&lt;li id="fn7" class="footnote-item"&gt;Daniluk, M., Rocktäschel, T., Weibl, J., &amp; Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. In Proceedings of ICLR 2017.&lt;/li&gt;  
&lt;li id="fn8" class="footnote-item"&gt;Caruana, R. (1993). Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning.&lt;/li&gt;  
&lt;li id="fn9" class="footnote-item"&gt;Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. In Proceedings of the 25th International Conference on Machine Learning (pp. 160–167).&lt;/li&gt;  
&lt;li id="fn10" class="footnote-item"&gt;Caruana, R. (1998). Multitask Learning. Autonomous Agents and Multi-Agent Systems, 27(1), 95–133.&lt;/li&gt;  
&lt;li id="fn11" class="footnote-item"&gt;Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537. Retrieved from &lt;a href="http://arxiv.org/abs/1103.0398"&gt;http://arxiv.org/abs/1103.0398&lt;/a&gt;.&lt;/li&gt;  
&lt;li id="fn12" class="footnote-item"&gt;Ruder, S., Bingel, J., Augenstein, I., &amp; Søgaard, A. (2017). Learning what to share between loosely related tasks. ArXiv Preprint ArXiv:1705.08142. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn13" class="footnote-item"&gt;Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems.&lt;/li&gt;  
&lt;li id="fn14" class="footnote-item"&gt;Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013).&lt;/li&gt;  
&lt;li id="fn15" class="footnote-item"&gt;Arora, S., Li, Y., Liang, Y., Ma, T., &amp; Risteski, A. (2016). A Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4, 385–399.&lt;/li&gt;  
&lt;li id="fn16" class="footnote-item"&gt;Mimno, D., &amp; Thompson, L. (2017). The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2863–2868).&lt;/li&gt;  
&lt;li id="fn17" class="footnote-item"&gt;Antoniak, M., &amp; Mimno, D. (2018). Evaluating the Stability of Embedding-based Word Similarities. Transactions of the Association for Computational Linguistics, 6, 107–119.&lt;/li&gt;  
&lt;li id="fn18" class="footnote-item"&gt;Wendlandt, L., Kummerfeld, J. K., &amp; Mihalcea, R. (2018). Factors Influencing the Surprising Instability of Word Embeddings. In Proceedings of NAACL-HLT 2018.&lt;/li&gt;  
&lt;li id="fn19" class="footnote-item"&gt;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from &lt;a href="http://arxiv.org/abs/1408.5882"&gt;http://arxiv.org/abs/1408.5882&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn20" class="footnote-item"&gt;Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543.&lt;/li&gt;  
&lt;li id="fn21" class="footnote-item"&gt;Levy, O., &amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from &lt;a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization"&gt;http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn22" class="footnote-item"&gt;Levy, O., Goldberg, Y., &amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from &lt;a href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570"&gt;https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn23" class="footnote-item"&gt;Le, Q. V., &amp; Mikolov, T. (2014). Distributed Representations of Sentences and Documents. International Conference on Machine Learning - ICML 2014, 32, 1188–1196. Retrieved from &lt;a href="http://arxiv.org/abs/1405.4053"&gt;http://arxiv.org/abs/1405.4053&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn24" class="footnote-item"&gt;Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp; Fidler, S. (2015). Skip-Thought Vectors. In Proceedings of NIPS 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1506.06726"&gt;http://arxiv.org/abs/1506.06726&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn25" class="footnote-item"&gt;Grover, A., &amp; Leskovec, J. (2016, August). node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 855-864). ACM.&lt;/li&gt;  
&lt;li id="fn26" class="footnote-item"&gt;Asgari, E., &amp; Mofrad, M. R. (2015). Continuous distributed representation of biological sequences for deep proteomics and genomics. PloS one, 10(11), e0141287.&lt;/li&gt;  
&lt;li id="fn27" class="footnote-item"&gt;Conneau, A., Lample, G., Ranzato, M., Denoyer, L., &amp; Jégou, H. (2018). Word Translation Without Parallel Data. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1710.04087"&gt;http://arxiv.org/abs/1710.04087&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn28" class="footnote-item"&gt;Artetxe, M., Labaka, G., &amp; Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.&lt;/li&gt;  
&lt;li id="fn29" class="footnote-item"&gt;Søgaard, A., Ruder, S., &amp; Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.&lt;/li&gt;  
&lt;li id="fn30" class="footnote-item"&gt;Ruder, S., Vulić, I., &amp; Søgaard, A. (2018). A Survey of Cross-lingual Word Embedding Models. To be published in Journal of Artificial Intelligence Research. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04902"&gt;http://arxiv.org/abs/1706.04902&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn31" class="footnote-item"&gt;Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2), 179-211.&lt;/li&gt;  
&lt;li id="fn32" class="footnote-item"&gt;Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.&lt;/li&gt;  
&lt;li id="fn33" class="footnote-item"&gt;Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (pp. 655–665). Retrieved from &lt;a href="http://arxiv.org/abs/1404.2188"&gt;http://arxiv.org/abs/1404.2188&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn34" class="footnote-item"&gt;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from &lt;a href="http://arxiv.org/abs/1408.5882"&gt;http://arxiv.org/abs/1408.5882&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn35" class="footnote-item"&gt;Kalchbrenner, N., Espeholt, L., Simonyan, K., Oord, A. van den, Graves, A., &amp; Kavukcuoglu, K. (2016). Neural Machine Translation in Linear Time. ArXiv Preprint ArXiv: Retrieved from &lt;a href="http://arxiv.org/abs/1610.10099"&gt;http://arxiv.org/abs/1610.10099&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn36" class="footnote-item"&gt;Wang, J., Yu, L., Lai, K. R., &amp; Zhang, X. (2016). Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), 225–230.&lt;/li&gt;  
&lt;li id="fn37" class="footnote-item"&gt;Bradbury, J., Merity, S., Xiong, C., &amp; Socher, R. (2017). Quasi-Recurrent Neural Networks. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01576"&gt;http://arxiv.org/abs/1611.01576&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn38" class="footnote-item"&gt;Socher, R., Perelygin, A., &amp; Wu, J. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–1642.&lt;/li&gt;  
&lt;li id="fn39" class="footnote-item"&gt;Tai, K. S., Socher, R., &amp; Manning, C. D. (2015). Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. Acl-2015, 1556–1566.&lt;/li&gt;  
&lt;li id="fn40" class="footnote-item"&gt;Levy, O., &amp; Goldberg, Y. (2014). Dependency-Based Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) (pp. 302–308). &lt;a href="https://doi.org/10.3115/v1/P14-2050"&gt;https://doi.org/10.3115/v1/P14-2050&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn41" class="footnote-item"&gt;Dyer, C., Kuncoro, A., Ballesteros, M., &amp; Smith, N. A. (2016). Recurrent Neural Network Grammars. In NAACL. Retrieved from &lt;a href="http://arxiv.org/abs/1602.07776"&gt;http://arxiv.org/abs/1602.07776&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn42" class="footnote-item"&gt;Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., &amp; Sima’an, K. (2017). Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.&lt;/li&gt;  
&lt;li id="fn43" class="footnote-item"&gt;Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).&lt;/li&gt;  
&lt;li id="fn44" class="footnote-item"&gt;Lebret, R., Grangier, D., &amp; Auli, M. (2016). Generating Text from Structured Data with Application to the Biography Domain. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1603.07771"&gt;http://arxiv.org/abs/1603.07771&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn45" class="footnote-item"&gt;Loyola, P., Marrese-Taylor, E., &amp; Matsuo, Y. (2017). A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes. In ACL 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1704.04856"&gt;http://arxiv.org/abs/1704.04856&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn46" class="footnote-item"&gt;Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., &amp; Hinton, G. (2015). Grammar as a Foreign Language. Advances in Neural Information Processing Systems.&lt;/li&gt;  
&lt;li id="fn47" class="footnote-item"&gt;Gillick, D., Brunk, C., Vinyals, O., &amp; Subramanya, A. (2016). Multilingual Language Processing From Bytes. In NAACL (pp. 1296–1306). Retrieved from &lt;a href="http://arxiv.org/abs/1512.00103"&gt;http://arxiv.org/abs/1512.00103&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn48" class="footnote-item"&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. ArXiv Preprint ArXiv:1609.08144.&lt;/li&gt;  
&lt;li id="fn49" class="footnote-item"&gt;Gehring, J., Auli, M., Grangier, D., Yarats, D., &amp; Dauphin, Y. N. (2017). Convolutional Sequence to Sequence Learning. ArXiv Preprint ArXiv:1705.03122. Retrieved from &lt;a href="http://arxiv.org/abs/1705.03122"&gt;http://arxiv.org/abs/1705.03122&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn50" class="footnote-item"&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems.&lt;/li&gt;  
&lt;li id="fn51" class="footnote-item"&gt;Chen, M. X., Foster, G., &amp; Parmar, N. (2018). The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. In Proceedings of ACL 2018.&lt;/li&gt;  
&lt;li id="fn52" class="footnote-item"&gt;Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR 2015.&lt;/li&gt;  
&lt;li id="fn53" class="footnote-item"&gt;Luong, M.-T., Pham, H., &amp; Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. In Proceedings of EMNLP 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1508.04025"&gt;http://arxiv.org/abs/1508.04025&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn54" class="footnote-item"&gt;Hermann, K. M., Kočiský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp; Blunsom, P. (2015). Teaching Machines to Read and Comprehend. Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1506.03340v1"&gt;http://arxiv.org/abs/1506.03340v1&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn55" class="footnote-item"&gt;Xu, K., Courville, A., Zemel, R. S., &amp; Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In Proceedings of ICML 2015.&lt;/li&gt;  
&lt;li id="fn56" class="footnote-item"&gt;Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1606.04080"&gt;http://arxiv.org/abs/1606.04080&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn57" class="footnote-item"&gt;Graves, A., Wayne, G., &amp; Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.&lt;/li&gt;  
&lt;li id="fn58" class="footnote-item"&gt;Weston, J., Chopra, S., &amp; Bordes, A. (2015). Memory Networks. In Proceedings of ICLR 2015.&lt;/li&gt;  
&lt;li id="fn59" class="footnote-item"&gt;Sukhbaatar, S., Szlam, A., Weston, J., &amp; Fergus, R. (2015). End-To-End Memory Networks. In Proceedings of NIPS 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1503.08895"&gt;http://arxiv.org/abs/1503.08895&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn60" class="footnote-item"&gt;Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., ... &amp; Socher, R. (2016, June). Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning (pp. 1378-1387).&lt;/li&gt;  
&lt;li id="fn61" class="footnote-item"&gt;Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., … Hassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory. Nature.&lt;/li&gt;  
&lt;li id="fn62" class="footnote-item"&gt;Henaff, M., Weston, J., Szlam, A., Bordes, A., &amp; LeCun, Y. (2017). Tracking the World State with Recurrent Entity Networks. In Proceedings of ICLR 2017.&lt;/li&gt;  
&lt;li id="fn63" class="footnote-item"&gt;Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems.&lt;/li&gt;  
&lt;li id="fn64" class="footnote-item"&gt;McCann, B., Bradbury, J., Xiong, C., &amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. In Advances in Neural Information Processing Systems.&lt;/li&gt;  
&lt;li id="fn65" class="footnote-item"&gt;Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.&lt;/li&gt;  
&lt;li id="fn66" class="footnote-item"&gt;Subramanian, S., Trischler, A., Bengio, Y., &amp; Pal, C. J. (2018). Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Proceedings of ICLR 2018.&lt;/li&gt;  
&lt;li id="fn67" class="footnote-item"&gt;Dai, A. M., &amp; Le, Q. V. (2015). Semi-supervised Sequence Learning. Advances in Neural Information Processing Systems (NIPS ’15). Retrieved from &lt;a href="http://arxiv.org/abs/1511.01432"&gt;http://arxiv.org/abs/1511.01432&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn68" class="footnote-item"&gt;Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of NAACL-HLT 2018.&lt;/li&gt;  
&lt;li id="fn69" class="footnote-item"&gt;Howard, J., &amp; Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1801.06146"&gt;http://arxiv.org/abs/1801.06146&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn70" class="footnote-item"&gt;Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In NAACL-HLT 2016.&lt;/li&gt;  
&lt;li id="fn71" class="footnote-item"&gt;Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn72" class="footnote-item"&gt;Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. (2016). Character-based Neural Machine Translation. In ICLR. Retrieved from &lt;a href="http://arxiv.org/abs/1511.04586"&gt;http://arxiv.org/abs/1511.04586&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn73" class="footnote-item"&gt;Lee, J., Cho, K., &amp; Bengio, Y. (2017). Fully Character-Level Neural Machine Translation without Explicit Segmentation. In Transactions of the Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn74" class="footnote-item"&gt;Jia, R., &amp; Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.&lt;/li&gt;  
&lt;li id="fn75" class="footnote-item"&gt;Miyato, T., Dai, A. M., &amp; Goodfellow, I. (2017). Adversarial Training Methods for Semi-supervised Text Classification. In Proceedings of ICLR 2017.&lt;/li&gt;  
&lt;li id="fn76" class="footnote-item"&gt;Yasunaga, M., Kasai, J., &amp; Radev, D. (2018). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.04903"&gt;http://arxiv.org/abs/1711.04903&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn77" class="footnote-item"&gt;Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17.&lt;/li&gt;  
&lt;li id="fn78" class="footnote-item"&gt;Kim, Y., Stratos, K., &amp; Kim, D. (2017). Adversarial Adaptation of Synthetic or Stale Data. In Proceedings of ACL (pp. 1297–1307).&lt;/li&gt;  
&lt;li id="fn79" class="footnote-item"&gt;Semeniuta, S., Severyn, A., &amp; Gelly, S. (2018). On Accurate Evaluation of GANs for Language Generation. Retrieved from &lt;a href="http://arxiv.org/abs/1806.04936"&gt;http://arxiv.org/abs/1806.04936&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn80" class="footnote-item"&gt;&lt;p&gt;Fang, M., Li, Y., &amp; Cohn, T. (2017). Learning how to Active Learn: A Deep Reinforcement Learning Approach. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="https://arxiv.org/pdf/1708.02383v1.pdf"&gt;https://arxiv.org/pdf/1708.02383v1.pdf&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn81" class="footnote-item"&gt;&lt;p&gt;Wu, J., Li, L., &amp; Wang, W. Y. (2018). Reinforced Co-Training. In Proceedings of NAACL-HLT 2018.&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn82" class="footnote-item"&gt;&lt;p&gt;Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In Proceedings of ICLR 2018.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn83" class="footnote-item"&gt;&lt;p&gt;Celikyilmaz, A., Bosselut, A., He, X., &amp; Choi, Y. (2018). Deep communicating agents for abstractive summarization. In Proceedings of NAACL-HLT 2018.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn84" class="footnote-item"&gt;&lt;p&gt;Ranzato, M. A., Chopra, S., Auli, M., &amp; Zaremba, W. (2016). Sequence level training with recurrent neural networks. In Proceedings of ICLR 2016.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn85" class="footnote-item"&gt;&lt;p&gt;Wang, X., Chen, W., Wang, Y.-F., &amp; Wang, W. Y. (2018). No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1804.09160"&gt;http://arxiv.org/abs/1804.09160&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn86" class="footnote-item"&gt;Liu, B., Tür, G., Hakkani-Tür, D., Shah, P., &amp; Heck, L. (2018). Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems. In Proceedings of NAACL-HLT 2018.&lt;/li&gt;  
&lt;li id="fn87" class="footnote-item"&gt;Kuncoro, A., Dyer, C., Hale, J., Yogatama, D., Clark, S., &amp; Blunsom, P. (2018). LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better. In Proceedings of ACL 2018 (pp. 1–11). Retrieved from &lt;a href="http://aclweb.org/anthology/P18-1132"&gt;http://aclweb.org/anthology/P18-1132&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn88" class="footnote-item"&gt;Blevins, T., Levy, O., &amp; Zettlemoyer, L. (2018). Deep RNNs Encode Soft Hierarchical Syntax. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1805.04218"&gt;http://arxiv.org/abs/1805.04218&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn89" class="footnote-item"&gt;Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., &amp; Bowman, S. R. (2018). GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.&lt;/li&gt;  
&lt;li id="fn90" class="footnote-item"&gt;McCann, B., Keskar, N. S., Xiong, C., &amp; Socher, R. (2018). The Natural Language Decathlon: Multitask Learning as Question Answering.&lt;/li&gt;  
&lt;li id="fn91" class="footnote-item"&gt;Lample, G., Denoyer, L., &amp; Ranzato, M. (2018). Unsupervised Machine Translation Using Monolingual Corpora Only. In Proceedings of ICLR 2018.&lt;/li&gt;  
&lt;li id="fn92" class="footnote-item"&gt;Artetxe, M., Labaka, G., Agirre, E., &amp; Cho, K. (2018). Unsupervised Neural Machine Translation. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1710.11041"&gt;http://arxiv.org/abs/1710.11041&lt;/a&gt;&lt;/li&gt;  
&lt;li id="fn93" class="footnote-item"&gt;Graves, A., Jaitly, N., &amp; Mohamed, A. R. (2013, December). Hybrid speech recognition with deep bidirectional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on (pp. 273-278). IEEE.&lt;/li&gt;  
&lt;li id="fn94" class="footnote-item"&gt;Ramachandran, P., Liu, P. J., &amp; Le, Q. V. (2017). Unsupervised Pretraining for Sequence to Sequence Learning. In Proceedings of EMNLP 2017.&lt;/li&gt;  
&lt;li id="fn95" class="footnote-item"&gt;Baker, C. F., Fillmore, C. J., &amp; Lowe, J. B. (1998, August). The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1 (pp. 86-90). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn96" class="footnote-item"&gt;Tjong Kim Sang, E. F., &amp; Buchholz, S. (2000, September). Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7 (pp. 127-132). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn97" class="footnote-item"&gt;Tjong Kim Sang, E. F., &amp; De Meulder, F. (2003, May). Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4 (pp. 142-147). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn98" class="footnote-item"&gt;Buchholz, S., &amp; Marsi, E. (2006, June). CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the tenth conference on computational natural language learning (pp. 149-164). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn99" class="footnote-item"&gt;Lafferty, J., McCallum, A., &amp; Pereira, F. C. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data.&lt;/li&gt;  
&lt;li id="fn100" class="footnote-item"&gt;Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn101" class="footnote-item"&gt;Collins, M. (2002, July). Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 1-8). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn102" class="footnote-item"&gt;Pang, B., Lee, L., &amp; Vaithyanathan, S. (2002, July). Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn103" class="footnote-item"&gt;Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.&lt;/li&gt;  
&lt;li id="fn104" class="footnote-item"&gt;Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning, C. (2004). Max-margin parsing. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.&lt;/li&gt;  
&lt;li id="fn105" class="footnote-item"&gt;Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning, C. (2004). Max-margin parsing. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.&lt;/li&gt;  
&lt;li id="fn106" class="footnote-item"&gt;Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., &amp; Weischedel, R. (2006, June). OntoNotes: the 90% solution. In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers (pp. 57-60). Association for Computational Linguistics.&lt;/li&gt;  
&lt;li id="fn107" class="footnote-item"&gt;Milne, D., &amp; Witten, I. H. (2008, October). Learning to link with wikipedia. In Proceedings of the 17th ACM conference on Information and knowledge management (pp. 509-518). ACM.&lt;/li&gt;  
&lt;li id="fn108" class="footnote-item"&gt;Mintz, M., Bills, S., Snow, R., &amp; Jurafsky, D. (2009, August). Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2 (pp. 1003-1011). Association for Computational Linguistics.&lt;/li&gt;  
&lt;/ol&gt;</content:encoded></item><item><title>ACL 2018 Highlights: Understanding Representations and Evaluation in More Challenging Settings</title><description>This post reviews two themes of ACL 2018: 1) gaining a better understanding what models capture and 2) to expose them to more challenging settings.</description><link>http://ruder.io/acl-2018-highlights/</link><guid isPermaLink="false">a7f32d3f-6648-44b2-889b-79a95f590d19</guid><category>nlp</category><category>natural language processing</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Thu, 26 Jul 2018 11:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/07/view_out_of_conference_venue-2.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2018/07/view_out_of_conference_venue-2.jpg" alt="ACL 2018 Highlights: Understanding Representations and Evaluation in More Challenging Settings"&gt;&lt;p&gt;&lt;link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot-number.min.css"&gt;  &lt;/p&gt;

&lt;script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot.min.js"&gt;&lt;/script&gt;  
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"&gt;&lt;/script&gt;  
&lt;script type="text/javascript"&gt;  
    $.bigfoot({useFootnoteOnlyOnce: false});
&lt;/script&gt;  

&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/acl-2018-highlights-understanding-representations-and-evaluation-in-more-challenging-settings/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I attended the &lt;a href="https://acl2018.org/"&gt;56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)&lt;/a&gt; in Melbourne, Australia from July 15-20, 2018 and presented three papers &lt;a href="http://ruder.io/acl-2018-highlights/#fn25"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn38"&gt;&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn26"&gt;&lt;/a&gt;. It is foolhardy to try to condense an entire conference into one topic; however, in retrospect, certain themes appear particularly pronounced. In 2015 and 2016, NLP conferences were dominated by word embeddings and some people were musing that &lt;a href="http://ruder.io/word-embeddings-1/"&gt;&lt;em&gt;Embedding Methods in Natural Language Processing&lt;/em&gt;&lt;/a&gt; was a more appropriate name for the Conference on &lt;a href="http://emnlp2018.org/"&gt;&lt;em&gt;Empirical Methods in Natural Language Processing&lt;/em&gt;&lt;/a&gt;, one of the top conferences in the field. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://nlp.stanford.edu/manning/talks/Simons-Institute-Manning-2017.pdf"&gt;According to Chris Manning&lt;/a&gt;, 2017 was the year of the BiLSTM with attention. While BiLSTMs optionally with attention are still ubiquitous, the main themes of this conference for me were to &lt;em&gt;gain a better understanding&lt;/em&gt; what the representations of such models capture and to expose them to &lt;em&gt;more challenging settings&lt;/em&gt;. In my review, I will mainly focus on contributions that touch on these themes but will also discuss other themes that I found of interest.&lt;/p&gt;

&lt;h1 id="understandingrepresentations"&gt;Understanding Representations&lt;/h1&gt;

&lt;h2&gt;Probing models&lt;/h2&gt;

&lt;p&gt;It was very refreshing to see that rather than introducing ever shinier new models, many papers methodically investigated existing models and what they capture. This was most commonly done by automatically creating a dataset that focuses on one particular aspect of the generalization behaviour and evaluating different trained models on this dataset:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Conneau et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn11"&gt;&lt;/a&gt; for instance evaluate different sentence embedding methods on ten datasets designed to capture certain linguistic features, such as predicting the length of a sentence, recovering word content, sensitivity to bigram shift, etc. They find that different encoder architectures can result in embeddings with different characteristics and that bag-of-embeddings is surprisingly good at capturing sentence-level properties, among other results.&lt;/li&gt;  
&lt;li&gt; Zhu et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn9"&gt;&lt;/a&gt; evaluate sentence embeddings by observing the change in similarity of generated triplets of sentences that differ in a certain semantic or syntactic aspect. They find---among other things---that SkipThought and InferSent can distinguish negation from synonymy, while InferSent is better at identifying semantic equivalence and dealing with quantifiers.&lt;/li&gt;  
&lt;li&gt; Pezzelle et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn10"&gt;&lt;/a&gt; focus specifically on quantifiers and test different CNN and LSTM models on their ability to predict quantifiers in single-sentence and multi-sentence contexts. They find that in single-sentence context, models outperform humans, while humans are slightly better in multi-sentence contexts.&lt;/li&gt;  
&lt;li&gt; Kuncoro et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn12"&gt;&lt;/a&gt; evaluate LSTMs on modeling subject-verb agreement. They find that with enough capacity, LSTMs can model subject-verb agreement, but that more syntax-sensitive models such as recurrent neural network grammars do even better.&lt;/li&gt;  
&lt;li&gt; Blevins et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn24"&gt;&lt;/a&gt; evaluate models pretrained on different tasks whether they capture a hierarchical notion of syntax. Specifically, they train the models to predict POS tags as well as constituent labels at different depths of a parse tree. They find that all models indeed encode a significant amount of syntax and---in particular---that language models learn some syntax.&lt;/li&gt;  
&lt;li&gt; Another interesting result regarding the generalization ability of language models is due to Lau et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn16"&gt;&lt;/a&gt; who find that a language model trained on a sonnet corpus captures meter implicitly at human-level performance.&lt;/li&gt;  
&lt;li&gt; Language models, however, also have their limitations. Spithourakis and Riedel&lt;a href="http://ruder.io/acl-2018-highlights/#fn23"&gt;&lt;/a&gt; observe that language models are bad at modelling numerals and propose several strategies to improve them.&lt;/li&gt;  
&lt;li&gt; Liu et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn37"&gt;&lt;/a&gt; show that LSTMs trained on natural language data are able to recall tokens from much longer sequence than models trained on non-language data at the Repl4NLP workshop.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;p&gt;In particular, I think better understanding what information LSTMs and language models will become more important, as they seem to be a key driver of progress in NLP going forward, as evidenced by our ACL paper on language model fine-tuning&lt;a href="http://ruder.io/acl-2018-highlights/#fn38"&gt;&lt;/a&gt; and &lt;a href="http://ruder.io/nlp-imagenet/"&gt;related approaches&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="understandingstateoftheartmodels"&gt;Understanding state-of-the-art models&lt;/h2&gt;

&lt;p&gt;While the above studies try to understand a specific aspect of the generalization ability of a particular model class, several papers focus on better understanding state-of-the-art models for a particular task:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Glockner et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn8"&gt;&lt;/a&gt; focused on the task of natural language inference. They created a dataset with sentences that differ by at most one word from sentences in the training data in order to probe if models can deal with simple lexical inferences. They find that current state-of-the-art models fail to capture many simple inferences.&lt;/li&gt;  
&lt;li&gt; Mudrakarta et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn22"&gt;&lt;/a&gt; analyse state-of-the-art QA models across different modalities and find that the models often ignore key question terms. They then perturb questions to craft adversarial examples that significantly lower models' accuracy.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;p&gt;I found many of the papers probing different aspects of models stimulating. I hope that the generation of such probing datasets will become a standard tool in the toolkit of every NLP researchers so that we will not only see more of such papers in the future but that such an analysis may also become part of the standard model evaluation, besides error and ablation analyses.&lt;/p&gt;

&lt;h2 id="analyzingtheinductivebias"&gt;Analyzing the inductive bias&lt;/h2&gt;

&lt;p&gt;Another way to gain a better understanding of a model is to analyze its inductive bias. The &lt;a href="https://sites.google.com/view/relsnnlp"&gt;Workshop on Relevance of Linguistic Structure in Neural Architectures for NLP (RELNLP)&lt;/a&gt; sought to explore how useful it is to incorporate linguistic structure into our models. One of the key points of Chris Dyer's talk during the workshop was whether RNNs have a useful inductive bias for NLP. In particular, he argued that there are several pieces of evidence indicating that RNNs prefer sequential recency, namely:  &lt;/p&gt;

&lt;ol&gt;  
&lt;li&gt; Gradients become attenuated across time. LSTMs or GRUs may help with this, but they also forget.&lt;/li&gt;  
&lt;li&gt; People have used training regimes like reversing the input sequence for machine translation.&lt;/li&gt;  
&lt;li&gt; People have used enhancements like attention to have direct connections back in time.&lt;/li&gt;  
&lt;li&gt; For modeling subject-verb agreement, the error rate increases with the number of attractors&lt;a href="http://ruder.io/acl-2018-highlights/#fn29"&gt;&lt;/a&gt;.&lt;/li&gt;  
&lt;/ol&gt;  

&lt;p&gt;According to Chomsky, sequential recency is not the right bias for learning human language. RNNs thus don't seem to have the right bias for modeling language, which in practice can lead to statistical inefficiency and poor generalization behaviour. Recurrent neural network grammars&lt;a href="http://ruder.io/acl-2018-highlights/#fn30"&gt;&lt;/a&gt;, a class of models that generates both a tree and a sequence sequentially by compressing a sentence into its constituents, instead have a bias for syntactic (rather than sequential) recency. &lt;/p&gt;

&lt;p&gt;However, it can often be hard to identify whether a model has a useful inductive bias. For identifying subject-verb agreement, Chris hypothesizes that LSTM language models learn a non-structural "first noun" heuristic that relies on matching the verb to the first noun in the sentence. In general, perplexity (and other aggregate metrics) are correlated with syntactic/structural competence, but are not particularly sensitive at distinguishing structurally sensitive models from models that use a simpler heuristic.&lt;/p&gt;

&lt;h2&gt;Using Deep Learning to understand language&lt;/h2&gt;

&lt;p&gt;In his talk at the workshop, Mark Johnson opined that while Deep Learning has revolutionized NLP, its primary benefit is economic: complex component pipelines have been replaced with end-to-end models and target accuracy can often be achieved more quickly and cheaply. Deep Learning has not changed our understanding of language. Its main contribution in this regard is to demonstrate that a neural network aka a computational model can perform certain NLP tasks, which shows that these tasks are not indicators of intelligence. While DL methods can pattern match and perform perceptual tasks really well, they struggle with tasks relying on deliberate reflection and conscious thought.&lt;/p&gt;

&lt;h2&gt;Incorporating linguistic structure&lt;/h2&gt;

&lt;p&gt;Jason Eisner questioned in his talk whether linguistic structures and categories actually exist or whether "scientist just like to organize data into piles" given that a linguistics-free approach works surprisingly well for MT. He finds that even "arbitrarily defined" categories such as the difference between the /b/ and /p/ phonemes can become hardened and accrue meaning. However, neural models are pretty good sponges to soak up whatever isn't modeled explicitly.&lt;/p&gt;

&lt;p&gt;He outlines four common ways to introduce linguistic information into models: a) via a pipeline-based approach, where linguistic categories are used as features; b) via data augmentation, where the data is augmented with linguistic categories; c) via multi-task learning; d) via structured modeling such as using a transition-based parser, a recurrent neural network grammar, or even classes that depend on each other such as BIO notation.&lt;/p&gt;

&lt;p&gt;In her talk at the workshop, Emily Bender questioned the premise of linguistics-free learning altogether: Even if you had a huge corpus in a language that you knew nothing about, without any other priors, e.g. what function words are, you would not be able to learn sentence structure or meaning. She also pointedly called out many ML papers that describe their approach as similar to how babies learn, without citing any actual developmental psychology or language acquisition literature. Babies in fact learn in situated, joint, emotional context, which carries a lot of signal and meaning.&lt;/p&gt;

&lt;h2&gt;Understanding the failure modes of LSTMs&lt;/h2&gt;

&lt;p&gt;Better understanding representations was also a theme at the &lt;a href="https://sites.google.com/site/repl4nlp2018/ &lt;br /&gt;
"&gt;Representation Learning for NLP workshop&lt;/a&gt;. During his talk, Yoav Goldberg detailed some of the efforts of his group to better understand representations of RNNs. In particular, he discussed recent work on extracting a finite state automaton from an RNN in order to better understand what the model has learned&lt;a href="http://ruder.io/acl-2018-highlights/#fn35"&gt;&lt;/a&gt;. He also reminded the audience that LSTM representations, even though they have been trained on one task, are not task-specific. They are often predictive of unintended aspects such as demographics in the data. Even when a model has been trained using a domain-adversarial loss to produce representations that are invariant of a certain aspect, the representations will be still slightly predictive of said attribute. It can thus be a challenge to completely remove unwanted information from encoded language data and even seemingly perfect LSTM models may have hidden failure modes.&lt;/p&gt;

&lt;p&gt;On the topic of failure modes of LSTMs, a statement that also fits well in this theme was uttered by this year's recipient of the ACL lifetime achievement award, Mark Steedman. He asked &lt;em&gt;"LSTMs work in practice, but can they work in theory?"&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=""&gt;Evaluation in more challenging settings&lt;/h1&gt;

&lt;h2&gt;Adversarial examples&lt;/h2&gt;

&lt;p&gt;A theme that is closely interlinked with gaining a better understanding of the limitations of state-of-the-art models is to propose ways how they can be improved. In particular, similar to adversarial example paper mentioned above&lt;a href="http://ruder.io/acl-2018-highlights/#fn22"&gt;&lt;/a&gt;, several papers tried to make models more robust to adversarial examples:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Cheng et al. &lt;a href="http://ruder.io/acl-2018-highlights/#fn21"&gt;&lt;/a&gt; propose to make both the encoder and decoder in NMT models more robust against input perturbations.&lt;/li&gt;  
&lt;li&gt; Ebrahimi et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn28"&gt;&lt;/a&gt; propose white-box adversarial examples to trick a character-level neural classifier by swapping few tokens.&lt;/li&gt;  
&lt;li&gt; Ribeiro et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn13"&gt;&lt;/a&gt; improve upon the previous method with semantic-preserving perturbations that induce changes in the model's predictions, which they generalize to rules that induce adversaries on many instances.&lt;/li&gt;  
&lt;li&gt; Bose et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn41"&gt;&lt;/a&gt; incorporate adversarial examples into noise contrastive estimation using an adversarially learned sampler. The sampler finds harder negative examples, which forces the model to learn better representations.&lt;/li&gt;  
&lt;/ul&gt;

&lt;h2&gt;Learning robust and fair representations&lt;/h2&gt;

&lt;p&gt;Tim Baldwin discussed different ways to make models more robust to a domain shift during his talk at the &lt;a href="https://sites.google.com/site/repl4nlp2018/ &lt;br /&gt;
"&gt;RepL4NLP workshop&lt;/a&gt;. The slides can be found &lt;a href="https://drive.google.com/file/d/1JhZKKCJjIDIqZdwRL0GEUcrWxscLS87l/view"&gt;here&lt;/a&gt;. For using a single source domain, he discussed a method to linguistically perturb training instances based on different types of syntactic and semantic noise&lt;a href="http://ruder.io/acl-2018-highlights/#fn32"&gt;&lt;/a&gt;. In the setting with multiple source domains, he proposed to train an adversarial model on the source domains&lt;a href="http://ruder.io/acl-2018-highlights/#fn33"&gt;&lt;/a&gt;. Finally, he discussed a method that allows to learn robust and privacy-preserving text representations&lt;a href="http://ruder.io/acl-2018-highlights/#fn34"&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Margaret Mitchell focused on fair and privacy-preserving representations during her talk at the workshop. In particular, she highlighted the difference between a descriptive and a normative view of the world. ML models learn representations that reflect a descriptive view of the data they're trained on. The data represents "the world as people talk about it". Research in fairness conversely seeks to create representations that reflect a normative view of the world, which captures our values and seeks to instill them in the representations.&lt;/p&gt;

&lt;h2&gt;Improving evaluation methodology&lt;/h2&gt;

&lt;p&gt;Besides making models more robust, several papers sought to improve the way we evaluate our models:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Finegan-Dollak et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn18"&gt;&lt;/a&gt; identify limitations and propose improvements to current evaluations of text-to-SQL systems. They show that the current train-test split and practice of anonymization of variables are flawed and release standardized and improved versions of seven datasets to mitigate these.&lt;/li&gt;  
&lt;li&gt; Dror et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn14"&gt;&lt;/a&gt; focus on a practice that should be commonplace, but is often not done or done poorly: statistical significance testing. In particular, they survey recent empirical papers in ACL and TACL 2017 finding that statistical significance testing is often ignored or misused and propose a simple protocol for statistical significance test selection for NLP tasks.&lt;/li&gt;  
&lt;li&gt; Chaganty et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn17"&gt;&lt;/a&gt; investigate the bias of automatic metrics such as BLEU and ROUGE and find that even an unbiased estimator only achieves a comparatively low error reduction. This highlights the need to improve both the correlation of automatic metric as well as reduce the variance of human annotation.&lt;/li&gt;  
&lt;/ul&gt;

&lt;h2&gt;Strong baselines&lt;/h2&gt;

&lt;p&gt;Another way to improve model evaluation is to compare new models against stronger baselines, in order to make sure that improvements are actually significant. Some papers focused on this line of research:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Shen et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn19"&gt;&lt;/a&gt; systematically compare simple word embedding-based methods with pooling to more complex models such as LSTMs and CNNs. They find that for most datasets, word embedding-based methods exhibit competitive or even superior performance.&lt;/li&gt;  
&lt;li&gt; Ethayarajh&lt;a href="http://ruder.io/acl-2018-highlights/#fn36"&gt;&lt;/a&gt; proposes a strong baseline for sentence embedding models at the RepL4NLP workshop.&lt;/li&gt;  
&lt;li&gt; In a similar vein, Ruder and Plank&lt;a href="http://ruder.io/acl-2018-highlights/#fn25"&gt;&lt;/a&gt; find that classic bootstrapping algorithms such as tri-training make for strong baselines for semi-supervised learning and even outperform recent state-of-the-art methods.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;p&gt;In the above paper, we also emphasize the importance of evaluating in more challenging settings, such as on out-of-distribution data and on different tasks. Our findings would have been different if we had just focused on a single task or only on in-domain data. We need to test our models under such adverse conditions to get a better sense of their robustness and how well they can actually generalize.&lt;/p&gt;

&lt;h2&gt;Creating harder datasets&lt;/h2&gt;

&lt;p&gt;In order to evaluate under such settings, more challenging datasets need to be created. Yejin Choi argued during the RepL4NLP panel discussion (a summary can be found &lt;a href="https://twitter.com/seb_ruder/status/1020196710050455554"&gt;here&lt;/a&gt;) that the community pays a lot of attention to easier tasks such as SQuAD or bAbI, which are close to solved. Yoav Goldberg even went so far as to say that "SQuAD is the MNIST of NLP". Instead, we should focus on solving harder tasks and develop more datasets with increasing levels of difficulty. If a dataset is too hard, people don't work on it. In particular, the community should not work on datasets for too long as datasets are getting solved very fast these days; creating novel and more challenging datasets is thus even more important. Two datasets that seek to go beyond SQuAD for reading comprehension were presented at the conference:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; &lt;a href="http://qangaroo.cs.ucl.ac.uk/"&gt;QAngaroo&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn39"&gt;&lt;/a&gt; focuses on reading comprehension that requires to gather several pieces of information via multiple steps of inference.&lt;/li&gt;  
&lt;li&gt; &lt;a href="https://github.com/deepmind/narrativeqa"&gt;NarrativeQA&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn40"&gt;&lt;/a&gt; requires understand of an underlying narrative by asking the reader to answer questions about stories by reading entire books or movie scripts.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;p&gt;Richard Socher also stressed the importance of training and evaluating a model across multiple tasks during &lt;a href="https://twitter.com/RichardSocher/status/1021917140801052672"&gt;his talk&lt;/a&gt; during the &lt;a href="https://mrqa2018.github.io/"&gt;Machine Reading for Question Answering workshop&lt;/a&gt;. In particular, he argues that NLP requires many types of reasoning, e.g. logical, linguistic, emotional, etc., which cannot all be satisfied by a single task. &lt;/p&gt;

&lt;h2&gt;Evaluation on multiple and low-resource languages&lt;/h2&gt;

&lt;p&gt;Another facet of this is to evaluate our models on multiple languages. Emily Bender surveyed 50 NAACL 2018 papers in her talk mentioned above and found that 42 papers evaluate on an unnamed mystery language (i.e. English). She emphasizes that it is important to name the language you work on as languages have different linguistic structures; not mentioning the language obfuscates this fact.&lt;/p&gt;

&lt;p&gt;If our methods are designed to be cross-lingual, then we should additionally evaluate them on the more challenging setting of low-resource languages. For instance, both of the following two papers observe that current methods for unsupervised bilingual dictionary methods fail if the target language is dissimilar to language such as with Estonian or Finnish:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Søgaard et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn26"&gt;&lt;/a&gt; probe the limitations of current methods further and highlight that such methods also fail when embeddings are trained on different domains or using different algorithms. They finally propose a metric to quantify the potential of such methods.&lt;/li&gt;  
&lt;li&gt; Artetxe et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn27"&gt;&lt;/a&gt; propose a new unsupervised self-training method that employs a better initialization to steer the optimization process and is particularly robust for dissimilar language pairs.&lt;/li&gt;  
&lt;/ul&gt;

&lt;p&gt;Several other papers also evaluate their approaches on low-resource languages:  &lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt; Dror et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn15"&gt;&lt;/a&gt; propose to use orthographic features for bilingual lexicon induction. Though these mostly help for related languages, they also evaluate on the dissimilar language pair English-Finnish.&lt;/li&gt;  
&lt;li&gt; Ren et al.&lt;a href="http://ruder.io/acl-2018-highlights/#fn20"&gt;&lt;/a&gt; finally propose to leverage another rich language for translation into a resource-poor language . They find that their model significantly improves the translation quality of rare languages.&lt;/li&gt;  
&lt;li&gt; Currey and Heafield&lt;a href="http://ruder.io/acl-2018-highlights/#fn31"&gt;&lt;/a&gt; propose an unsupervised tree-to-sequence model for NMT by adapting the Gumbel tree-LSTM. Their model proves particularly useful for low-resource languages.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;h1 id=""&gt;Progress in NLP&lt;/h1&gt;

&lt;p&gt;Another theme during the conference for me was that the field is visibly making progress. Marti Hearst, president of the ACL, echoed this sentiment during her presidential address. She used to demonstrate what our models can and can't do using the example of Stanley Kubrick's HAL 9000 (seen below). In recent years, this has become a less useful exercise as our models have learned to perform tasks that seemed previously decades away such as recognizing and producing human speech or lipreading&lt;a href="http://ruder.io/acl-2018-highlights/#fn7"&gt;[1]&lt;/a&gt;. Naturally, we are still far away from tasks that require deep language understanding and reasoning such as having an argument; nevertheless, this progress is remarkable.  &lt;/p&gt;

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/hal9000.png" width="150" alt="ACL 2018 Highlights: Understanding Representations and Evaluation in More Challenging Settings"&gt;  
    &lt;figcaption&gt;Hal 9000. (Source: &lt;a href="https://creativecommons.org/licenses/by/3.0" title="Creative Commons Attribution 3.0"&gt;CC BY 3.0&lt;/a&gt;, &lt;a href="https://commons.wikimedia.org/w/index.php?curid=11651154"&gt;Wikimedia&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;Marti also paraphrased NLP and IR pioneer Karen Spärck Jones saying that research is not going around in circles, but climbing a spiral or---maybe more fittingly---different staircases that are not necessarily linked but go in the same direction. She also expressed a sentiment that seems to resonate with a lot of people: In the 1980s and 90s, with only a few papers to read, it was definitely easier to keep track of the state of the art. To make this easier, I've recently created a document to collect the &lt;a href="http://nlpprogress.com/"&gt;state of the art across different NLP tasks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With the community growing, she encouraged people to participate and volunteer and announced an ACL Distinguished Service Award for the most dedicated members. ACL 2018 also saw the launch (after EACL in 1982 and NAACL in 2000) of its third chapter, AACL, the &lt;a href="http://aaclweb.org/"&gt;Asia-Pacific Chapter of the Association for Computational Linguistics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The business meeting during the conference focused on measures to address a particular challenge of the growing community: the escalating number of submissions and the need for more reviewers. We can expect to see new efforts to deal with the large number of submissions at the conferences next year.&lt;/p&gt;

&lt;h1 id="reinforcementlearning"&gt;Reinforcement learning&lt;/h1&gt;

&lt;p&gt;&lt;a href="http://ruder.io/emnlp-2016-highlights/"&gt;Back in 2016&lt;/a&gt;, it seemed as though reinforcement learning (RL) was finding its footing in NLP and being applied to more and more tasks. These days, it seems that the dynamic nature of RL makes it most useful for tasks that intrinsically have some temporal dependency such as selecting data during training&lt;a href="http://ruder.io/acl-2018-highlights/#fn1"&gt;[1]&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn2"&gt;[1]&lt;/a&gt; and modelling dialogue, while supervised learning seems to be better suited for most other tasks. Another important application of RL is to optimize the end metric such as ROUGE or BLEU directly instead of optimizing a surrogate loss such as cross-entropy. Successful applications of this are summarization&lt;a href="http://ruder.io/acl-2018-highlights/#fn3"&gt;[1]&lt;/a&gt;&lt;a href="http://ruder.io/acl-2018-highlights/#fn4"&gt;[1]&lt;/a&gt; and machine translation&lt;a href="http://ruder.io/acl-2018-highlights/#fn5"&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Inverse reinforcement learning can be valuable in settings where the reward is too complex to be specified. A successful application of this is visual storytelling&lt;a href="http://ruder.io/acl-2018-highlights/#fn6"&gt;[1]&lt;/a&gt;. RL is particularly promising for sequential decision making problems in NLP such as playing text-based games, navigating webpages, and completing tasks. The &lt;a href="https://www.cs.ucsb.edu/~william/papers/ACL2018DRL4NLP.pdf"&gt;Deep Reinforcement Learning for NLP tutorial&lt;/a&gt; provided a comprehensive overview of the space.&lt;/p&gt;

&lt;h1 id=""&gt;Tutorials&lt;/h1&gt;

&lt;p&gt;There were other great tutorials as well. I particularly enjoyed the &lt;a href="https://github.com/philschulz/VITutorial"&gt;Variational Inference and Deep Generative Models tutorial&lt;/a&gt;. The tutorials on &lt;a href="https://github.com/allenai/acl2018-semantic-parsing-tutorial"&gt;Semantic Parsing&lt;/a&gt; and about &lt;a href="http://faculty.washington.edu/ebender/100things-sem_prag.html"&gt;"100 things you always wanted to know about semantics &amp;amp; pragmatics"&lt;/a&gt; also seemed really worthwhile. A complete list of the tutorials can be found &lt;a href="https://acl2018.org/tutorials/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover image: View from the conference venue.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href="http://isabelleaugenstein.github.io/"&gt;Isabelle Augenstein&lt;/a&gt; for some paper suggestions.&lt;/em&gt;&lt;/p&gt;

&lt;ol class="footnotes-list"&gt;  
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Fang, M., Li, Y., &amp; Cohn, T. (2017). Learning how to Active Learn: A Deep Reinforcement Learning Approach. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="https://arxiv.org/pdf/1708.02383v1.pdf"&gt;https://arxiv.org/pdf/1708.02383v1.pdf&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;Wu, J., Li, L., &amp; Wang, W. Y. (2018). Reinforced Co-Training. In Proceedings of NAACL-HLT 2018.&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn3" class="footnote-item"&gt;&lt;p&gt;Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In Proceedings of ICLR 2018.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn4" class="footnote-item"&gt;&lt;p&gt;Celikyilmaz, A., Bosselut, A., He, X., &amp; Choi, Y. (2018). Deep communicating agents for abstractive summarization. In Proceedings of NAACL-HLT 2018.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn5" class="footnote-item"&gt;&lt;p&gt;Ranzato, M. A., Chopra, S., Auli, M., &amp; Zaremba, W. (2016). Sequence level training with recurrent neural networks. In Proceedings of ICLR 2016.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn6" class="footnote-item"&gt;&lt;p&gt;Wang, X., Chen, W., Wang, Y.-F., &amp; Wang, W. Y. (2018). No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1804.09160"&gt;http://arxiv.org/abs/1804.09160&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn7" class="footnote-item"&gt;&lt;p&gt;Chung, J. S., &amp; Zisserman, A. (2016, November). Lip reading in the wild. In Asian Conference on Computer Vision (pp. 87-103). Springer, Cham.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn8" class="footnote-item"&gt;&lt;p&gt;Glockner, M., Shwartz, V., &amp; Goldberg, Y. (2018). Breaking NLI Systems with Sentences that Require Simple Lexical Inferences. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1805.02266"&gt;http://arxiv.org/abs/1805.02266&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn9" class="footnote-item"&gt;&lt;p&gt;Zhu, X., Li, T., &amp; De Melo, G. (2018). Exploring Semantic Properties of Sentence Embeddings. In Proceedings of ACL 2018 (pp. 1–6). Retrieved from &lt;a href="http://aclweb.org/anthology/P18-2100"&gt;http://aclweb.org/anthology/P18-2100&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn10" class="footnote-item"&gt;&lt;p&gt;Pezzelle, S., Steinert-Threlkeld, S., Bernardi, R., &amp; Szymanik, J. (2018). Some of Them Can be Guessed! Exploring the Effect of Linguistic Context in Predicting Quantifiers. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1806.00354"&gt;http://arxiv.org/abs/1806.00354&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn11" class="footnote-item"&gt;&lt;p&gt;Conneau, A., Kruszewski, G., Lample, G., Barrault, L., &amp; Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1805.01070"&gt;http://arxiv.org/abs/1805.01070&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn12" class="footnote-item"&gt;&lt;p&gt;Kuncoro, A., Dyer, C., Hale, J., Yogatama, D., Clark, S., &amp; Blunsom, P. (2018). LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better. In Proceedings of ACL 2018 (pp. 1–11). Retrieved from &lt;a href="http://aclweb.org/anthology/P18-1132"&gt;http://aclweb.org/anthology/P18-1132&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn13" class="footnote-item"&gt;&lt;p&gt;  
Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2018). Semantically Equivalent Adversarial Rules for Debugging NLP Models. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn14" class="footnote-item"&gt;&lt;p&gt;  
Dror, R., Baumer, G., Shlomov, S., &amp; Reichart, R. (2018). The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing. In Proceedings of ACL 2018. Retrieved from &lt;a href="https://ie.technion.ac.il/~roiri/papers/ACL-2018-sig-cr.pdf"&gt;https://ie.technion.ac.il/~roiri/papers/ACL-2018-sig-cr.pdf&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn15" class="footnote-item"&gt;&lt;p&gt;  
Dror, R., Baumer, G., Shlomov, S., &amp; Reichart, R. (2018). Riley, P., &amp; Gildea, D. (2018). Orthographic Features for Bilingual Lexicon Induction. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn16" class="footnote-item"&gt;&lt;p&gt;  
Lau, J. H., Cohn, T., Baldwin, T., Brooke, J., &amp; Hammond, A. (2018). Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1807.03491"&gt;http://arxiv.org/abs/1807.03491&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn17" class="footnote-item"&gt;&lt;p&gt;  
Chaganty, A. T., Mussman, S., &amp; Liang, P. (2018). The price of debiasing automatic metrics in natural language evaluation. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1807.02202"&gt;http://arxiv.org/abs/1807.02202&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn18" class="footnote-item"&gt;&lt;p&gt;  
Finegan-Dollak, C., Kummerfeld, J. K., Zhang, L., Ramanathan, K., Sadasivam, S., Zhang, R., &amp; Radev, D. (2018). Improving Text-to-SQL Evaluation Methodology. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1806.09029"&gt;http://arxiv.org/abs/1806.09029&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn19" class="footnote-item"&gt;&lt;p&gt;  
Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., … Carin, L. (2018). Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn20" class="footnote-item"&gt;&lt;p&gt;  
Ren, S., Chen, W., Liu, S., Li, M., Zhou, M., &amp; Ma, S. (2018). Triangular Architecture for Rare Language Translation. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1805.04813"&gt;http://arxiv.org/abs/1805.04813&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn21" class="footnote-item"&gt;&lt;p&gt;  
Cheng, Y., Tu, Z., Meng, F., Zhai, J., &amp; Liu, Y. (2018). Towards Robust Neural Machine Translation. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn22" class="footnote-item"&gt;&lt;p&gt;  
Mudrakarta, P. K., Taly, A., Brain, G., Sundararajan, M., &amp; Google, K. D. (2018). Did the Model Understand the Question? In Proceedings of ACL 2018. Retrieved from &lt;a href="https://arxiv.org/pdf/1805.05492.pdf"&gt;https://arxiv.org/pdf/1805.05492.pdf&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn23" class="footnote-item"&gt;&lt;p&gt;  
Spithourakis, G. P., &amp; Riedel, S. (2018). Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn24" class="footnote-item"&gt;&lt;p&gt;  
Blevins, T., Levy, O., &amp; Zettlemoyer, L. (2018). Deep RNNs Encode Soft Hierarchical Syntax. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1805.04218"&gt;http://arxiv.org/abs/1805.04218&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn25" class="footnote-item"&gt;&lt;p&gt;  
Ruder, S., &amp; Plank, B. (2018). Strong Baselines for Neural Semi-supervised Learning under Domain Shift. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn26" class="footnote-item"&gt;&lt;p&gt;  
Søgaard, A., Ruder, S., &amp; Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn27" class="footnote-item"&gt;&lt;p&gt;  
Artetxe, M., Labaka, G., &amp; Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn28" class="footnote-item"&gt;&lt;p&gt;  
Ebrahimi, J., Rao, A., Lowd, D., &amp; Dou, D. (2018). HotFlip: White-Box Adversarial Examples for Text Classification. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1712.06751"&gt;http://arxiv.org/abs/1712.06751&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn29" class="footnote-item"&gt;&lt;p&gt;  
Linzen, T., Dupoux, E., &amp; Goldberg, Y. (2016). Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Proceedings of TACL 2016. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01368"&gt;http://arxiv.org/abs/1611.01368&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn30" class="footnote-item"&gt;&lt;p&gt;  
Dyer, C., Kuncoro, A., Ballesteros, M., &amp; Smith, N. A. (2016). Recurrent Neural Network Grammars. In NAACL. Retrieved from &lt;a href="http://arxiv.org/abs/1602.07776"&gt;http://arxiv.org/abs/1602.07776&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn31" class="footnote-item"&gt;&lt;p&gt;  
Currey, A., &amp; Heafield, K. (2018). Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation. In Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP (pp. 1–7).  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn32" class="footnote-item"&gt;&lt;p&gt;  
Li, Y., Cohn, T., &amp; Baldwin, T. (2017). Robust Training under Linguistic Adversity. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (Vol. 2, pp. 21–27).  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn33" class="footnote-item"&gt;&lt;p&gt;  
Li, Y., Baldwin, T., &amp; Cohn, T. (2018). Learning Domain-Robust Text Representations using Adversarial Training. In Proceedings of NAACL-HLT 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn34" class="footnote-item"&gt;&lt;p&gt;  
Li, Y., Baldwin, T., &amp; Cohn, T. (2018). Towards Robust and Privacy-preserving Text Representations. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn35" class="footnote-item"&gt;&lt;p&gt;  
Weiss, G., Goldberg, Y., &amp; Yahav, E. (2018). Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples. In Proceedings of ICML 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.09576"&gt;http://arxiv.org/abs/1711.09576&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn36" class="footnote-item"&gt;&lt;p&gt;  
Ethayarajh, K. (2018). Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline. In Proceedings of the 3rd Workshop on Representation Learning for NLP. Retrieved from &lt;a href="http://www.aclweb.org/anthology/W18-3012"&gt;http://www.aclweb.org/anthology/W18-3012&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn37" class="footnote-item"&gt;&lt;p&gt;  
Liu, N. F., Levy, O., Schwartz, R., Tan, C., &amp; Smith, N. A. (2018). LSTMs Exploit Linguistic Attributes of Data. In Proceedings of the 3rd Workshop on Representation Learning for NLP,. Retrieved from &lt;a href="http://arxiv.org/abs/1805.11653"&gt;http://arxiv.org/abs/1805.11653&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn38" class="footnote-item"&gt;&lt;p&gt;  
Howard, J., &amp; Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1801.06146"&gt;http://arxiv.org/abs/1801.06146&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn39" class="footnote-item"&gt;&lt;p&gt;  
Welbl, J., Stenetorp, P., &amp; Riedel, S. (2018). Constructing Datasets for Multi-hop Reading Comprehension Across Documents. In Transactions of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1710.06481"&gt;http://arxiv.org/abs/1710.06481&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn40" class="footnote-item"&gt;&lt;p&gt;  
Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., &amp; Grefenstette, E. (2018). The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1712.07040"&gt;http://arxiv.org/abs/1712.07040&lt;/a&gt;  
&lt;/p&gt;&lt;/li&gt;  
&lt;li id="fn41" class="footnote-item"&gt;&lt;p&gt;  
Bose, A., Ling, H., &amp; Cao, Y. (2018). Adversarial Contrastive Estimation. In Proceedings of ACL 2018.  
&lt;/p&gt;&lt;/li&gt;  
&lt;/ol&gt;</content:encoded></item><item><title>NLP's ImageNet moment has arrived</title><description>Big changes are underway in the world of Natural Language Processing (NLP). The long reign of word vectors has seen a new line of challengers emerge.</description><link>http://ruder.io/nlp-imagenet/</link><guid isPermaLink="false">7b621349-37e7-4ab1-9cfd-e67de9129a2d</guid><category>natural language processing</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Thu, 12 Jul 2018 08:27:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/07/lm_objective.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2018/07/lm_objective.png" alt="NLP's ImageNet moment has arrived"&gt;&lt;p&gt;&lt;link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot-number.min.css"&gt;  &lt;/p&gt;

&lt;script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot.min.js"&gt;&lt;/script&gt;  
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"&gt;&lt;/script&gt;  
&lt;script type="text/javascript"&gt;  
    $.bigfoot({useFootnoteOnlyOnce: false});
&lt;/script&gt;  

&lt;p&gt;&lt;em&gt;This post originally appeared at &lt;a href="https://thegradient.pub/nlp-imagenet/"&gt;TheGradient&lt;/a&gt; and was edited by &lt;a href="http://www.andreykurenkov.com/"&gt;Andrey Kurenkov&lt;/a&gt;, &lt;a href="https://tweetdeck.twitter.com/"&gt;Eric Wang&lt;/a&gt;, and &lt;a href="http://acganesh.com/"&gt;Aditya Ganesh&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Big changes are underway in the world of Natural Language Processing (NLP). The long reign of word vectors as NLP’s core representation technique has seen an exciting new line of challengers emerge: &lt;a href="https://arxiv.org/abs/1802.05365"&gt;ELMo&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn1" id="fnref1"&gt;[1]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1801.06146"&gt;ULMFiT&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn2" id="fnref2"&gt;[2]&lt;/a&gt;&lt;/sup&gt;, and the &lt;a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"&gt;OpenAI transformer&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn3" id="fnref3"&gt;[3]&lt;/a&gt;&lt;/sup&gt;. These works &lt;a href="https://blog.openai.com/language-unsupervised/"&gt;made&lt;/a&gt; &lt;a href="https://techcrunch.com/2018/06/15/machines-learn-language-better-by-using-a-deep-understanding-of-words/"&gt;headlines&lt;/a&gt; by demonstrating that pretrained language models can be used to achieve state-of-the-art results on a wide range of NLP tasks. Such methods herald a watershed moment: they may have the same wide-ranging impact on NLP as pretrained ImageNet models had on computer vision.  &lt;/p&gt;

&lt;h2 id="fromshallowtodeeppretraining"&gt;From Shallow to Deep Pre-Training&lt;/h2&gt;  

&lt;p&gt;Pretrained word vectors have brought NLP a long way.  Proposed in 2013 as an approximation to language modeling, &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases"&gt;word2vec&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn4" id="fnref4"&gt;[4]&lt;/a&gt;&lt;/sup&gt; found adoption through its efficiency and ease of use in a time when hardware was a lot slower and deep learning models were not widely supported. Since then, the standard way of conducting NLP projects has largely remained unchanged: word embeddings pretrained on large amounts of unlabeled data via algorithms such as word2vec and &lt;a href="https://nlp.stanford.edu/pubs/glove.pdf"&gt;GloVe&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn5" id="fnref5"&gt;[5]&lt;/a&gt;&lt;/sup&gt; are used to initialize the first layer of a neural network, the rest of which is then trained on data of a particular task. On most tasks with limited amounts of training data, this led to a boost of &lt;a href="http://www.aclweb.org/anthology/D14-1181"&gt;two to three percentage points&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn6" id="fnref6"&gt;[6]&lt;/a&gt;&lt;/sup&gt;. Though these pretrained word embeddings have been immensely influential, they have a major limitation: they only incorporate previous knowledge in the first layer of the model---the rest of the network still needs to be trained from scratch.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/word2vec_relations.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Relations captured by word2vec. (Source: &lt;a href="https://www.tensorflow.org/versions/master/tutorials/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Word2vec and related methods are &lt;em&gt;shallow&lt;/em&gt; approaches that trade expressivity for efficiency. Using word embeddings is like initializing a computer vision model with pretrained representations that only encode edges: they will be helpful for many tasks, but they fail to capture higher-level information that might be even more useful. A model initialized with word embeddings needs to learn from scratch not only to disambiguate words, but also to derive meaning from a sequence of words. This is the core aspect of language understanding, and it requires modeling complex language phenomena such as compositionality, polysemy, anaphora, long-term dependencies, agreement, negation, and many more. It should thus come as no surprise that NLP models initialized with these shallow representations still require a huge number of examples to achieve good performance.&lt;/p&gt;  

&lt;p&gt;At the core of the recent advances of ULMFiT, ELMo, and the OpenAI transformer is one key paradigm shift: going from just initializing the first layer of our models to pretraining the entire model with hierarchical representations. If learning word vectors is like only learning edges, these approaches are like learning the full hierarchy of features, from edges to shapes to high-level semantic concepts.&lt;/p&gt;  

&lt;p&gt;Interestingly, pretraining entire models to learn both low and high level features has been practiced for years by the computer vision (CV) community. Most often, this is done by learning to classify images on the large ImageNet dataset. ULMFiT, ELMo, and the OpenAI transformer have now brought the NLP community close to having an &amp;quot;&lt;strong&gt;ImageNet for language&lt;/strong&gt;&amp;quot;---that is, a task that enables models to learn higher-level nuances of language, similarly to how ImageNet has enabled training of CV models that learn general-purpose features of images. In the rest of this piece, we’ll unpack just why these approaches seem so promising by extending and building on this analogy to ImageNet.&lt;/p&gt;  

&lt;h2 id="imagenet"&gt;ImageNet&lt;/h2&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/imagenet_challenge.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;The ImageNet Large Scale Visual Recognition Challenge. (Source: &lt;a href="https://www.slideshare.net/xavigiro/image-classification-on-imagenet-d1l4-2017-upc-deep-learning-for-computer-vision/"&gt;Xavier Giro-o-Nieto&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;ImageNet’s impact on the course of machine learning research can hardly be overstated. The dataset was originally published in 2009 and quickly evolved into the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). In 2012, the &lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;deep neural network&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn7" id="fnref7"&gt;[7]&lt;/a&gt;&lt;/sup&gt; submitted by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton performed 41% better than the next best competitor, demonstrating that deep learning was a viable strategy for machine learning and arguably &lt;a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/"&gt;triggering the explosion of deep learning&lt;/a&gt; in ML research.&lt;/p&gt;  

&lt;p&gt;The success of ImageNet highlighted that in the era of deep learning, data was at least as important as algorithms. Not only did the ImageNet dataset enable that very important 2012 demonstration of the power of deep learning, but it also allowed a breakthrough of similar importance in transfer learning: researchers soon realized that the weights learned in state of the art models for ImageNet could be used to initialize models for completely other datasets and improve performance significantly. This &amp;quot;fine-tuning&amp;quot; approach allowed achieving good performance with as little as one positive example per category &lt;a href="https://arxiv.org/abs/1310.1531"&gt;(Donahue et al., 2014)&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn8" id="fnref8"&gt;[8]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/decaf_ilsvrc2012-sun397.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Features trained on ILSVRC-2012 generalize to the SUN-397 dataset. (Source: &lt;a href="https://arxiv.org/abs/1310.1531"&gt;Donahue et al., 2014&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;Pretrained ImageNet models have been used to achieve state-of-the-art results in tasks such as &lt;a href="https://arxiv.org/abs/1703.06870"&gt;object detection&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn9" id="fnref9"&gt;[9]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1612.01105"&gt;semantic segmentation&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn10" id="fnref10"&gt;[10]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1701.01779"&gt;human pose estimation&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn11" id="fnref11"&gt;[11]&lt;/a&gt;&lt;/sup&gt;, and &lt;a href="https://arxiv.org/abs/1705.07750"&gt;video recognition&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn12" id="fnref12"&gt;[12]&lt;/a&gt;&lt;/sup&gt;. At the same time, they have enabled the application of CV to domains where the number of training examples is small and annotation is expensive. Transfer learning via pretraining on ImageNet is in fact so effective in CV that not using it is now considered foolhardy &lt;a href="https://arxiv.org/abs/1805.00932"&gt;(Mahajan et al., 2018)&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn13" id="fnref13"&gt;[13]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;  

&lt;h2 id="whatsinanimagenet"&gt;What’s in an ImageNet?&lt;/h2&gt;  

&lt;p&gt;In order to determine what an ImageNet for language might look like, we first have to identify what makes ImageNet good for transfer learning. &lt;a href="https://arxiv.org/abs/1608.08614"&gt;Previous studies&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn14" id="fnref14"&gt;[14]&lt;/a&gt;&lt;/sup&gt; have only shed partial light on this question: reducing the number of examples per class or the number of classes only results in a small performance drop, while fine-grained classes and more data are not always better.&lt;/p&gt;  

&lt;p&gt;Rather than looking at the data directly, it is more prudent to probe what the models trained on the data learn. It is common knowledge that features of deep neural networks trained on ImageNet &lt;a href="https://arxiv.org/abs/1411.1792"&gt;transition from general to task-specific from the first to the last layer&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn15" id="fnref15"&gt;[15]&lt;/a&gt;&lt;/sup&gt;: lower layers learn to model low-level features such as edges, while higher layers model higher-level concepts such as &lt;a href="https://distill.pub/2017/feature-visualization/"&gt;patterns and entire parts or objects&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn16" id="fnref16"&gt;[16]&lt;/a&gt;&lt;/sup&gt; as can be seen in the figure below. Importantly, knowledge of edges, structures, and the visual composition of objects is relevant for many CV tasks, which sheds light on why these layers are transferred. A key property of an ImageNet-like dataset is thus to encourage a model to learn features that will likely generalize to new tasks in the problem domain.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/feature_visualization.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Visualization of the information captured by features across different layers in GoogLeNet trained on ImageNet. (Source: &lt;a href="https://distill.pub/2017/feature-visualization/"&gt;Distill&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;Beyond this, it is difficult to make further generalizations about why transfer from ImageNet works quite so well. For instance, another possible advantage of the ImageNet dataset is the quality of the data. ImageNet’s creators went to great lengths to ensure reliable and consistent annotations. However, work in distant supervision serves as a counterpoint, indicating that large amounts of weakly labelled data might often be sufficient. In fact, recently researchers at Facebook showed that they could pre-train a model by &lt;a href="https://arxiv.org/abs/1805.00932"&gt;predicting hashtags on billions of social media images&lt;/a&gt; to state-of-the-art accuracy on ImageNet.&lt;/p&gt;  

&lt;p&gt;Without any more concrete insights, we are left with two key desiderata:&lt;/p&gt;  

&lt;ol&gt;  
&lt;li&gt;  
&lt;p&gt;An ImageNet-like dataset should be &lt;em&gt;sufficiently large&lt;/em&gt;, i.e. on the order of millions of training examples.&lt;/p&gt;  
&lt;/li&gt;  
&lt;li&gt;  
&lt;p&gt;It should be &lt;em&gt;representative of the problem space&lt;/em&gt; of the discipline.&lt;/p&gt;  
&lt;/li&gt;  
&lt;/ol&gt;  

&lt;h2 id="animagenetforlanguage"&gt;An ImageNet for language&lt;/h2&gt;  

&lt;p&gt;In NLP, models are typically a lot shallower than their CV counterparts. Analysis of features has thus mostly focused on the first embedding layer, and little work has investigated the properties of higher layers for transfer learning. Let us consider the datasets that are large enough, fulfilling desideratum #1. Given the current state of NLP, there are several contenders&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn17" id="fnref17"&gt;[17]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;  

&lt;p&gt;&lt;strong&gt;Reading comprehension&lt;/strong&gt; is the task of answering a natural language question about a paragraph. The most popular dataset for this task is the &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;Stanford Question Answering Dataset (SQuAD)&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn18" id="fnref18"&gt;[18]&lt;/a&gt;&lt;/sup&gt;, which contains more than 100,000 question-answer pairs and asks models to answer a question by highlighting a span in the paragraph as can be seen below.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/squad_example.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Question-answer pairs for a sample passage in the SQuAD dataset (&lt;a href="https://arxiv.org/abs/1606.05250"&gt;Rajpurkar et al., 2016&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;&lt;strong&gt;Natural language inference&lt;/strong&gt; is the task of identifying the relation (entailment, contradiction, and neutral) that holds between a piece of text and a hypothesis. The most popular dataset for this task, the &lt;a href="https://nlp.stanford.edu/projects/snli/"&gt;Stanford Natural Language Inference (SNLI) Corpus&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn19" id="fnref19"&gt;[19]&lt;/a&gt;&lt;/sup&gt;, contains 570k human-written English sentence pairs. Examples of the dataset can be seen below.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/snli_examples.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Examples from the SNLI dataset. (&lt;a href="https://nlp.stanford.edu/pubs/snli_paper.pdf"&gt;Bowman et al., 2015&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt;, translating text in one language to text in another language, is one of the most studied tasks in NLP, and---over the years---has accumulated vast amounts of training data for popular language pairs, e.g. 40M English-French sentence pairs in WMT 2014. See below for two example translation pairs.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/mt_examples.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;French-to-English translations from newstest2014 (&lt;a href="https://arxiv.org/abs/1710.11041"&gt;Artetxe et al., 2018&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;&lt;strong&gt;Constituency parsing&lt;/strong&gt; seeks to extract the syntactic structure of a sentence in the form of a (linearized) constituency parse tree as can be seen below. In the past, &lt;a href="https://arxiv.org/abs/1412.7449"&gt;millions of weakly labelled parses&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn20" id="fnref20"&gt;[20]&lt;/a&gt;&lt;/sup&gt; have been used for training sequence-to-sequence models for this task.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/constituency_parsing_example.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;A parse tree and its linearization. (&lt;a href="https://arxiv.org/abs/1412.7449"&gt;Vinyals et al., 2015&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;&lt;strong&gt;Language modeling (LM)&lt;/strong&gt; aims to predict the next word given its previous word. Existing benchmark datasets consist of up to &lt;a href="http://www.statmt.org/lm-benchmark/"&gt;1B words&lt;/a&gt;, but as the task is unsupervised, any number of words can be used for training. See below for examples from the popular WikiText-2 dataset consisting of Wikipedia articles.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/language_modeling_examples.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;Examples from the WikiText-2 language modeling dataset. (Source: &lt;a href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset"&gt;Salesforce&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;All of these tasks provide---or would allow the collection of---a sufficient number of examples for training. Indeed, &lt;a href="https://arxiv.org/abs/1705.02364"&gt;the&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn21" id="fnref21"&gt;[21]&lt;/a&gt;&lt;/sup&gt; &lt;a href="https://arxiv.org/abs/1804.00079"&gt;above&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn22" id="fnref22"&gt;[22]&lt;/a&gt;&lt;/sup&gt; &lt;a href="https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors"&gt;tasks&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn23" id="fnref23"&gt;[23]&lt;/a&gt;&lt;/sup&gt; (and many others such as &lt;a href="https://arxiv.org/abs/1708.00524"&gt;sentiment analysis&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn24" id="fnref24"&gt;[24]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1412.7449"&gt;constituency parsing&lt;/a&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn20" id="fnref20:1"&gt;[20:1]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1506.06726"&gt;skip-thoughts&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn25" id="fnref25"&gt;[25]&lt;/a&gt;&lt;/sup&gt;, and &lt;a href="https://arxiv.org/abs/1511.01432"&gt;autoencoding&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn26" id="fnref26"&gt;[26]&lt;/a&gt;&lt;/sup&gt;) have been used to pretrain representations in recent months.&lt;/p&gt;  

&lt;p&gt;While any data contains &lt;a href="https://arxiv.org/abs/1607.06520"&gt;some bias&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn27" id="fnref27"&gt;[27]&lt;/a&gt;&lt;/sup&gt;, human annotators may inadvertently introduce additional signals that a model can exploit. &lt;a href="https://www.aclweb.org/anthology/P16-1223"&gt;Recent&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn28" id="fnref28"&gt;[28]&lt;/a&gt;&lt;/sup&gt; &lt;a href="https://arxiv.org/abs/1803.02324"&gt;studies&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn29" id="fnref29"&gt;[29]&lt;/a&gt;&lt;/sup&gt; reveal that state-of-the-art models for tasks such as reading comprehension and natural language inference do not in fact exhibit deep natural language understanding but pick up on such cues to perform superficial pattern matching. For instance, &lt;a href="https://arxiv.org/abs/1803.02324"&gt;Gururangan et al. (2018)&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn29" id="fnref29:1"&gt;[29:1]&lt;/a&gt;&lt;/sup&gt; show that annotators tend to produce entailment examples simply by removing gender or number information and generate contradictions by introducing negations. A model that simply exploits these cues can correctly classify the hypothesis without looking at the premise in about 67% of the SNLI dataset.&lt;/p&gt;  

&lt;p&gt;The more difficult question thus is: Which task is most representative of the space of NLP problems? In other words, which task allows us to learn most of the knowledge or relations required for understanding natural language?&lt;/p&gt;  

&lt;h2 id="thecaseforlanguagemodelling"&gt;The case for language modelling&lt;/h2&gt;  

&lt;p&gt;In order to predict the most probable next word in a sentence, a model is required not only to be able to express syntax (the grammatical form of the predicted word must match its modifier or verb) but also model semantics. Even more, the most accurate models must incorporate what could be considered &lt;em&gt;world knowledge&lt;/em&gt; or &lt;em&gt;common sense&lt;/em&gt;. Consider the incomplete sentence &amp;quot;The service was poor, but the food was&amp;quot;. In order to predict the succeeding word such as “yummy” or “delicious”, the model must not only memorize what attributes are used to describe food, but also be able to identify that the conjunction “but” introduces a contrast, so that the new attribute has the opposing sentiment of “poor”.&lt;/p&gt;  

&lt;p&gt;Language modelling, the last approach mentioned, has been shown to capture many facets of language relevant for downstream tasks, such as &lt;a href="https://arxiv.org/abs/1611.01368"&gt;long-term dependencies&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn30" id="fnref30"&gt;[30]&lt;/a&gt;&lt;/sup&gt;, &lt;a href="https://arxiv.org/abs/1803.11138"&gt;hierarchical relations&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn31" id="fnref31"&gt;[31]&lt;/a&gt;&lt;/sup&gt;, and &lt;a href="https://arxiv.org/abs/1704.01444"&gt;sentiment&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn32" id="fnref32"&gt;[32]&lt;/a&gt;&lt;/sup&gt;. Compared to related unsupervised tasks such as skip-thoughts and autoencoding, &lt;a href="https://openreview.net/forum?id=BJeYYeaVJ7"&gt;language modelling performs better on syntactic tasks even with less training data&lt;/a&gt;.&lt;/p&gt;  

&lt;p&gt;Among the biggest benefits of language modelling is that training data comes for free with any text corpus and that potentially unlimited amounts of training data are available. This is particularly significant, as NLP deals not only with the English language. More than 4,500 languages are spoken around the world by more than 1,000 speakers. Language modeling as a pretraining task opens the door to developing models for previously underserved languages. For very low-resource languages where even unlabeled data is scarce, multilingual language models may be trained on multiple related languages at once, analogous to work on &lt;a href="https://arxiv.org/abs/1706.04902"&gt;cross-lingual embeddings&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn33" id="fnref33"&gt;[33]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/ulmfit.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;The different stages of ULMFiT. (&lt;a href="https://arxiv.org/abs/1801.06146"&gt;Howard and Ruder, 2018&lt;/a&gt;)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;So far, our argument for language modeling as a pretraining task has been purely conceptual. Pretraining a language model was &lt;a href="https://arxiv.org/abs/1511.01432"&gt;first proposed in 2015&lt;/a&gt; &lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn26" id="fnref26:1"&gt;[26:1]&lt;/a&gt;&lt;/sup&gt;, but it remained unclear whether a single pretrained language model was useful for many tasks. In recent months, we finally obtained overwhelming empirical proof: &lt;a href="https://arxiv.org/abs/1802.05365"&gt;Embeddings from Language Models (ELMo)&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1801.06146"&gt;Universal Language Model Fine-tuning (ULMFiT)&lt;/a&gt;, and the &lt;a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"&gt;OpenAI Transformer&lt;/a&gt; have empirically demonstrated how language modeling can be used for pretraining, as shown by the above figure from ULMFiT. All three methods employed pretrained language models to achieve state-of-the-art on a diverse range of tasks in Natural Language Processing, including text classification, question answering, natural language inference, coreference resolution, sequence labeling, and many others.&lt;/p&gt;  

&lt;p&gt;In many cases such as with ELMo in the figure below, these improvements ranged between 10-20% better than the state-of-the-art on widely studied benchmarks, all with the single core method of leveraging a pretrained language model. ELMo furthermore won the best &lt;a href="https://naacl2018.wordpress.com/2018/04/11/outstanding-papers/"&gt;paper award at NAACL-HLT 2018&lt;/a&gt;, one of the top conferences in the field. Finally, these models have been shown to be extremely sample-efficient, achieving good performance with only hundreds of examples and are even able to perform zero-shot learning.&lt;/p&gt;  

&lt;figure&gt;  
    &lt;center&gt;
&lt;img src="http://ruder.io/content/images/2018/07/elmo.png" width="450" alt="NLP's ImageNet moment has arrived"&gt;  
    &lt;figcaption&gt;The improvements ELMo achieved on a wide range of NLP tasks. (Source: Matthew Peters)&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;  

&lt;p&gt;In light of this step change, &lt;strong&gt;it is very likely that in a year’s time NLP practitioners will download pretrained language models rather than pretrained word embeddings&lt;/strong&gt; for use in their own models, similarly to how pre-trained ImageNet models are the starting point for most CV projects nowadays.&lt;/p&gt;  

&lt;p&gt;However, similar to word2vec, the task of language modeling naturally has its own limitations: It is only a proxy to true language understanding, and a single monolithic model is ill-equipped to capture the required information for certain downstream tasks. For instance, in order to answer questions about or follow the trajectory of characters in a story, a model needs to learn to perform &lt;a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)"&gt;anaphora&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Coreference#Coreference_resolution"&gt;coreference resolution&lt;/a&gt;. In addition, language models can only capture what they have seen. Certain types of information, such as most common sense knowledge, are &lt;a href="https://arxiv.org/abs/1705.11168"&gt;difficult to learn from text alone&lt;/a&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn34" id="fnref34"&gt;[34]&lt;/a&gt;&lt;/sup&gt; and require incorporating external information.&lt;/p&gt;  

&lt;p&gt;One outstanding question is how to transfer the information from a pre-trained language model to a downstream task. The two main paradigms for this are whether to use the pre-trained language model as a fixed feature extractor and incorporate its representation as features into a randomly initialized model as used in &lt;a href="https://arxiv.org/abs/1802.05365"&gt;ELMo&lt;/a&gt;, or whether to fine-tune the entire language model as done by &lt;a href="https://arxiv.org/abs/1801.06146"&gt;ULMFiT&lt;/a&gt;. The latter fine-tuning approach is what is typically done in CV where either the top-most or &lt;a href="https://arxiv.org/abs/1310.1531"&gt;several of the top layers are fine-tuned&lt;/a&gt;. While NLP models are typically more shallow and thus require different fine-tuning techniques than their vision counterparts, recent pretrained models are getting deeper. The next months will show the impact of each of the core components of transfer learning for NLP: an expressive language model encoder such as a deep BiLSTM or the &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Transformer&lt;/a&gt;, the amount and nature of the data used for pretraining, and the method used to fine-tune the pretrained model.&lt;/p&gt;  

&lt;h2 id="butwheresthetheory"&gt;But where’s the theory?&lt;/h2&gt;  

&lt;p&gt;Our analysis thus far has been mostly conceptual and empirical, as it is still poorly understood why models trained on ImageNet---and consequently on language modeling---transfer so well. One way to think about the generalization behaviour of pretrained models more formally is under a model of &lt;em&gt;bias learning&lt;/em&gt; (Baxter, 2000)&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn35" id="fnref35"&gt;[35]&lt;/a&gt;&lt;/sup&gt;. Assume our problem domain covers all permutations of tasks in a particular discipline, e.g. computer vision, which forms our &lt;em&gt;environment&lt;/em&gt;. We are provided with a number of datasets that allow us to induce a &lt;em&gt;family&lt;/em&gt; of hypothesis spaces \(\mathrm{H} = {\mathcal{H}}\). Our goal in bias learning is to find a &lt;em&gt;bias&lt;/em&gt;, i.e. a hypothesis space \(\mathcal{H} \in \mathrm{H}\) that maximizes performance on the entire (potentially infinite) environment.  &lt;/p&gt;

&lt;p&gt;Empirical and theoretical results in multi-task learning (Caruana, 1997; Baxter, 2000)&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn36" id="fnref36"&gt;[36]&lt;/a&gt;&lt;/sup&gt;&lt;sup class="footnote-ref"&gt;&lt;a href="http://ruder.io/nlp-imagenet/#fn35" id="fnref35:1"&gt;[35:1]&lt;/a&gt;&lt;/sup&gt; indicate that a bias that is learned on &lt;em&gt;sufficiently many&lt;/em&gt; tasks is likely to generalize to unseen tasks drawn from the same environment. Viewed through the lens of multi-task learning, a model trained on ImageNet learns a large number of binary classification tasks (one for each class). These tasks, all drawn from the space of natural, real-world images, are likely to be representative of many other CV tasks. In the same vein, a language model---by learning a large number of classification tasks, one for each word---induces representations that are likely helpful for many other tasks in the realm of natural language. Still, much more research is necessary to gain a better theoretical understanding why language modeling seems to work so well for transfer learning.&lt;/p&gt;  

&lt;h2 id="theimagenetmoment"&gt;The ImageNet moment&lt;/h2&gt;  

&lt;p&gt;The time is ripe for practical transfer learning to make inroads into NLP. In light of the impressive empirical results of &lt;a href="https://arxiv.org/abs/1802.05365"&gt;ELMo&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1801.06146"&gt;ULMFiT&lt;/a&gt;, and &lt;a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"&gt;OpenAI&lt;/a&gt; it only seems to be a question of time until pretrained word embeddings will be dethroned and replaced by pretrained language models in the toolbox of every NLP practitioner. This will likely open many new applications for NLP in settings with limited amounts of labeled data. The king is dead, long live the king!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For further reading, check out the &lt;a href="https://news.ycombinator.com/item?id=17489564"&gt;discussion on HackerNews&lt;/a&gt;.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cover image due to Matthew Peters.&lt;/em&gt;&lt;/p&gt;  

&lt;ol class="footnotes-list"&gt;  
&lt;li id="fn1" class="footnote-item"&gt;&lt;p&gt;Peters, Matthew E., et al. &amp;quot;Deep contextualized word representations.&amp;quot; Proceedings of NAACL-HLT (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn2" class="footnote-item"&gt;&lt;p&gt;Howard, Jeremy, and Sebastian Ruder. &amp;quot;Fine-tuned Language Models for Text Classification.&amp;quot; Proceedings of ACL (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref2" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn3" class="footnote-item"&gt;&lt;p&gt;Radford, Alec, et al. &amp;quot;Improving Language Understanding by Generative Pre-Training.&amp;quot; &lt;a href="http://ruder.io/nlp-imagenet/#fnref3" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn4" class="footnote-item"&gt;&lt;p&gt;Mikolov, Tomas, et al. &amp;quot;Distributed representations of words and phrases and their compositionality.&amp;quot; Advances in neural information processing systems. 2013. &lt;a href="http://ruder.io/nlp-imagenet/#fnref4" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn5" class="footnote-item"&gt;&lt;p&gt;Pennington, Jeffrey, Richard Socher, and Christopher Manning. &amp;quot;Glove: Global vectors for word representation.&amp;quot; Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014. &lt;a href="http://ruder.io/nlp-imagenet/#fnref5" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn6" class="footnote-item"&gt;&lt;p&gt;Kim, Yoon. &amp;quot;Convolutional neural networks for sentence classification.&amp;quot; Proceedings of EMNLP (2014). &lt;a href="http://ruder.io/nlp-imagenet/#fnref6" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn7" class="footnote-item"&gt;&lt;p&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &amp;quot;Imagenet classification with deep convolutional neural networks.&amp;quot; Advances in neural information processing systems. 2012. &lt;a href="http://ruder.io/nlp-imagenet/#fnref7" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn8" class="footnote-item"&gt;&lt;p&gt;Donahue, Jeff, et al. &amp;quot;Decaf: A deep convolutional activation feature for generic visual recognition.&amp;quot; International conference on machine learning. 2014. &lt;a href="http://ruder.io/nlp-imagenet/#fnref8" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn9" class="footnote-item"&gt;&lt;p&gt;He, Kaiming, et al. &amp;quot;Mask r-cnn.&amp;quot; Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017. &lt;a href="http://ruder.io/nlp-imagenet/#fnref9" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn10" class="footnote-item"&gt;&lt;p&gt;Zhao, Hengshuang, et al. &amp;quot;Pyramid scene parsing network.&amp;quot; IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2017. &lt;a href="http://ruder.io/nlp-imagenet/#fnref10" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn11" class="footnote-item"&gt;&lt;p&gt;Papandreou, George, et al. &amp;quot;Towards accurate multi-person pose estimation in the wild.&amp;quot; CVPR. Vol. 3. No. 4. 2017. &lt;a href="http://ruder.io/nlp-imagenet/#fnref11" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn12" class="footnote-item"&gt;&lt;p&gt;Carreira, Joao, and Andrew Zisserman. &amp;quot;Quo vadis, action recognition? a new model and the kinetics dataset.&amp;quot; Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017. &lt;a href="http://ruder.io/nlp-imagenet/#fnref12" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn13" class="footnote-item"&gt;&lt;p&gt;Mahajan, Dhruv, et al. &amp;quot;Exploring the Limits of Weakly Supervised Pretraining.&amp;quot; arXiv preprint arXiv:1805.00932 (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref13" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn14" class="footnote-item"&gt;&lt;p&gt;Huh, Minyoung, Pulkit Agrawal, and Alexei A. Efros. &amp;quot;What makes ImageNet good for transfer learning?.&amp;quot; arXiv preprint arXiv:1608.08614 (2016). &lt;a href="http://ruder.io/nlp-imagenet/#fnref14" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn15" class="footnote-item"&gt;&lt;p&gt;Yosinski, Jason, et al. &amp;quot;How transferable are features in deep neural networks?.&amp;quot; Advances in neural information processing systems. 2014. &lt;a href="http://ruder.io/nlp-imagenet/#fnref15" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn16" class="footnote-item"&gt;&lt;p&gt;Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. &amp;quot;Feature visualization.&amp;quot; Distill 2.11 (2017): e7. &lt;a href="http://ruder.io/nlp-imagenet/#fnref16" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn17" class="footnote-item"&gt;&lt;p&gt;For a comprehensive overview of progress in NLP tasks, you can refer to this &lt;a href="https://github.com/sebastianruder/NLP-progress"&gt;GitHub repository&lt;/a&gt;. &lt;a href="http://ruder.io/nlp-imagenet/#fnref17" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn18" class="footnote-item"&gt;&lt;p&gt;Rajpurkar, Pranav, et al. &amp;quot;Squad: 100,000+ questions for machine comprehension of text.&amp;quot; Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP, 2016). &lt;a href="http://ruder.io/nlp-imagenet/#fnref18" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn19" class="footnote-item"&gt;&lt;p&gt;Bowman, Samuel R., et al. &amp;quot;A large annotated corpus for learning natural language inference.&amp;quot; Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP, 2015). &lt;a href="http://ruder.io/nlp-imagenet/#fnref19" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn20" class="footnote-item"&gt;&lt;p&gt;Vinyals, Oriol, et al. &amp;quot;Grammar as a foreign language.&amp;quot; Advances in Neural Information Processing Systems. 2015. &lt;a href="http://ruder.io/nlp-imagenet/#fnref20" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="http://ruder.io/nlp-imagenet/#fnref20:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn21" class="footnote-item"&gt;&lt;p&gt;Conneau, Alexis, et al. &amp;quot;Supervised learning of universal sentence representations from natural language inference data.&amp;quot; Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP, 2017). &lt;a href="http://ruder.io/nlp-imagenet/#fnref21" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn22" class="footnote-item"&gt;&lt;p&gt;Subramanian, Sandeep, et al. &amp;quot;Learning general purpose distributed sentence representations via large scale multi-task learning.&amp;quot; Proceedings of the International Conference on Learning Representations, ICLR (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref22" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn23" class="footnote-item"&gt;&lt;p&gt;McCann, Bryan, et al. &amp;quot;Learned in translation: Contextualized word vectors.&amp;quot; Advances in Neural Information Processing Systems. 2017. &lt;a href="http://ruder.io/nlp-imagenet/#fnref23" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn24" class="footnote-item"&gt;&lt;p&gt;Felbo, Bjarke, et al. &amp;quot;Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.&amp;quot; Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP, 2017). &lt;a href="http://ruder.io/nlp-imagenet/#fnref24" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn25" class="footnote-item"&gt;&lt;p&gt;Kiros, Ryan, et al. &amp;quot;Skip-thought vectors.&amp;quot; Advances in neural information processing systems. 2015. &lt;a href="http://ruder.io/nlp-imagenet/#fnref25" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn26" class="footnote-item"&gt;&lt;p&gt;Dai, Andrew M., and Quoc V. Le. &amp;quot;Semi-supervised sequence learning.&amp;quot; Advances in neural information processing systems. 2015. &lt;a href="http://ruder.io/nlp-imagenet/#fnref26" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn27" class="footnote-item"&gt;&lt;p&gt;Bolukbasi, Tolga, et al. &amp;quot;Man is to computer programmer as woman is to homemaker? debiasing word embeddings.&amp;quot; Advances in Neural Information Processing Systems. 2016. &lt;a href="http://ruder.io/nlp-imagenet/#fnref27" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn28" class="footnote-item"&gt;&lt;p&gt;Chen, Danqi, Jason Bolton, and Christopher D. Manning. &amp;quot;A thorough examination of the cnn/daily mail reading comprehension task.&amp;quot; Proceedings of the Meeting of the Association for Computational Linguistics (2016). &lt;a href="http://ruder.io/nlp-imagenet/#fnref28" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn29" class="footnote-item"&gt;&lt;p&gt;Gururangan, Suchin, et al. &amp;quot;Annotation artifacts in natural language inference data.&amp;quot; Proceedings of NAACL-HLT (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref29" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="http://ruder.io/nlp-imagenet/#fnref29:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn30" class="footnote-item"&gt;&lt;p&gt;Linzen, T., Dupoux, E., &amp;amp; Goldberg, Y. (2016). Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Proceedings of TACL 2016. &lt;a href="http://ruder.io/nlp-imagenet/#fnref30" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn31" class="footnote-item"&gt;&lt;p&gt;Gulordava, K., Bojanowski, P., Grave, E., Linzen, T., &amp;amp; Baroni, M. (2018). Colorless green recurrent networks dream hierarchically. In Proceedings of NAACL-HLT 2018. &lt;a href="http://ruder.io/nlp-imagenet/#fnref31" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn32" class="footnote-item"&gt;&lt;p&gt;Radford, A., Jozefowicz, R., &amp;amp; Sutskever, I. (2017). Learning to Generate Reviews and Discovering Sentiment. arXiv preprint arXiv:1704.01444. &lt;a href="http://ruder.io/nlp-imagenet/#fnref32" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn33" class="footnote-item"&gt;&lt;p&gt;Ruder, Sebastian, Ivan Vulic, and Anders Søgaard. &amp;quot;A Survey of Cross-lingual Word Embedding Models.&amp;quot; Journal of Artificial Intelligence Research (2018). &lt;a href="http://ruder.io/nlp-imagenet/#fnref33" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn34" class="footnote-item"&gt;&lt;p&gt;Lucy, Li, and Jon Gauthier. &amp;quot;Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning.&amp;quot; Proceedings of the First Workshop on Language Grounding for Robotics (2017). &lt;a href="http://ruder.io/nlp-imagenet/#fnref34" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn35" class="footnote-item"&gt;&lt;p&gt;Jonathan Baxter. 2000. A Model of Inductive Bias Learning. Journal of Artificial Intelligence Research 12:149–198. &lt;a href="http://ruder.io/nlp-imagenet/#fnref35" class="footnote-backref"&gt;↩︎&lt;/a&gt; &lt;a href="http://ruder.io/nlp-imagenet/#fnref35:1" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;li id="fn36" class="footnote-item"&gt;&lt;p&gt;Rich Caruana. 1993. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning. &lt;a href="http://ruder.io/nlp-imagenet/#fnref36" class="footnote-backref"&gt;↩︎&lt;/a&gt;&lt;/p&gt;  
&lt;/li&gt;  
&lt;/ol&gt;</content:encoded></item><item><title>Tracking the Progress in Natural Language Processing</title><description>This post presents a new resource to track the progress in NLP, including the datasets and the current state-of-the-art for the most common NLP tasks.</description><link>http://ruder.io/tracking-progress-nlp/</link><guid isPermaLink="false">b5a914f8-54d9-4a2e-afc3-6c66dff83a97</guid><category>nlp</category><category>natural language processing</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Fri, 22 Jun 2018 17:31:55 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a href="https://nlpprogress.com"&gt;Go directly to the document tracking the progress in NLP&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Research in Machine Learning and in Natural Language Processing (NLP) is moving so fast these days, it is hard to keep up. This is an issue for people in the field, but it is an even bigger obstacle for people wanting to get into NLP and those seeking to make the leap from tutorials to reproducing papers and conducting their own research. Without expert guidance and prior knowledge, it can be a painstaking process to identify the most common datasets and the current state-of-the-art for your task of interest.&lt;/p&gt;

&lt;p&gt;A number of resources exist that could help with this process, but each has deficits: The Association of Computation Linguistics (ACL) has a &lt;a href="https://aclweb.org/aclwiki/State_of_the_art"&gt;wiki page&lt;/a&gt; tracking the state-of-the-art, but the page is not maintained and contributing is not straightforward. The &lt;a href="https://www.eff.org/ai/metrics"&gt;Electronic Frontier Foundation&lt;/a&gt; and the &lt;a href="https://aiindex.org/"&gt;AI Index&lt;/a&gt; try to do something similar for all of AI but only cover a few language tasks. The &lt;a href="http://lremap.elra.info/"&gt;Language Resources and Evaluation (LRE) Map&lt;/a&gt; collects language resources presented at LREC and other conferences, but does not allow to break them out by tasks or popularity. Similarly, the &lt;a href="http://alt.qcri.org/semeval2018/index.php?id=tasks"&gt;International Workshop on Semantic Evaluation (SemEval)&lt;/a&gt; hosts a small number of tasks each year, which provide new datasets that typically have not been widely studied before. There are also resources that focus on &lt;a href="http://rodrigob.github.io/are_we_there_yet/build/"&gt;computer vision&lt;/a&gt; and &lt;a href="https://github.com/syhw/wer_are_we"&gt;speech recognition&lt;/a&gt; as well as &lt;a href="https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems#nlp"&gt;this repo&lt;/a&gt;, which focuses on all of ML.&lt;/p&gt;

&lt;p&gt;As an alternative, I have created a &lt;a href="https://github.com/sebastianruder/NLP-progress"&gt;GitHub repository&lt;/a&gt; that keeps track of the datasets and the current state-of-the-art for the most common tasks in NLP. The repository is kept as simple as possible to make maintenance and contribution easy. If I missed your favourite task or dataset or your new state-of-the-art result or if I made any error, you can simply submit a pull request.&lt;/p&gt;

&lt;p&gt;The aim is to have a comprehensive and up-to-date resource where everyone can see at a glance the state-of-the-art for the tasks they care about. Datasets, which already do a great job at tracking this such as &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;SQuAD&lt;/a&gt; or &lt;a href="https://nlp.stanford.edu/projects/snli/"&gt;SNLI&lt;/a&gt; using a public leaderboard will simply be referenced instead.&lt;/p&gt;

&lt;p&gt;My hope is that such a resource will give a broader sense of progress in the field than results in individual papers. It might also make it easier to identify tasks or areas where progress has been lacking. Another benefit is that such a resource may encourage serendipity: chancing upon an interesting new task or method. Finally, a positive by-product of having the state-of-the-art for each task easily accessible may be that it will be harder to justify (accidentally) comparing to weak baselines. For instance, the perplexity of the best baseline on the Penn Treebank varied dramatically across 10 language modeling papers submitted to ICLR 2018 (see below).&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/ppl_language_modeling_iclr_2018.png" style="width: 70%" title="Learning rate schedules with warm restarts"&gt;
&lt;figcaption&gt;Figure 1: Comparison of perplexity (PPL) of proposed model vs. PPL of best baseline across 10 language modeling papers submitted to ICLR 2018 (credit: &lt;a href="https://twitter.com/AaronJaech/status/924678973497446400"&gt;@AaronJaech&lt;/a&gt;)&lt;/figcaption&gt;  
&lt;/figure&gt;</content:encoded></item><item><title>Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems</title><description>Highlights of NAACL-HLT 2018 focusing on Generalization, the Test-of-Time awards, and Dialogue Systems, including memorable quotes from the conference.</description><link>http://ruder.io/highlights-naacl-2018/</link><guid isPermaLink="false">3f5db7de-65f0-4dd6-869e-7bf911b8ca0f</guid><category>natural language processing</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Tue, 12 Jun 2018 13:42:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/06/IMG_20180603_183707_conference_venue.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2018/06/IMG_20180603_183707_conference_venue.jpg" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/highlights-of-naacl-hlt-2018-generalization-test-of-time-and-dialogue-systems/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I attended &lt;a href="http://naacl2018.org/"&gt;NAACL-HLT 2018&lt;/a&gt; in New Orleans last week. I didn’t manage to catch as many talks and posters this time around (there were just too many inspiring people to talk to!), so my highlights and the trends I observed mainly focus on invited talks and workshops.&lt;/p&gt;

&lt;p&gt;Specifically, my highlights concentrate on three topics, which were prominent throughout the conference: Generalization, the Test-of-Time awards, and Dialogue Systems. For more information about other topics, you can check out the &lt;a href="http://naacl2018.org/downloads/naacl_hlt_2018_handbook.pdf"&gt;conference handbook&lt;/a&gt; and the &lt;a href="https://aclanthology.coli.uni-saarland.de/events/naacl-2018"&gt;proceedings&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First of all, there were four quotes from the conference that particularly resonated with me (some of them are paraphrased):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;People worked on MT before the BLEU score&lt;/em&gt;. - Kevin Knight. It’s natural to work on tasks where evaluation is easy. Instead, we should encourage more people to tackle hard problems that are not easy to evaluate. These are often the most rewarding to work on.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;BLEU is an understudy. It was never meant to replace human judgement and never expected to last this long&lt;/em&gt;. - Kishore Papineni, co-creator of BLEU, the most commonly used metric for machine translation. No approach is perfect. Even the authors of landmark papers were aware their methods had flaws. The best we can do is provide a fair evaluation of our technique.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;We decided to sample an equal number of positive and negative reviews---was that a good idea?&lt;/em&gt; - Bo Pang, first author of one of the first papers on sentiment analysis (7k+ citations). In addition to being aware of the flaws of our method, we should be explicit about the assumptions we make so that future work can either build upon them or discard them if they prove unhelpful or turn out to be false.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I pose the following challenge to the community: we should evaluate on out-of-distribution data or on a new task&lt;/em&gt;. - Percy Liang.
We never know how well our model truly generalizes if we just test it on data of the same distribution. In order to develop models that can be applied to the real world, we need to evaluate on out-of-distribution data or on a new task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Percy Liang’s quote ties into one of the topics that received increasing attention at the conference: how can we train models that are less brittle and that generalize better?&lt;/p&gt;

&lt;h2 id="generalization"&gt;Generalization&lt;/h2&gt;

&lt;p&gt;Over the last years, much of the research within the NLP community focused on improving LSTM-based models on benchmark tasks. At NAACL, it seemed that increasingly people were thinking about how to get models to generalize beyond the conditions during training, reflecting a similar sentiment in the Deep Learning community in general (a &lt;a href="https://arxiv.org/abs/1611.03530"&gt;paper on generalization&lt;/a&gt; won the best paper award at ICLR 2017).&lt;/p&gt;

&lt;p&gt;One aspect is generalizing from few examples, which is difficult with the current generation of neural network-based methods. Charles Yang, professor of Linguistics, Computer Science and Psychology at the University of Pennsylvania put this in a cognitive science context.&lt;/p&gt;

&lt;p&gt;Machine learning and NLP researchers in the neural network era frequently like to motivate their work by referencing the remarkable generalization ability of young children. One piece of information that often is eluded, however, is that generalization in young children is also not without its errors, because it requires learning a rule and accounting for exceptions. For instance, when learning to count, young children still frequently make mistakes, as they have to balance rule-based generalization (for regular numbers such as sixteen, seventeen, eighteen, etc.) with memorizing exceptions (numbers such as fifteen, twenty, thirty, etc.).&lt;/p&gt;

&lt;p&gt;However, once a child can flawlessly count to 72, it &lt;a href="https://penntoday.upenn.edu/research/penn-linguist-determines-tipping-point-for-children-learning-to-count"&gt;can generalize to any new numbers&lt;/a&gt;. This magic number, 72, is given by the so-called tolerance principle, which prescribes that in order for a generalization rule to be productive, there can be at most N/ln(N) exceptions, where N is the total number of examples as can be seen in the Figure below. For counting, 72/ln(72) ≈ 17, which is exactly the number of exceptions until 72.&lt;/p&gt;

&lt;p&gt;Hitchhiker’s Guide to the Galaxy fans, however, need not be disappointed: in Chinese, the magic number is 42. Chinese only has 11 exceptions. As 42/ln(42) ≈ 11, Chinese children typically only need to learn to count up to 42 in order to generalize, which explains why Chinese children usually learn to count faster.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180602_092025_tolerance_principle.jpg" style="width: 70%" title="Tolerance principle" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 1: The Tolerance Principle&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;It is also interesting to note that even though young children can count up to a certain number, they can’t tell you, for example, which number is higher than 24. Only after they’ve learned the rule can they actually apply it productively.&lt;/p&gt;

&lt;p&gt;The tolerance principle implies that if the total number of examples covered by the rule is smaller, it is easier to incorporate comparatively more exceptions. While children can productively learn language from few examples, this indicates that for few-shot learning (at least in the cognitive process of language acquisition), big data may actually be harmful. Insights from cognitive science may thus help us in developing models that generalize better.&lt;/p&gt;

&lt;p&gt;The choice of the best paper of the conference, &lt;a href="https://arxiv.org/abs/1802.05365"&gt;Deep contextualized word representations&lt;/a&gt;, also demonstrates an increasing interest in generalization. Embeddings from Language Models (ELMo) showed significant improvements over the state-of-the-art on a wide range of tasks as can be seen below. This---together with &lt;a href="https://arxiv.org/abs/1801.06146"&gt;better ways to fine-tune pre-trained language models&lt;/a&gt;---reveals the potential of transfer learning for NLP.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180604_160920_elmo.jpg" style="width: 70%" title="ELMo improvements" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 2: Improvements by ELMo on six diverse NLP tasks&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Generalization was also the topic of the &lt;a href="https://newgeneralization.github.io/"&gt;Workshop on New Forms of Generalization in Deep Learning and Natural Language Processing&lt;/a&gt;, which sought to analyze the failings of brittle models and propose new ways to evaluate and new models that would enable better generalization. Throughout the workshop, the room (seen below) was packed most of the time, which is both a testament to the prestige of the speakers and the community interest in the topic.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180605_110109_gendeep_room.jpg" style="width: 70%" title="Gen-Deep 2018" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 3: The packed venue of the Gen-Deep Workshop&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In the &lt;a href="https://newgeneralization.github.io/slides/YejinChoi.pdf"&gt;first talk of the workshop&lt;/a&gt;, Yejin Choi argued that natural language understanding (NLU) does not generalize to natural language generation (NLG): while pre-Deep Learning NLG models often started with NLU output, post-DL NLG seems less dependent on NLU. However, current neural NLG heavily depends on language models and neural NLG can be brittle; in many cases, baselines based on templates can actually work better.&lt;/p&gt;

&lt;p&gt;Despite advances in NLG, generating a coherent paragraph still does not work well and models end up generating generic, contentless, bland text full of repetitions and contradictions. But if we feed our models natural language input, why do they produce unnatural language output?&lt;/p&gt;

&lt;p&gt;Yejin identified two limitations of language models in this context. Language models are passive learners: in the real world, one can’t learn to write just by reading; and similarly, she argued, even RNNs need to “practice” writing. Secondly, language models are surface learners: they need “world” models and must be sensitive to the “latent process” behind language. In reality, people don’t write to maximize the probability of the next token, but rather seek to fulfill certain communicative goals.&lt;/p&gt;

&lt;p&gt;To address this, Yejin and collaborators proposed in an &lt;a href="https://arxiv.org/abs/1805.06087"&gt;upcoming ACL 2018 paper&lt;/a&gt; to augment the generator with multiple discriminative models that grade the output along different dimensions inspired by Grice’s maxims.&lt;/p&gt;

&lt;p&gt;Yejin also sought to explain why there is a significant performance gaps between different NLU tasks such as machine translation and dialogue. For Type 1 or shallow NLU tasks, there is a strong alignment between input and output and models can often match surface patterns. For Type 2 or deep NLU tasks, the alignment between input and output is weaker; in order to perform well, a model needs to be able to abstract and reason, and requires certain types of knowledge, especially common sense knowledge. In particular, commonsense knowledge has somewhat fallen out of favour; past approaches, which were mostly proposed in the 80s, did not have access to a lot of computing power and were mostly done by non-NLP people. Overall, NLU traditionally focuses on understanding only “natural” language, while NLG also requires understanding machine language, which may be unnatural.&lt;/p&gt;

&lt;p&gt;Devi Parikh discussed &lt;a href="https://newgeneralization.github.io/slides/DeviParikh.pptx"&gt;generalization “opportunities” in visual question answering&lt;/a&gt; (VQA) and illustrated successes and failures of VQA models. In particular, VQA models are not very good at generalizing to novel instances; the distance of a test image from the k-nearest neighbours seen during training can predict the success or failure of a model with about 67% accuracy.&lt;/p&gt;

&lt;p&gt;Devi also showed that in many cases, VQA models do not even consider the entire question: in 50% of cases, only half the question is read. In particular, certain prefixes demonstrate the power of language priors: if the question begins with “Is there a clock…?”, the answer is “yes” 98% of the time; if the question begins with “Is the man wearing glasses…?”, the answer is “yes” 94% of the time. In order to counteract these biases, Devi and her collaborators introduced a &lt;a href="https://arxiv.org/abs/1612.00837"&gt;new dataset of complimentary scenes&lt;/a&gt;, which are very similar but differ in their answers. They also proposed a &lt;a href="https://arxiv.org/abs/1712.00377"&gt;new setting for VQA&lt;/a&gt; where for every question type, train and test sets have different prior distributions of answers.&lt;/p&gt;

&lt;p&gt;The final discussion with senior panelists (seen below) was arguably the highlight of the workshop; a summary can be found &lt;a href="https://twitter.com/seb_ruder/status/1004110419563286529"&gt;here&lt;/a&gt;. The main takeaways are that we need to develop models with inductive biases and that we need to do a better job of educating people on how to design experiments and identify biases in datasets.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180605_161710_panel.jpg" style="width: 100%" title="Gen-Deep 2018 panel" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 4: Panel discussion at the Generalization workshop (from left to right: Chris Manning, Percy Liang, Sam Bowman, Yejin Choi, Devi Parikh)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="testoftimeawards"&gt;Test-of-time awards&lt;/h2&gt;

&lt;p&gt;Another highlight of the conference was the test-of-time awards session, which highlighted persons and papers that had a huge impact on the field. At the beginning of the session, Aravind Yoshi (see below), NLP pioneer and professor of Computer Science at the University of Pennsylvania, who passed away on December 31, 2017 was honored in touching epitaphs by close friends and people who knew him. The commemoration was a powerful reminder that research is about more than the papers we publish, but about the people we help and the lives we touch.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180603_170850_aravind_joshi.jpg" style="width: 70%" title="Aravind Joshi" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 5: Aravind Joshi&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Afterwards, three papers (all published in 2002) were honored with &lt;a href="https://naacl2018.wordpress.com/2018/03/22/test-of-time-award-papers/"&gt;test-of-time awards&lt;/a&gt;. For each paper, one of the original authors presented the paper and reflected on its impact. The first paper presented was &lt;a href="https://www.aclweb.org/anthology/P02-1040.pdf"&gt;BLEU: a Method for Automatic Evaluation of Machine Translation&lt;/a&gt;, which introduced the original BLEU metric, now commonplace in machine translation (MT). Kishore Papineni recounted that the name was inspired by &lt;a href="http://www.cs.cmu.edu/~aberger/mt.html"&gt;Candide&lt;/a&gt;, an experimental MT system at IBM in the early 1990s and by IBM’s nickname, Big Blue, as all authors were at IBM at that time.&lt;/p&gt;

&lt;p&gt;Before BLEU, machine translation evaluation was cumbersome, expensive, and thought to be as difficult as training an MT model. Despite its huge impact, BLEU’s creators were apprehensive before its initial publication. Once published, BLEU seemed to split the community in two camps: those who loved it, and those who hated it; the authors hadn’t expected such a strong reaction.&lt;/p&gt;

&lt;p&gt;BLEU is still criticized today. It was meant as a corpus-level metric; individual sentence errors should be averaged out across the entire corpus. Kishore conceded that in hindsight, they made a few mistakes: they should have included smoothing and statistical significance testing; an initial version was also case insensitive, which caused confusion.&lt;/p&gt;

&lt;p&gt;In summary, BLEU has many known limitations and inspired many colorful variants. On the whole, however, it is an understudy (as the acronym BiLingual Evaluation Understudy implies): it was never meant to replace human judgement and---notably---was never expected to last this long.&lt;/p&gt;

&lt;p&gt;The second honored paper was &lt;a href="http://www.aclweb.org/anthology/W02-1001"&gt;Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms&lt;/a&gt; by Michael Collins, which introduced the Structured Perceptron, one of the foundational and easiest to understand algorithms for general structured prediction.&lt;/p&gt;

&lt;p&gt;Lastly, Bo Pang looked back on her paper &lt;a href="http://www.aclweb.org/anthology/W02-1011"&gt;Thumbs up? Sentiment Classification using Machine Learning Techniques&lt;/a&gt;, which was the first paper of her PhD and one of the first papers on sentiment analysis, now an active research area in the NLP community. Prior to the paper, people had worked on classifying the subjectivity of sentences and the semantic orientation (polarity) of adjectives; sentiment classification was thus a natural progression.&lt;/p&gt;

&lt;p&gt;Over the years, the paper has accumulated more than 7,000 citations. One reason why the paper was so impactful was that the authors decided to release a dataset with it. Bo was critical of the sampling choices they made that “messed” with the natural distribution of the data: they capped the number of reviews of prolific authors, which was probably a good idea. However, they sampled an equal number of positive and negative reviews, which set the standard that many approaches later followed and is still the norm for sentiment analysis today. A better idea might have been to stay true to the natural distribution of the data.&lt;/p&gt;

&lt;p&gt;I found the test-of-time award session both insightful and humbling: we can derive many insights from traditional approaches and combining traditional with more or recent approaches is often a useful direction; at the same time, even the authors of landmark papers are critical of their own work and aware of its own flaws.&lt;/p&gt;

&lt;h2 id="dialoguesystems"&gt;Dialogue systems&lt;/h2&gt;

&lt;p&gt;Another pervasive topic at the conference was dialogue systems. On the first day, researchers from PolyAI gave an excellent &lt;a href="https://www.poly-ai.com/naacl18"&gt;tutorial on Conversational AI&lt;/a&gt;. On the second day, Mari Ostendorf, professor of Electrical Engineering at the University of Washington and faculty advisor to the Sounding Board team, which won the inaugural Alexa Prize competition, shared some of the secrets to their winning socialbot. A socialbot in this context is a bot with which you can have a conversation, in contrast to a personal assistant that is designed to accomplish user-specified goals.&lt;/p&gt;

&lt;p&gt;A good socialbot should have the same qualities as someone you enjoy talking to at a cocktail party: it should have something interesting to say and show interest in the conversation partner. To illustrate this, an example conversation with the Sounding Board bot can be seen below.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180602_141024_alexa_prize_conversation.jpg" style="width: 70%" title="Sounding Board bot conversation" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 6: An example conversation with the Sounding Board bot&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;With regard to saying something interesting, the team found that users react positively to learning something new but negatively to old or unpleasant news; a challenge here is to filter what to present to people. In addition, users lose interest when they receive too much content that they do not care about.&lt;/p&gt;

&lt;p&gt;Regarding expressing interest, users appreciate an acknowledgement of their reactions and requests. While some users need encouragement to express opinions, some prompts can be annoying (“The article mentioned Google. Have you heard of Google?”).&lt;/p&gt;

&lt;p&gt;They furthermore found that modeling prosody is important. Prosody deals with modeling the patterns of stress and intonation in speech. Instead of sounding monotonous, a bot that incorporates prosody seems more engaging, can better articulate intent, communicate sentiment or sarcasm, express empathy or enthusiasm, or change the topic. In some cases, prosody is also essential for avoiding---often hilarious---misunderstandings: for instance, the Alexa’s default voice pattern for ‘Sounding Board’ sounds like ‘Sounding Bored’.&lt;/p&gt;

&lt;p&gt;Using a knowledge graph, the bot can have deeper conversations by staying “sort of on topic without totally staying on topic”. Mari also shared four key lessons that they learned working with 10M conversations:&lt;/p&gt;

&lt;h3 id="lesson1asrisimperfect"&gt;Lesson #1: ASR is imperfect&lt;/h3&gt;

&lt;p&gt;While automatic speech recognition (ASR) has reached lower and lower word error rates in recent years, ASR is far from being solved. In particular, ASR in dialogue agents is tuned for short commands, not conversational speech. Whereas it can accurately identify for instance many obscure music groups, it struggles with more conversational or informal input. In addition, current development platforms provide developers with an impoverished representation of speech that does not contain segmentation or prosody and often misses certain intents and affects. Problems caused by this missing information can be seen below.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180602_142859_prosody.jpg" style="width: 70%" title="Problems caused by missing prosody information" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 7: Problems caused by missing prosody information and missing punctuation&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In practice, the Sounding Board team found it helpful for the socialbot to behave similarly to attendees at a cocktail party: if the intent of the entire utterance is not completely clear, the bot responds to whatever part it understood, e.g. to a somewhat unintelligible “cause does that you’re gonna state that’s cool” it might respond “I’m happy you like that.” They also found that often, asking the conversation partner to repeat an utterance will not yield a much better result; instead, it is often better just to change the topic.&lt;/p&gt;

&lt;h3 id="lesson2usersvary"&gt;Lesson #2: Users vary&lt;/h3&gt;

&lt;p&gt;The team discovered something what might seem obvious but has wide-ranging consequences in practice: users vary a lot across different dimensions. Users have different interests, different opinions on issues, and a different sense of humor. Interaction styles, similarly, can range from terse to verbose (seen below), from polite to rude. Users can also interact with the bot in pursuit of widely different goals: they may seek information, intend to share opinions, try to get to know the bot, or seek to explore the limitations of the bot. Modeling the user involves both determining what to say as well as listening to what the user says.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180602_143521_interaction_styles.jpg" style="width: 70%" title="Interaction styles of talkative and terse users" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 8: Interaction styles of talkative and terse users&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h3 id="lesson3itsawildworld"&gt;Lesson #3: It’s a wild world&lt;/h3&gt;

&lt;p&gt;There is a lot of problematic content and many issues exist that need to be navigated. Problematic content can consist of offensive or controversial material or sensitive and depressing topics. In addition, users might act adversarially and deliberately try to get the bot to produce such content. Examples of such behaviour can be seen below. Other users (e.g. such as those suffering from a mental illness) are in turn risky to deal with. Overall, filtering content is a hard problem. As one example, Mari mentioned that early in the project, the bot made the following witty observation / joke: “You know what I realized the other day? Santa Claus is the most elaborate lie ever told”.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180602_143955_adversarial_users.jpg" style="width: 70%" title="Adversarial user examples" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 9: Adversarial user examples&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h3 id="lesson4shallowconversations"&gt;Lesson #4: Shallow conversations&lt;/h3&gt;

&lt;p&gt;As the goal of the Alexa Prize was to maintain a conversation of 20 minutes, in light of the current limited understanding and generation capabilities, the team focused on a dialog strategy of shallow conversations. Even for shallow conversations, however, switching to related topics can still be fragile due to word sense ambiguities. It is notable that among the top 3 Alexa Prize teams, &lt;a href="https://arxiv.org/abs/1712.07558"&gt;Deep Learning was only used by one team and only for reranking&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In total, a competition such as the Alexa Prize that brings academia and industry together is useful as it allows researchers to access data from real users at a large scale, which impacts the problems they choose to solve and the resulting solutions. It also teaches students about complete problems and real-world challenges and provides funding support for students. The team found it in particular beneficial someone from industry available to support the partnership, to provide advice on tools, and feedback on progress.&lt;/p&gt;

&lt;p&gt;On the other hand, privacy-preserving access to user data, such as prosody info for spoken language and speaker/author demographics for text and speech still needs work. For spoken dialog systems, richer speech interfaces are furthermore necessary. Finally, while competitions are great kickstarters, they nevertheless require a substantial engineering effort.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Finally, in her keynote address on dialogue models, Dilek Hakkani-Tür, Research Scientist at Google Research, argued that over the recent years, chitchat systems and task-oriented dialogue systems have been converging. However, current production systems are essentially a walled garden of domains and only allow directed dialogue and limited personalization. Models have to be learned from developers using a limited set of APIs and tools and are hard to scale. At the same time, conversation is a skill even for humans. It is thus important to learn from real users, not from developers.&lt;/p&gt;

&lt;p&gt;In order to learn &lt;i&gt;about&lt;/i&gt; users, we can leverage personal knowledge graphs learned from user assertions, such as “show me directions to my daughter’s school”. Semantic recall, i.e. remembering entities from previous user interactions, e.g. “Do you remember the restaurant we ordered Asian food from?” is important. Personalized natural language understanding can also leverage data from user’s device (in a privacy-preserving manner) and employ user modeling for dialogue generation.&lt;/p&gt;

&lt;p&gt;For learning &lt;i&gt;from&lt;/i&gt; users, actions can be learned from user demonstration and/or explanation or from experience and feedback (mostly using RL for dialogue systems). In both cases, transcription and annotation are bottlenecks. A user can’t be expected to transcribe or annotate data; on the other hand, it is easier to give feedback after the system repeats an utterance.&lt;/p&gt;

&lt;p&gt;Generally, task-oriented dialogue can be treated as a game between two parties (see below): the seeker has a goal, which is fixed or flexible, while the provider has access to APIs to perform the task. The dialogue policy of the seeker decides the next seeker action. This is typically determined using “user simulators”, which are often sequence-to-sequence models. Most recently, hierarchical sequence-to-sequence models have been used with a focus on following the user goals and generating diverse responses.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180604_091136_task-oriented_dialogue_game.jpg" style="width: 70%" title="Task-oriented dialogue as a game" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 10: Task-oriented dialogue as a game&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The dialogue policy of the provider similarly determines the next provider action, which is realized either via supervised or reinforcement learning (RL). For RL, reward estimation and policy shaping are important. Recent approaches jointly learn seeker and provider policies. End-to-end dialogue models with deep RL are critical for learning from user feedback, while component-wise training benefits from additional data for each component.&lt;/p&gt;

&lt;p&gt;In practice, a combination of supervised and reinforcement learning is best and outperforms both purely supervised learning and supervised learning with policy-only RL as can be seen below.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2018/06/IMG_20180604_094901_supervised-rl.jpg" style="width: 70%" title="Supervised and reinforcement learning for dialogue modeling" alt="Highlights of NAACL-HLT 2018: Generalization, Test-of-time, and Dialogue Systems"&gt;
&lt;figcaption&gt;Figure 11: Supervised and reinforcement learning for dialogue modeling&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Overall, the conference was a great opportunity to see fantastic research and meet great people. See you all at ACL 2018!&lt;/p&gt;</content:encoded></item><item><title>An overview of proxy-label approaches for semi-supervised learning</title><description>&lt;p&gt;Note: Parts of this post are based on my ACL 2018 paper &lt;a href="https://arxiv.org/abs/1804.09530"&gt;Strong Baselines for Neural Semi-supervised Learning under Domain Shift&lt;/a&gt; with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Plank&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selftraining"&gt;Self-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#multiviewtraining"&gt;Multi-view training&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Co-training&lt;/li&gt;
&lt;li&gt;Democratic Co-learning&lt;/li&gt;
&lt;li&gt;Tri-training&lt;/li&gt;
&lt;li&gt;Tri-training with disagreement&lt;/li&gt;
&lt;li&gt;Asymmetric tri-training&lt;/li&gt;
&lt;li&gt;Multi-task tri-training&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selfensembling"&gt;Self-ensembling&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Ladder networks&lt;/li&gt;
&lt;li&gt;Virtual Adversarial Training&lt;/li&gt;
&lt;li&gt;\(\Pi\) model&lt;/li&gt;
&lt;li&gt;Temporal&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><link>http://ruder.io/semi-supervised/</link><guid isPermaLink="false">da237af8-8674-4d25-af3f-7290029c6247</guid><category>deep learning</category><category>semi-supervised learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Thu, 26 Apr 2018 07:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Note: Parts of this post are based on my ACL 2018 paper &lt;a href="https://arxiv.org/abs/1804.09530"&gt;Strong Baselines for Neural Semi-supervised Learning under Domain Shift&lt;/a&gt; with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Plank&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selftraining"&gt;Self-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#multiviewtraining"&gt;Multi-view training&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Co-training&lt;/li&gt;
&lt;li&gt;Democratic Co-learning&lt;/li&gt;
&lt;li&gt;Tri-training&lt;/li&gt;
&lt;li&gt;Tri-training with disagreement&lt;/li&gt;
&lt;li&gt;Asymmetric tri-training&lt;/li&gt;
&lt;li&gt;Multi-task tri-training&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selfensembling"&gt;Self-ensembling&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Ladder networks&lt;/li&gt;
&lt;li&gt;Virtual Adversarial Training&lt;/li&gt;
&lt;li&gt;\(\Pi\) model&lt;/li&gt;
&lt;li&gt;Temporal Ensembling&lt;/li&gt;
&lt;li&gt;Mean Teacher&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#relatedmethodsandareas"&gt;Related methods and areas&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Distillation&lt;/li&gt;
&lt;li&gt;Learning from weak supervision&lt;/li&gt;
&lt;li&gt;Learning with noisy labels&lt;/li&gt;
&lt;li&gt;Data augmentation&lt;/li&gt;
&lt;li&gt;Ensembling a single model&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unsupervised learning constitutes one of the main challenges for current machine learning models and one of the key elements that is missing for &lt;a href="http://ruder.io/highlights-nips-2016/#generalartificialintelligence"&gt;general artificial intelligence&lt;/a&gt;. While unsupervised learning on its own is still elusive, researchers have a made a lot of progress in &lt;em&gt;combining&lt;/em&gt; unsupervised learning with supervised learning. This branch of machine learning research is called semi-supervised learning.&lt;/p&gt;

&lt;p&gt;Semi-supervised learning has a long history. For a (slightly outdated) overview, refer to Zhu (2005) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] and Chapelle et al. (2006) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;]. Particularly recently, semi-supervised learning has seen some success, considerably reducing the error rate on important benchmarks. Semi-supervised learning also makes an appearance in &lt;a href="https://www.sec.gov/Archives/edgar/data/1018724/000119312518121161/d456916dex991.htm"&gt;Amazon's annual letter to shareholders&lt;/a&gt; where it is credited with reducing the amount of labelled data needed to achieve the same accuracy improvement by \(40\times\).&lt;/p&gt;

&lt;p&gt;In this blog post, I will focus on a particular class of semi-supervised learning algorithms that produce &lt;em&gt;proxy labels&lt;/em&gt; on unlabelled data, which are used as targets together with the labelled data. These proxy labels are produced by the model itself or variants of it without any additional supervision; they thus do not reflect the ground truth but might still provide some signal for learning. In a sense, these labels can be considered &lt;em&gt;noisy&lt;/em&gt; or &lt;em&gt;weak&lt;/em&gt;. I will highlight the connection to learning from noisy labels, weak supervision as well as other related topics in the end of this post.&lt;/p&gt;

&lt;p&gt;This class of models is of particular interest in my opinion, as a) deep neural networks have been shown to be good at dealing with noisy labels and b) these models have achieved state-of-the-art in semi-supervised learning for computer vision. Note that many of these ideas are not new and many related methods have been developed in the past. In one half of this post, I will thus cover classic methods and discuss their relevance for current approaches; in the other half, I will discuss techniques that have recently achieved state-of-the-art performance. Some of the following approaches have been referred to as &lt;em&gt;self-teaching&lt;/em&gt; or &lt;em&gt;bootstrapping&lt;/em&gt; algorithms; I am not aware of a term that captures all of them, so I will simply refer to them as &lt;em&gt;proxy-label&lt;/em&gt; methods.&lt;/p&gt;

&lt;p&gt;I will divide these methods in three groups, which I will discuss in the following: 1) self-training, which uses a model's own predictions as proxy labels; 2) multi-view learning, which uses the predictions of models trained with different &lt;em&gt;views&lt;/em&gt; of the data; and 3) self-ensembling, which ensembles variations of a model's own predictions and uses these as feedback for learning. I will show pseudo-code for the most important algorithms. You can find the LaTeX source &lt;a href="https://github.com/sebastianruder/semi-supervised"&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;There are many interesting and equally important directions for semi-supervised learning that I will not cover in this post, e.g. graph-convolutional neural networks [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="selftraining"&gt;Self-training&lt;/h2&gt;

&lt;p&gt;Self-training (Yarowsky, 1995; McClosky et al., 2006) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] is one of the earliest and simplest approaches to semi-supervised learning and the most straightforward example of how a model's own predictions can be incorporated into training. As the name implies, self-training leverages a model's own predictions on unlabelled data in order to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next.&lt;/p&gt;

&lt;p&gt;Formally, self-training trains a model \(m\) on a labeled training set \(L\) and an unlabeled data set \(U\). At each iteration, the model provides predictions \(m(x)\) in the form of a probability distribution over the \(C\) classes for all unlabeled examples \(x\) in \(U\). If the probability assigned to the most likely class is higher than a predetermined threshold \(\tau\), \(x\) is added to the labeled examples with \(\DeclareMathOperator*{\argmax}{argmax} p(x) = \argmax m(x)\) as pseudo-label. This process is generally repeated for a fixed number of iterations or until no more predictions on unlabelled examples are confident. This instantiation is the most widely used and shown in Algorithm 1. &lt;br&gt;
&lt;img src="http://ruder.io/content/images/2018/03/self-training.png" alt=""&gt;
Classic self-training has shown mixed success. In parsing it proved successful with small datasets (Reichart, and Rappoport, 2007; Huang and Harper, 2009) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] or when a generative component is used together with a reranker when more data is available (McClosky et al., 2006; Suzuki and Isozaki , 2008) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;]. Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], while others report limited success on a variety of NLP tasks (He and Zhou, 2011; Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;The main downside of self-training is that the model is unable to correct its own mistakes. If the model's predictions on unlabelled data are confident but wrong, the erroneous data is nevertheless incorporated into training and the model's errors are amplified. This effect is exacerbated if the domain of the unlabelled data is different from that of the labelled data; in this case, the model's confidence will be a poor predictor of its performance.&lt;/p&gt;

&lt;h2 id="multiviewtraining"&gt; Multi-view training&lt;/h2&gt;

&lt;p&gt;Multi-view training aims to train different models with different &lt;em&gt;views&lt;/em&gt; of the data. Ideally, these views complement each other and the models can collaborate in improving each other's performance. These views can differ in different ways such as in the features they use, in the architectures of the models, or in the data on which the models are trained.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Co-training&lt;/strong&gt; &amp;nbsp; Co-training (Blum and Mitchell, 1998) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] is a classic multi-view training method, which makes comparatively strong assumptions. It requires that the data \(L\) can be represented using two conditionally independent feature sets \(L^1\) and \(L^2\) and that each feature set is sufficient to train a good model. After the initial models \(m_1\) and \(m_2\) are trained on their respective feature sets, at each iteration, only inputs that are confident (i.e. have a probability higher than a threshold \(\tau\)) according to &lt;em&gt;exactly one&lt;/em&gt; of the two models are moved to the training set of &lt;em&gt;the other&lt;/em&gt; model. One model thus provides the labels to the inputs on which the &lt;em&gt;other&lt;/em&gt; model is uncertain. Co-training can be seen in Algorithm 2.
&lt;img src="http://ruder.io/content/images/2018/04/co-training.png" alt=""&gt;
In the original co-training paper (Blum and Mitchell, 1998), co-training is used to classify web pages using the text on the page as one view and the anchor text of hyperlinks on other pages pointing to the page as the other view. As two conditionally independent views are not always available, Chen et al. (2011) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] propose pseudo-multiview regularization (Chen et al., 2011) in order to split the features into two mutually exclusive views so that co-training is effective. To this end, pseudo-multiview regularization constrains the models so that at least one of them has a zero weight for each feature. This is similar to the orthogonality constraint recently used in domain adaptation to encourage shared and private spaces (Bousmalis et al., 2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. A second constraint requires the models to be confident on different subsets of \(U\). Chen et al. (2011) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] use pseudo-multiview regularization to adapt co-training to domain adaptation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Democratic Co-learning&lt;/strong&gt; Rather than treating different feature sets as views, democratic co-learning (Zhou and Goldman, 2004) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] employs models with &lt;em&gt;different inductive biases&lt;/em&gt;. These can be different network architectures in the case of neural networks or completely different learning algorithms. Democratic co-learning first trains each model separately on the complete labelled data \(L\). The models then make predictions on the unlabelled data \(U\). If a majority of models confidently agree on the label of an example, the example is added to the labelled dataset. Confidence is measured in the original formulation by measuring if the sum of the mean confidence intervals \(w\) of the models, which agreed on the label is larger than the sum of the models that disagreed. This process is repeated until no more examples are added. The final prediction is made with a majority vote weighted with the confidence intervals of the models. The full algorithm can be seen below. \(M\) is the set of all models that predict the same label \(j\) for an example \(x\). 
&lt;img src="http://ruder.io/content/images/2018/04/democratic_co-learning-2.png" alt=""&gt;
&lt;strong&gt;Tri-training&lt;/strong&gt; &amp;nbsp; Tri-training (Zhou and Li, 2005) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] is one of the best known multi-view training   methods. It can be seen as an instantiation of democratic co-learning, which leverages the agreement of three independently trained models to reduce the bias of predictions on unlabeled data. The main requirement for tri-training is that the initial models are diverse. This can be achieved using different model architectures as in democratic co-learning. The most common way to obtain diversity for tri-training, however, is to obtain different variations \(S_i\) of the original training data \(L\) using bootstrap sampling. The three models \(m_1\), \(m_2\), and \(m_3\) are then trained on these bootstrap samples, as depicted in Algorithm 4. An unlabeled data point is added to the training set of a model \(m_i\) if the other two models \(m_j\) and \(m_k\) agree on its label. Training stops when the classifiers do not change anymore.
&lt;img src="http://ruder.io/content/images/2018/04/tri-training.png" alt=""&gt;
Despite having been proposed more than 10 years ago, before the advent of Deep Learning, we found in a &lt;a href="https://arxiv.org/abs/1804.09530"&gt;recent paper&lt;/a&gt; (Ruder and Plank, 2018) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] that classic tri-training is a strong baseline for neural semi-supervised with and without domain shift for NLP and that it outperforms even recent state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tri-training with disagreement&lt;/strong&gt; &amp;nbsp; Tri-training &lt;em&gt;with disagreement&lt;/em&gt; (Søgaard, 2010) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. In order to achieve this, it adds a simple modification to the original algorithm (altering line 8 in Algorithm 2), requiring that for an unlabeled data point on which \(m_j\) and \(m_k\) &lt;em&gt;agree&lt;/em&gt;, the other model \(m_i\) &lt;em&gt;disagrees&lt;/em&gt; on the prediction. Tri-training with disagreement is more data-efficient than tri-training and has achieved competitive results on part-of-speech tagging (Søgaard, 2010).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Asymmetric tri-training&lt;/strong&gt; &amp;nbsp; Asymmetic tri-training (Saito et al., 2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] is a recently proposed extension of tri-training that achieved state-of-the-art results for unsupervised domain adaptation in computer vision. For unsupervised domain adaptation, the test data and unlabeled data are from a different domain than the labelled examples. To adapt tri-training to this shift, asymmetric tri-training learns one of the models &lt;em&gt;only&lt;/em&gt; on proxy labels and not on labelled examples (a change to line 10 in Algorithm 4) and uses only this model to classify target domain examples at test time. In addition, all three models share the same feature extractor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-task tri-training&lt;/strong&gt; &amp;nbsp; Tri-training typically relies on training separate models on bootstrap samples of a potentially large amount of training data, which is expensive. Multi-task tri-training (MT-Tri) (Ruder and Plank, 2018) aims to reduce both the time and space complexity of tri-training by leveraging insights from multi-task learning (MTL) (Caruana, 1993) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] to share knowledge across models and accelerate training. Rather than storing and training each model separately, MT-Tri shares the parameters of the models and trains them jointly using MTL. Note that the model does only &lt;em&gt;pseudo&lt;/em&gt; MTL as all three models effectively perform the same task.&lt;/p&gt;

&lt;p&gt;The output softmax layers are model-specific and are only updated for the input of the respective model. As the models leverage a joint representation, diversity is even more crucial. We need to ensure that the features used for prediction in the softmax layers of the different models are as diverse as possible, so that the models can still learn from each other's predictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training. Similar to pseudo-view regularization, we thus use an orthogonality constraint (Bousmalis et al., 2016) on two of the three softmax output layers as an additional loss term. &lt;/p&gt;

&lt;p&gt;The pseudo-code can be seen below. In contrast to classic tri-training, we can train the multi-task model with its three model-specific outputs jointly and &lt;em&gt;without&lt;/em&gt; bootstrap sampling on the labeled source domain data until convergence, as the orthogonality constraint enforces different representations between models \(m_1\) and \(m_2\). From this point, we can leverage the pair-wise agreement of two output layers to add pseudo-labeled examples as training data to the third model. We train the third output layer \(m_3\) only on pseudo-labeled target instances in order to make tri-training more robust to a domain shift. For the final prediction, we use majority voting of all three output layers. For more information about multi-task tri-training, self-training, other tri-training variants, you can refer to our recent &lt;a href="https://arxiv.org/abs/1804.09530"&gt;ACL 2018 paper&lt;/a&gt;. &lt;br&gt;
&lt;img src="http://ruder.io/content/images/2018/04/multi-task_tri-training-1.png" alt=""&gt;&lt;/p&gt;

&lt;h2 id="selfensembling"&gt; Self-ensembling&lt;/h2&gt;

&lt;p&gt;Self-ensembling methods are very similar to multi-view learning approaches in that they combine different variants of a model. Multi-task tri-training, for instance, can also be seen as a self-ensembling method where different variations of a model are used to create a stronger ensemble prediction. In contrast to multi-view learning, diversity is not a key concern. Self-ensembling approaches mostly use a single model under different configurations in order to make the model's predictions more robust. Most of the following methods are very recent and several have achieved state-of-the-art results in computer vision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ladder networks&lt;/strong&gt; &amp;nbsp; The \(\Gamma\) (gamma) version of Ladder Networks (Rasmus et al., 2015) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] aims to make a model more robust to noise. For each unlabelled example, it uses the model's prediction on the clean example as a proxy label for prediction on a perturbed version of the example. This way, the model learns to develop features that are invariant to noise and predictive of the labels on the labelled training data. Ladder networks have been mostly used in computer vision where many forms of perturbation and data augmentation are available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virtual Adversarial Training&lt;/strong&gt; &amp;nbsp; If perturbing the original sample is not possible or desired, we can instead perturb the example in feature space. Rather than randomly perturbing it by e.g. adding dropout, we can apply the &lt;em&gt;worst possible&lt;/em&gt; perturbation for the model, which transforms the input into an adversarial sample. While adversarial training requires access to the labels to perform these perturbations, &lt;em&gt;virtual&lt;/em&gt; adversarial training (Miyato et al., 2017) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] requires no labels and is thus suitable for semi-supervised learning. Virtual adversarial training effectively seeks to make the model robust to perturbations in directions to which it is most sensitive and has achieved good results on text classification datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;\(\Pi\) model&lt;/strong&gt; &amp;nbsp; Rather than treating clean predictions as proxy labels, the \(\Pi\) (pi) model (Laine and Aila, 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;] ensembles the predictions of the model under two different perturbations of the input data and two different dropout conditions \(z\) and \(\tilde{z}\). The full pseudo-code can be seen in Algorithm 6 below. \(g(x)\) is the stochastic input augmentation function. The first loss term encourages the predictions under the two different noise settings to be consistent, with \(\lambda\) determining the contribution, while the second loss term is the standard cross-entropy loss \(H\) with respect to the label \(y\). In contrast to the models we encountered before, we apply the unsupervised loss component to both unlabelled and labelled examples.
&lt;img src="http://ruder.io/content/images/2018/04/pi-model-3.png" alt=""&gt;
&lt;strong&gt;Temporal Ensembling&lt;/strong&gt; &amp;nbsp; Instead of ensembling over the same model under different noise configurations, we can ensemble over different models. As training separate models is expensive, we can instead ensemble the predictions of a model &lt;em&gt;at different timesteps&lt;/em&gt;. We can save the ensembled proxy labels \(Z\) as an exponential moving average of the model's past predictions on all examples as depicted below in order to save space. As we initialize the proxy labels as a zero vector, they are biased towards \(0\). We can correct this bias similar to Adam (Kingma and Ba, 2015) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;] based on the current epoch \(t\) to obtain bias-corrected target vectors \(\tilde{z}\). We then update the model similar to the \(\Pi\) model. 
&lt;img src="http://ruder.io/content/images/2018/04/temporal_ensembling-3.png" alt=""&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Teacher&lt;/strong&gt; &amp;nbsp; Finally, instead of averaging the &lt;em&gt;predictions&lt;/em&gt; of our model over training time, we can average the model weights. Mean teacher (Tarvainen and Valpola, 2017) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] stores an exponential moving average of the model parameters. For every example, this mean teacher model is then used to obtain proxy labels \(\tilde{z}\). The consistency loss and supervised loss are computed as in temporal ensembling.&lt;/p&gt;

&lt;p&gt;Mean teacher has achieved state-of-the-art results for semi-supervised learning for computer vision. For reference, on ImageNet with 10% of the labels, it achieves an error rate of \(9.11\), compared to an error rate of \(3.79\) using &lt;em&gt;all&lt;/em&gt; labels with the state-of-the-art. For more information about self-ensembling methods, have a look at &lt;a href="https://thecuriousaicompany.com/mean-teacher/"&gt;this intuitive blog post&lt;/a&gt; by the Curious AI company. We have run experiments with temporal ensembling for NLP tasks, but did not manage to obtain consistent results. My assumption is that the unsupervised consistency loss is more suitable for continuous inputs. Mean teacher might work better, as averaging weights aka Polyak averaging (Polyak and Juditsky, 1992) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] is a tried method for accelerating optimization.&lt;/p&gt;

&lt;p&gt;Very recently, Oliver et al. (2018) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] raise some questions regarding the true applicability of these methods: They find that the performance difference to a properly tuned supervised baseline is smaller than typically reported, that transfer learning from a labelled dataset (e.g. ImageNet) outperforms the presented methods, and that performance degrades severely under a domain shift. In order to deal with the latter, algorithms such as asymmetric or multi-task tri-training learn different representations for the target distribution. It remains to be seen if these insights translate to other domains; a combination of transfer learning and semi-supervised adaptation to the target domain seems particularly promising.&lt;/p&gt;

&lt;h2 id="relatedmethodsandareas"&gt; Related methods and areas&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Distillation&lt;/strong&gt; &amp;nbsp; Proxy-label approaches can be seen as different forms of distillation (Hinton et al., 2015) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;]. Distillation was originally conceived as a method to compress the information of a large model or an ensemble in a smaller model. In the standard setup, a typically large and fully trained &lt;em&gt;teacher&lt;/em&gt; model provides proxy targets for a &lt;em&gt;student&lt;/em&gt; model, which is generally smaller and faster. Self-learning is akin to distillation without a teacher, where the student is left to learn by themselves and with no-one to correct its mistakes. For multi-view learning, different models work together to teach each other, alternately acting as both teachers and students. Self-ensembling, finally, has one model assuming the dual role of teacher and student: As a teacher, it generates new targets, which are then incorporated by itself as a student for learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning from weak supervision&lt;/strong&gt; &amp;nbsp; Learning from weak supervision, as the name implies, can be seen as a weaker form of supervised learning or alternatively as a stronger form of semi-supervised learning: While supervised learning provides us with labels that we know to be correct and semi-supervised learning only provides us with a small set of labelled examples, weak supervision allows us to obtain labels that we know to be noisy for the unlabelled data as a further signal for learning. Typically, the weak annotator is an unsupervised method that is very different from the model we use for learning the task. For sentiment analysis, this could be a simple lexicon-based method [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]. Many of the presented methods could be extended to the weak supervision setting by incorporating the weak labels as feedback. Self-ensembling methods, for instance, might employ another teacher model that gauges the quality of weakly annotated examples similar to Deghani et al. (2018) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;]. For an overview of weak supervision, have a look at &lt;a href="https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html"&gt;this blog post&lt;/a&gt; by Stanford's Hazy Research group.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning with noisy labels&lt;/strong&gt; &amp;nbsp; Learning with noisy labels is similar to learning from weak supervision. In both cases, labels are available that cannot be completely trusted. For learning with noisy labels, labels are typically assumed to be permuted with a fixed random permutation. While proxy-label approaches supply the noisy labels themselves, when learning with noisy labels, the labels are part of the data. Similar to learning from weak supervision, we can try to model the noise to assess the quality of the labels (Sukhbaatar et al., 2015) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. Similar to self-ensembling methods, we can enforce consistency between the model's preditions and the proxy labels (Reed et al., 2015) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt; &amp;nbsp; Several self-ensembling methods employ data augmentation to enforce consistency between model predictions under different noise settings. Data augmentation is mostly used in computer vision, but  noise in the form of different dropout masks can also be applied to the model parameters as in the \(\Pi\) model and has also been used in LSTMs (Zolna et al., 2018) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;]. While regularization in the form of dropout, batch normalization, etc. can be used when labels are available in order to make predictions more robust, a consistency loss is required in the case without labels. For supervised learning, adversarial training can be employed to obtain adversarial examples and has been used successfully e.g. for part-of-speech tagging (Yasunaga et al., 2018) [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ensembling a single model&lt;/strong&gt; &amp;nbsp; The discussed self-ensembling methods all employ ensemble predictions not just to make predictions more robust, but as feedback to improve the model itself during training in a self-reinforcing loop. In the supervised setting, this feedback might not be necessary; ensembling a single model is still useful, however, to save time compared to training multiple models. Two methods that have been proposed to ensemble a model from a single training run are checkpoint ensembles and snapshot ensembles. Checkpoint ensembles (Sennrich et al., 2016) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] ensemble the last \(n\) checkpoints of a single training run and have been used to achieve state-of-the-art in machine translation. Snapshot ensembles (Huang et al., 2017) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] ensemble models converged to different minima during a training run and have been used to achieve state-of-the-art in object recognition. &lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this post was able to give you an insight into a part of the semi-supervised learning landscape that seems to be particularly useful to improve the performance of current models. While learning completely without labelled data is unrealistic at this point, semi-supervised learning enables us to augment our small labelled datasets with large amounts of available unlabelled data. Most of the discussed methods are promising in that they treat the model as a black box and can thus be used with any existing supervised learning model. As always, if you have any questions or noticed any mistakes, feel free to write a comment in the comments section below.&lt;/p&gt;

&lt;h2 id="references"&gt; References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Zhu, X. (2005). Semi-Supervised Learning Literature Survey. &lt;a href="http://ruder.io/semi-supervised/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Chapelle, O., Schölkopf, B., &amp;amp; Zien, A. (2006). Semi-Supervised Learning. Interdisciplinary sciences computational life sciences (Vol. 1). &lt;a href="http://doi.org/10.1007/s12539-009-0016-2"&gt;http://doi.org/10.1007/s12539-009-0016-2&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Kipf, T. N., &amp;amp; Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics (pp. 189-196). Association for Computational Linguistics. &lt;a href="http://ruder.io/semi-supervised/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;McClosky, D., Charniak, E., &amp;amp; Johnson, M. (2006). Effective self-training for parsing. Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, 152–159. &lt;a href="http://ruder.io/semi-supervised/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Reichart, R., &amp;amp; Rappoport, A. (2007). Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (pp. 616-623) &lt;a href="http://ruder.io/semi-supervised/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Suzuki, J., &amp;amp; Isozaki, H. (2008). Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. Proceedings of ACL-08: HLT, 665-673. &lt;a href="http://ruder.io/semi-supervised/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Petrov, S., &amp;amp; McDonald, R. (2012). Overview of the 2012 shared task on parsing the web. In Notes of the first workshop on syntactic analysis of non-canonical language (sancl) (Vol. 59). &lt;a href="http://ruder.io/semi-supervised/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;He, Y., &amp;amp; Zhou, D. (2011). Self-training from labeled features for sentiment analysis. Information Processing &amp;amp; Management, 47(4), 606-616. &lt;a href="http://ruder.io/semi-supervised/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Plank, B. (2011). Domain adaptation for parsing. University Library Groniongen][Host]. &lt;a href="http://ruder.io/semi-supervised/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Van Asch, V., &amp;amp; Daelemans, W. (2016). Predicting the Effectiveness of Self-Training: Application to Sentiment Classification. arXiv preprint arXiv:1601.03288. &lt;a href="http://ruder.io/semi-supervised/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;van der Goot, R., Plank, B., &amp;amp; Nissim, M. (2017). To normalize, or not to normalize: The impact of normalization on part-of-speech tagging. arXiv preprint arXiv:1707.05116. &lt;a href="http://ruder.io/semi-supervised/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Huang, Z., &amp;amp; Harper, M. (2009). Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2 (pp. 832-841). Association for Computational Linguistics. &lt;a href="http://ruder.io/semi-supervised/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Zhou, Z.-H., &amp;amp; Li, M. (2005). Tri-Training: Exploiting Unlabled Data Using Three Classifiers. IEEE Trans.Data Eng., 17(11), 1529–1541. &lt;a href="http://doi.org/10.1109/TKDE.2005.186"&gt;http://doi.org/10.1109/TKDE.2005.186&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Søgaard, A. (2010). Simple semi-supervised training of part-of-speech taggers. Proceedings of the ACL 2010 Conference Short Papers. &lt;a href="http://ruder.io/semi-supervised/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Saito, K., Ushiku, Y., &amp;amp; Harada, T. (2017). Asymmetric Tri-training for Unsupervised Domain Adaptation. In ICML 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08400"&gt;http://arxiv.org/abs/1702.08400&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Blum, A., &amp;amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory (pp. 92-100). ACM. &lt;a href="http://ruder.io/semi-supervised/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Chen, M., Weinberger, K. Q., &amp;amp; Blitzer, J. C. (2011). Co-Training for Domain Adaptation. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/semi-supervised/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Chen, M., Weinberger, K. Q., &amp;amp; Chen, Y. (2011). Automatic Feature Decomposition for Single View Co-training. Proceedings of the 28th International Conference on Machine Learning (ICML-11), 953–960. &lt;a href="http://ruder.io/semi-supervised/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., &amp;amp; Erhan, D. (2016). Domain Separation Networks. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/semi-supervised/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Zhou, Y., &amp;amp; Goldman, S. (2004). Democratic Co-Learning. In 16th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2004. &lt;a href="http://ruder.io/semi-supervised/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ruder, S., &amp;amp; Plank, B. (2018). Strong Baselines for Neural Semi-supervised Learning under Domain Shift. In Proceedings of ACL 2018. &lt;a href="http://ruder.io/semi-supervised/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Caruana, R. (1993). Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning. &lt;a href="http://ruder.io/semi-supervised/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Rasmus, A., Valpola, H., Honkala, M., Berglund, M., &amp;amp; Raiko, T. (2015). Semi-Supervised Learning with Ladder Network. arXiv Preprint arXiv:1507.02672. Retrieved from &lt;a href="http://arxiv.org/abs/1507.02672"&gt;http://arxiv.org/abs/1507.02672&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Miyato, T., Dai, A. M., &amp;amp; Goodfellow, I. (2017). Adversarial Training Methods for Semi-supervised Text Classification. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Yasunaga, M., Kasai, J., &amp;amp; Radev, D. (2018). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.04903"&gt;http://arxiv.org/abs/1711.04903&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Laine, S., &amp;amp; Aila, T. (2017). Temporal Ensembling for Semi-Supervised Learning. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/semi-supervised/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Tarvainen, A., &amp;amp; Valpola, H. (2017). Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1703.01780"&gt;http://arxiv.org/abs/1703.01780&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Hinton, G., Vinyals, O., &amp;amp; Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv Preprint arXiv:1503.02531. &lt;a href="https://doi.org/10.1063/1.4931082"&gt;https://doi.org/10.1063/1.4931082&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh neural machine translation systems for WMT 16. arXiv preprint arXiv:1606.02891. &lt;a href="http://ruder.io/semi-supervised/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Polyak, B. T., &amp;amp; Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4), 838-855. &lt;a href="http://ruder.io/semi-supervised/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Dehghani, M., Mehrjou, A., Gouws, S., Kamps, J., &amp;amp; Schölkopf, B. (2018). Fidelity-Weighted Learning. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.02799"&gt;http://arxiv.org/abs/1711.02799&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Kiritchenko, S., Zhu, X., &amp;amp; Mohammad, S. M. (2014). Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, 50, 723-762. &lt;a href="http://ruder.io/semi-supervised/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., &amp;amp; Rabinovich, A. (2015). Training Deep Neural Networks on Noisy Labels with Bootstrapping. ICLR 2015 Workshop Track. Retrieved from &lt;a href="http://arxiv.org/abs/1412.6596"&gt;http://arxiv.org/abs/1412.6596&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., &amp;amp; Fergus, R. (2015). Training Convolutional Networks with Noisy Labels. Workshop Track - ICLR 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1406.2080"&gt;http://arxiv.org/abs/1406.2080&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Zolna, K., Arpit, D., Suhubdy, D., &amp;amp; Bengio, Y. (2018). Fraternal Dropout. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.00066"&gt;http://arxiv.org/abs/1711.00066&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Oliver, A., Odena, A., Raffel, C., Cubuk, E. D., &amp;amp; Goodfellow, I. J. (2018). Realistic Evaluation of Semi-Supervised Learning Algorithms. arXiv preprint arXiv:1804.09170. &lt;a href="http://ruder.io/semi-supervised/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Text Classification with TensorFlow Estimators</title><description>This post is a tutorial on how to use Estimators in TensorFlow to classify text.</description><link>http://ruder.io/text-classification-tensorflow-estimators/</link><guid isPermaLink="false">4dd8b7a6-a266-4ddb-b6b2-b3444bcc48b3</guid><category>nlp</category><category>tensorflow</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 16 Apr 2018 13:24:29 GMT</pubDate><content:encoded>&lt;p&gt;&lt;head&gt; &lt;br&gt;
  &lt;meta charset="utf-8"&gt;
  &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
  &lt;title&gt;nlp&lt;em&gt;estimators&lt;/em&gt;blog.md&lt;/title&gt;
  &lt;link rel="stylesheet" href="https://stackedit.io/style.css"&gt;
&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This post was written together with the awesome &lt;a href="https://twitter.com/eisenjulian"&gt;Julian Eisenschlos&lt;/a&gt; and was originally published on the &lt;a href="https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe"&gt;TensorFlow blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hello there! Throughout this post we will show you how to classify text using Estimators in TensorFlow. Here’s the outline of what we’ll cover:&lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt;Loading data using Datasets.&lt;/li&gt;  
&lt;li&gt;Building baselines using pre-canned estimators.&lt;/li&gt;  
&lt;li&gt;Using word embeddings.&lt;/li&gt;  
&lt;li&gt;Building custom estimators with convolution and LSTM layers.&lt;/li&gt;  
&lt;li&gt;Loading pre-trained word vectors.&lt;/li&gt;  
&lt;li&gt;Evaluating and comparing models using TensorBoard.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;hr&gt;  

&lt;p&gt;Welcome to Part 4 of a blog series that introduces TensorFlow Datasets and Estimators. You don’t need to read all of the previous material, but take a look if you want to refresh any of the following concepts. &lt;a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"&gt;Part 1&lt;/a&gt; focused on pre-made Estimators, &lt;a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"&gt;Part 2&lt;/a&gt; discussed feature columns, and &lt;a href="https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html"&gt;Part 3&lt;/a&gt; how to create custom Estimators.&lt;/p&gt;  

&lt;p&gt;Here in Part 4, we will build on top of all the above to tackle a different family of problems in Natural Language Processing (NLP). In particular, this article demonstrates how to solve a text classification task using custom TensorFlow estimators, embeddings, and the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/layers"&gt;tf.layers&lt;/a&gt; module. Along the way, we’ll learn about word2vec and transfer learning as a technique to bootstrap model performance when labeled data is a scarce resource.&lt;/p&gt;  

&lt;p&gt;We will show you relevant code snippets. &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb"&gt;Here&lt;/a&gt;’s the complete Jupyter Notebook  that you can run locally or on &lt;a href="https://colab.research.google.com/drive/1oXjNYSJ3VsRvAsXN4ClmtsVEgPW_CX_c?hl=en#forceEdit=true&amp;offline=true&amp;sandboxMode=true"&gt;Google Colaboratory&lt;/a&gt;. The plain &lt;code&gt;.py&lt;/code&gt;  source file is also available &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py"&gt;here&lt;/a&gt;. Note that the code was written to demonstrate how Estimators work functionally and was not optimized for maximum performance.&lt;/p&gt;  

&lt;h3 id="the-task"&gt;The task&lt;/h3&gt;  

&lt;p&gt;The dataset we will be using is the IMDB &lt;a href="http://ai.stanford.edu/~amaas/data/sentiment/"&gt;Large Movie Review Dataset&lt;/a&gt;, which consists of &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; highly polar movie reviews for training, and &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for testing. We will use this dataset to train a binary classification model, able to predict whether a review is positive or negative.&lt;/p&gt;  

&lt;p&gt;For illustration, here’s a piece of a negative review (with &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; stars) in the dataset:&lt;/p&gt;  

&lt;blockquote&gt;  
&lt;p&gt;Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film.&lt;/p&gt;  
&lt;/blockquote&gt;  

&lt;p&gt;&lt;em&gt;Keras&lt;/em&gt; provides a convenient handler for importing the dataset which is also available as a serialized numpy array &lt;code&gt;.npz&lt;/code&gt; file to download &lt;a href="https://s3.amazonaws.com/text-datasets/imdb.npz"&gt;here&lt;/a&gt;. For text classification, it is standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. For this reason, each review consists of a series of word indexes that go from &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;4&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (the most frequent word in the dataset &lt;strong&gt;the&lt;/strong&gt;) to &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;4999&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;4&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which corresponds to &lt;strong&gt;orange&lt;/strong&gt;. Index &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; represents the beginning of the sentence and the index &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is assigned to all unknown (also known as &lt;em&gt;out-of-vocabulary&lt;/em&gt; or &lt;em&gt;OOV&lt;/em&gt;) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.&lt;/p&gt;  

&lt;p&gt;After we’ve loaded the data in memory we pad each of the sentences with &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to a fixed size (here: &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;) so that we have two &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-dimensional &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25000\times200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.72777em; vertical-align: -0.08333em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mbin"&gt;×&lt;/span&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; arrays for training and testing respectively.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;vocab_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;5000&lt;/span&gt;  
sentence_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;200&lt;/span&gt;  
&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_train&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;(&lt;/span&gt;x_test_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_test&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; imdb&lt;span class="token punctuation"&gt;.&lt;/span&gt;load_data&lt;span class="token punctuation"&gt;(&lt;/span&gt;num_words&lt;span class="token operator"&gt;=&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
x_train &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    x_train_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
x_test &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    x_test_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;h3 id="input-functions"&gt;Input Functions&lt;/h3&gt;  

&lt;p&gt;The Estimator framework uses &lt;em&gt;input functions&lt;/em&gt; to split the data pipeline from the model itself. Several helper methods are available to create them, whether your data is in a &lt;code&gt;.csv&lt;/code&gt; file, or in a &lt;code&gt;pandas.DataFrame&lt;/code&gt;, whether it fits in memory or not. In our case, we can use &lt;code&gt;Dataset.from_tensor_slices&lt;/code&gt; for both the train and test sets.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;x_len_train &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; x_train_variable&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
x\_len\_test &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; x_test_variable&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;parser&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;,&lt;/span&gt; length&lt;span class="token punctuation"&gt;,&lt;/span&gt; y&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    features &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;"x"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; x&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;"len"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; length&lt;span class="token punctuation"&gt;}&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; features&lt;span class="token punctuation"&gt;,&lt;/span&gt; y

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;train_input_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;data&lt;span class="token punctuation"&gt;.&lt;/span&gt;Dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;from_tensor_slices&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train&lt;span class="token punctuation"&gt;,&lt;/span&gt; x_len_train&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_train&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;shuffle&lt;span class="token punctuation"&gt;(&lt;/span&gt;buffer_size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train_variable&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;batch&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;&lt;span class="token builtin"&gt;map&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;parser&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;repeat&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    iterator &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;make_one_shot_iterator&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; iterator&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_next&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;eval_input_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;data&lt;span class="token punctuation"&gt;.&lt;/span&gt;Dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;from_tensor_slices&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_test&lt;span class="token punctuation"&gt;,&lt;/span&gt; x_len_test&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_test&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;batch&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;&lt;span class="token builtin"&gt;map&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;parser&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    iterator &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;make_one_shot_iterator&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; iterator&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_next&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional &lt;code&gt;"len"&lt;/code&gt; key that captures the length of the original, unpadded sequence, which we will use later.&lt;/p&gt;  

&lt;h3 id="building-a-baseline"&gt;Building a baseline&lt;/h3&gt;  

&lt;p&gt;It’s good practice to start any machine learning project trying basic baselines. The simpler the better as having a simple and robust baseline is key to understanding exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.&lt;/p&gt;  

&lt;p&gt;With that in mind, let us start by trying out one of the simplest models for text classification. That would be a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. As this model does not care about the order of words in a sentence, we normally refer to it as a &lt;em&gt;Bag-of-Words&lt;/em&gt; approach. Let’s see how we can implement this model using an &lt;code&gt;Estimator&lt;/code&gt;.&lt;/p&gt;  

&lt;p&gt;We start out by defining the feature column that is used as input to our classifier. As we have seen in &lt;a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"&gt;Part 2&lt;/a&gt;, &lt;code&gt;categorical_column_with_identity&lt;/code&gt; is the right choice for this pre-processed text input. If we were feeding raw text tokens other &lt;code&gt;feature_columns&lt;/code&gt; could do a lot of the pre-processing for us. We can now use the pre-made &lt;code&gt;LinearClassifier&lt;/code&gt;.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;column &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;feature_column&lt;span class="token punctuation"&gt;.&lt;/span&gt;categorical_column_with_identity&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'x'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; vocab_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;LinearClassifier&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    feature_columns&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;column&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'bow_sparse'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;Finally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. As we do not aim to maximize performance in this blog post, we only train our models for &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; steps.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;train_and_evaluate&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;train_input_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt; steps&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;25000&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    eval_results &lt;span class="token operator"&gt;=&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;eval_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predictions &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;p&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'logistic'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; p &lt;span class="token keyword"&gt;in&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;predict&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;eval_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;reset_default_graph&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; 
    &lt;span class="token comment"&gt;# Add a PR summary in addition to the summaries that the classifier writes&lt;/span&gt;
    pr &lt;span class="token operator"&gt;=&lt;/span&gt; summary_lib&lt;span class="token punctuation"&gt;.&lt;/span&gt;pr_curve&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'precision_recall'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; predictions&lt;span class="token operator"&gt;=&lt;/span&gt;predictions&lt;span class="token punctuation"&gt;,&lt;/span&gt; labels&lt;span class="token operator"&gt;=&lt;/span&gt;y_test&lt;span class="token punctuation"&gt;.&lt;/span&gt;astype&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;bool&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; num_thresholds&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;21&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;with&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;Session&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;as&lt;/span&gt; sess&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        writer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;summary&lt;span class="token punctuation"&gt;.&lt;/span&gt;FileWriter&lt;span class="token punctuation"&gt;(&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'eval'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sess&lt;span class="token punctuation"&gt;.&lt;/span&gt;graph&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        writer&lt;span class="token punctuation"&gt;.&lt;/span&gt;add_summary&lt;span class="token punctuation"&gt;(&lt;/span&gt;sess&lt;span class="token punctuation"&gt;.&lt;/span&gt;run&lt;span class="token punctuation"&gt;(&lt;/span&gt;pr&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; global_step&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        writer&lt;span class="token punctuation"&gt;.&lt;/span&gt;close&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;One of the benefits of choosing a simple model is that it is much more interpretable. The more complex a model, the harder it is to inspect and the more it tends to work like a black box. In this example, we can load the weights from our model’s last checkpoint and take a look at what tokens correspond to the  biggest weights in absolute value. The results look like what we would expect.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token comment"&gt;# Load the tensor with the model weights&lt;/span&gt;  
weights &lt;span class="token operator"&gt;=&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_variable_value&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'linear/linear_model/x/weights'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;flatten&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token comment"&gt;# Find biggest weights in absolute value&lt;/span&gt;  
extremes &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;concatenate&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sorted_indexes&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;8&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sorted_indexes&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token number"&gt;8&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token comment"&gt;# word_inverted_index is a dictionary that maps from indexes back to tokens&lt;/span&gt;  
extreme_weights &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token builtin"&gt;sorted&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    &lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;weights&lt;span class="token punctuation"&gt;[&lt;/span&gt;i&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; word_inverted_index&lt;span class="token punctuation"&gt;[&lt;/span&gt;i &lt;span class="token operator"&gt;-&lt;/span&gt; index_offset&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; i &lt;span class="token keyword"&gt;in&lt;/span&gt; extremes&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;span class="token comment"&gt;# Create plot&lt;/span&gt;  
y_pos &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;arange&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;extreme_weights&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;bar&lt;span class="token punctuation"&gt;(&lt;/span&gt;y_pos&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;pair&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; pair &lt;span class="token keyword"&gt;in&lt;/span&gt; extreme_weights&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; align&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'center'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; alpha&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.5&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;xticks&lt;span class="token punctuation"&gt;(&lt;/span&gt;y_pos&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;pair&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; pair &lt;span class="token keyword"&gt;in&lt;/span&gt; extreme_weights&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; rotation&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;45&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; ha&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'right'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;ylabel&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'Weight'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;title&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'Most significant tokens'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;show&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/token_weights.png" alt="Model weights" style="width: 60%"&gt;&lt;/p&gt;  

&lt;p&gt;As we can see, tokens with the most positive weight such as ‘refreshing’ are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf&lt;/a&gt; scores.&lt;/p&gt;  

&lt;h3 id="embeddings"&gt;Embeddings&lt;/h3&gt;  

&lt;p&gt;The next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space—when learned from a large enough corpus—has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an &lt;code&gt;embedding_column&lt;/code&gt;. The representation seen by the model is the mean of the embeddings for each token (see the &lt;code&gt;combiner&lt;/code&gt; argument in the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column"&gt;docs&lt;/a&gt;). We can plug in the embedded features into a pre-canned &lt;code&gt;DNNClassifier&lt;/code&gt;.&lt;/p&gt;  

&lt;p&gt;A note for the keen observer: an &lt;code&gt;embedding_column&lt;/code&gt; is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn’t make sense to use an &lt;code&gt;embedding_column&lt;/code&gt; directly in a &lt;code&gt;LinearClassifier&lt;/code&gt; because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embedding_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;50&lt;/span&gt;  
word\_embedding\_column &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;feature_column&lt;span class="token punctuation"&gt;.&lt;/span&gt;embedding_column&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    column&lt;span class="token punctuation"&gt;,&lt;/span&gt; dimension&lt;span class="token operator"&gt;=&lt;/span&gt;embedding_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;
classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;DNNClassifier&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    hidden_units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    feature_columns&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;word_embedding_column&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'bow_embeddings'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;We can use TensorBoard to visualize our &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;50&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-dimensional word vectors projected into &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;\mathbb{R}^3&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.814108em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.814108em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.814108em;"&gt;&lt;span class="" style="top: -3.063em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathrm mtight"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; using &lt;a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"&gt;t-SNE&lt;/a&gt;. We expect similar words to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviours.&lt;/p&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/embeddings.gif" alt="embeddings"&gt;&lt;/p&gt;  

&lt;h3 id="convolutions"&gt;Convolutions&lt;/h3&gt;  

&lt;p&gt;At this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.&lt;/p&gt;  

&lt;p&gt;Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for &lt;a href="https://www.tensorflow.org/tutorials/layers"&gt;image classification&lt;/a&gt;. The intuition is that certain sequences of words, or &lt;em&gt;n-grams&lt;/em&gt;, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.&lt;/p&gt;  

&lt;p&gt;The following image shows how a filter matrix &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;F \in \mathbb{R}^{d\times m}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.849108em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.888208em; vertical-align: -0.0391em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathit" style="margin-right: 0.13889em;"&gt;F&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.849108em;"&gt;&lt;span class="" style="top: -3.063em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathit mtight"&gt;d&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mathit mtight"&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; slides across each &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;3&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-gram window of tokens to build a new feature map. Afterwards a &lt;em&gt;pooling&lt;/em&gt; layer is usually applied to combine adjacent results.&lt;/p&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/conv.png" alt="text convolution" style="width: 80%"&gt;&lt;br&gt;  
&lt;small&gt;&lt;/small&gt;&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;  
Source: &lt;a href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9"&gt;Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks&lt;/a&gt; by &lt;strong&gt;Severyn&lt;/strong&gt; et al. [2015]&lt;/p&gt;

&lt;p&gt;Let us look at the full model architecture. The use of dropout layers is a regularization technique that makes the model less likely to overfit.&lt;/p&gt;  

&lt;div class="mermaid"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" id="mermaid-svg-qmCUPkCnZ5TFF69q" height="100%" viewbox="0 0 1415.640625 112.5" style="max-width:1415.640625px;"&gt;&lt;g&gt;&lt;g class="output"&gt;&lt;g class="clusters"/&gt;&lt;g class="edgePaths"&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M191.4375,46.25L216.4375,46.25L241.4375,46.25" marker-end="url(#arrowhead84)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead84" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M335.515625,46.25L360.515625,46.25L385.515625,46.25" marker-end="url(#arrowhead85)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead85" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M539.84375,46.25L564.84375,46.25L589.84375,46.25" marker-end="url(#arrowhead86)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead86" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M796.28125,46.25L821.28125,46.25L846.28125,46.25" marker-end="url(#arrowhead87)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead87" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M1044.65625,46.25L1069.65625,46.25L1094.65625,46.25" marker-end="url(#arrowhead88)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead88" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M1188.734375,46.25L1213.734375,46.25L1238.734375,46.25" marker-end="url(#arrowhead89)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead89" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabels"&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="nodes"&gt;&lt;g class="node" style="opacity: 1;" id="id1" transform="translate(105.71875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-85.71875" y="-26.25" width="171.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-75.71875,-16.25)"&gt;&lt;foreignobject width="151.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Embedding Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id2" transform="translate(288.4765625,46.25)"&gt;&lt;rect rx="5" ry="5" x="-47.0390625" y="-26.25" width="94.078125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-37.0390625,-16.25)"&gt;&lt;foreignobject width="74.08203125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Dropout&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id3" transform="translate(462.6796875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-77.1640625" y="-26.25" width="154.328125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-67.1640625,-16.25)"&gt;&lt;foreignobject width="134.3359375" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Convolution1D&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id4" transform="translate(693.0625,46.25)"&gt;&lt;rect rx="5" ry="5" x="-103.21875" y="-26.25" width="206.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-93.21875,-16.25)"&gt;&lt;foreignobject width="186.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;GlobalMaxPooling1D&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id5" transform="translate(945.46875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-99.1875" y="-26.25" width="198.375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-89.1875,-16.25)"&gt;&lt;foreignobject width="178.37890625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Hidden Dense Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id6" transform="translate(1141.6953125,46.25)"&gt;&lt;rect rx="5" ry="5" x="-47.0390625" y="-26.25" width="94.078125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-37.0390625,-16.25)"&gt;&lt;foreignobject width="74.08203125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Dropout&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id7" transform="translate(1307.1875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-68.453125" y="-26.25" width="136.90625" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-58.453125,-16.25)"&gt;&lt;foreignobject width="116.9140625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Output Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;  
&lt;h3 id="creating-a-custom-estimator"&gt;Creating a custom estimator&lt;/h3&gt;  
&lt;p&gt;As seen in previous blog posts, the &lt;code&gt;tf.estimator&lt;/code&gt; framework provides a high-level API for training machine learning models, defining &lt;code&gt;train()&lt;/code&gt;, &lt;code&gt;evaluate()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. There is a small family of pre-made estimators, like the ones we used earlier, but it’s most likely that you will need to &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;build your own&lt;/a&gt;.&lt;/p&gt;  
&lt;p&gt;Writing a custom estimator means writing a &lt;code&gt;model_fn(features, labels, mode, params)&lt;/code&gt; that returns an &lt;code&gt;EstimatorSpec&lt;/code&gt;. The first step will be mapping the features into our embedding layer:&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;input_layer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;contrib&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;embed_sequence&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    features&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'x'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    embedding_size&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    initializer&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Then we use &lt;code&gt;tf.layers&lt;/code&gt; to process each output sequentially.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;training &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;(&lt;/span&gt;mode &lt;span class="token operator"&gt;==&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;ModeKeys&lt;span class="token punctuation"&gt;.&lt;/span&gt;TRAIN&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
dropout_emb &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dropout&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;input_layer&lt;span class="token punctuation"&gt;,&lt;/span&gt;  
                                rate&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.2&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                                training&lt;span class="token operator"&gt;=&lt;/span&gt;training&lt;span class="token punctuation"&gt;)&lt;/span&gt;
conv &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;conv1d&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    inputs&lt;span class="token operator"&gt;=&lt;/span&gt;dropout_emb&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    filters&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;32&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    kernel_size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;3&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;"same"&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    activation&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;relu&lt;span class="token punctuation"&gt;)&lt;/span&gt;
pool &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;reduce_max&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_tensor&lt;span class="token operator"&gt;=&lt;/span&gt;conv&lt;span class="token punctuation"&gt;,&lt;/span&gt; axis&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
hidden &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;pool&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;250&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; activation&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;relu&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
dropout &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dropout&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;hidden&lt;span class="token punctuation"&gt;,&lt;/span&gt; rate&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.2&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; training&lt;span class="token operator"&gt;=&lt;/span&gt;training&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
logits &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;dropout_hidden&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Finally, we will use a &lt;code&gt;Head&lt;/code&gt; to simplify the writing of our last part of the &lt;code&gt;model_fn&lt;/code&gt;. The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. This is also used in the pre-made estimators and provides us with the benefit of a uniform evaluation function across all of our models. We will use &lt;code&gt;binary_classification_head&lt;/code&gt;, which is a head for single label binary classification that uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; as the loss function under the hood.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;head &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;contrib&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;binary_classification_head&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
optimizer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;.&lt;/span&gt;AdamOptimizer&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;_train_op_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;loss&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;summary&lt;span class="token punctuation"&gt;.&lt;/span&gt;scalar&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'loss'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; loss&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; optimizer&lt;span class="token punctuation"&gt;.&lt;/span&gt;minimize&lt;span class="token punctuation"&gt;(&lt;/span&gt;
        loss&lt;span class="token operator"&gt;=&lt;/span&gt;loss&lt;span class="token punctuation"&gt;,&lt;/span&gt;
        global_step&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_global_step&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;return&lt;/span&gt; head&lt;span class="token punctuation"&gt;.&lt;/span&gt;create_estimator_spec&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    features&lt;span class="token operator"&gt;=&lt;/span&gt;features&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    labels&lt;span class="token operator"&gt;=&lt;/span&gt;labels&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    mode&lt;span class="token operator"&gt;=&lt;/span&gt;mode&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    logits&lt;span class="token operator"&gt;=&lt;/span&gt;logits&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    train_op_fn&lt;span class="token operator"&gt;=&lt;/span&gt;_train_op_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Running this model is just as easy as before:&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;initializer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;random_uniform&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; embedding_size&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1.0&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token number"&gt;1.0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
params &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; initializer&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
cnn_classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;Estimator&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_fn&lt;span class="token operator"&gt;=&lt;/span&gt;model_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt;  
                                        model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'cnn'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
                                        params&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;cnn_classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="lstm-networks"&gt;LSTM Networks&lt;/h3&gt;  
&lt;p&gt;Using the &lt;code&gt;Estimator&lt;/code&gt; API and the same model &lt;code&gt;head&lt;/code&gt;, we can also create a classifier that uses a &lt;em&gt;Long Short-Term Memory&lt;/em&gt; (&lt;em&gt;LSTM&lt;/em&gt;) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.&lt;/p&gt;  
&lt;p&gt;One of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models turn out deeper and more complex, which usually produces slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients, that said, with sufficient tuning they can obtain state-of-the-art results for many problems. As a rule of thumb CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.&lt;/p&gt;  
&lt;p&gt;Each cell processes one token embedding at a time updating its internal state based on a differentiable computation that depends on both the embedding vector &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;x_t&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.43056em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.58056em; vertical-align: -0.15em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathit"&gt;x&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.280556em;"&gt;&lt;span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathit mtight"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.15em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and the previous state &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;h_{t-1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.69444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.902771em; vertical-align: -0.208331em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathit"&gt;h&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.301108em;"&gt;&lt;span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathit mtight"&gt;t&lt;/span&gt;&lt;span class="mbin mtight"&gt;−&lt;/span&gt;&lt;span class="mord mathrm mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.208331em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. In order to get a better understanding of how LSTMs work, you can refer to Chris Olah’s &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;blog post&lt;/a&gt;.&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM Architecture"&gt;&lt;br&gt;  
&lt;small&gt;&lt;/small&gt;&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;  
Source: &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt; by &lt;strong&gt;Chris Olah&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The complete LSTM model can be expressed by the following simple flowchart:&lt;/p&gt;  
&lt;div class="mermaid"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" id="mermaid-svg-LwaRGVWE7XOXWZj1" height="100%" viewbox="0 0 578.125 206.9140625" style="max-width:578.125px;"&gt;&lt;g&gt;&lt;g class="output"&gt;&lt;g class="clusters"/&gt;&lt;g class="edgePaths"&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M191.4375,46.25L216.4375,46.25L241.4375,46.25" marker-end="url(#arrowhead102)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead102" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M272.4790948275862,72.5L241.4375,106.66666666666666L241.4375,115.20833333333334L296.328125,123.75L351.21875,115.20833333333334L351.21875,106.66666666666666L320.1771551724138,72.5" marker-end="url(#arrowhead103)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead103" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M351.21875,46.25L376.21875,46.25L401.21875,46.25" marker-end="url(#arrowhead104)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead104" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabels"&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform="translate(296.328125,123.75)"&gt;&lt;g transform="translate(-43.1640625,-16.25)" class="label"&gt;&lt;foreignobject width="86.328125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"&gt;Recursion&lt;/span&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="nodes"&gt;&lt;g class="node" style="opacity: 1;" id="id1" transform="translate(105.71875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-85.71875" y="-26.25" width="171.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-75.71875,-16.25)"&gt;&lt;foreignobject width="151.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Embedding Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id2" transform="translate(296.328125,46.25)"&gt;&lt;rect rx="5" ry="5" x="-54.890625" y="-26.25" width="109.78125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-44.890625,-16.25)"&gt;&lt;foreignobject width="89.78515625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;LSTM Cell&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id3" transform="translate(469.671875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-68.453125" y="-26.25" width="136.90625" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-58.453125,-16.25)"&gt;&lt;foreignobject width="116.9140625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Output Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;  
&lt;p&gt;In the beginning of this post, we padded all documents up to &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; tokens, which is necessary to build a proper tensor. However, when a document contains fewer than &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; words, we don’t want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence’s end. We can do this by using the &lt;code&gt;"len"&lt;/code&gt; feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;lstm_cell &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;rnn_cell&lt;span class="token punctuation"&gt;.&lt;/span&gt;BasicLSTMCell&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
_&lt;span class="token punctuation"&gt;,&lt;/span&gt; final_states &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;dynamic_rnn&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
        lstm_cell&lt;span class="token punctuation"&gt;,&lt;/span&gt; inputs&lt;span class="token punctuation"&gt;,&lt;/span&gt; sequence_length&lt;span class="token operator"&gt;=&lt;/span&gt;features&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'len'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32&lt;span class="token punctuation"&gt;)&lt;/span&gt;
logits &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;final_states&lt;span class="token punctuation"&gt;.&lt;/span&gt;h&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="pre-trained-vectors"&gt;Pre-trained vectors&lt;/h3&gt;  
&lt;p&gt;Most of the models that we have shown before rely on word embeddings as a first layer. So far, we have initialized this embedding layer randomly. However, &lt;a href="https://arxiv.org/abs/1607.01759"&gt;much&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1301.3781"&gt;previous&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1103.0398"&gt;work&lt;/a&gt; has shown that using embeddings pre-trained on a large unlabeled corpus as initialization is beneficial, particularly when training on only a small number of labeled examples. The most popular pre-trained embedding is &lt;a href="https://www.tensorflow.org/tutorials/word2vec"&gt;word2vec&lt;/a&gt;. Leveraging knowledge from unlabeled data via pre-trained embeddings is an instance of &lt;em&gt;&lt;a href="http://ruder.io/transfer-learning/"&gt;transfer learning&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;  
&lt;p&gt;To this end, we will show you how to use them in an &lt;code&gt;Estimator&lt;/code&gt;. We will use the pre-trained vectors from another popular model, &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;GloVe&lt;/a&gt;.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embeddings &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
&lt;span class="token keyword"&gt;with&lt;/span&gt; &lt;span class="token builtin"&gt;open&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'glove.6B.50d.txt'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'r'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; encoding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'utf-8'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;as&lt;/span&gt; f&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token keyword"&gt;for&lt;/span&gt; line &lt;span class="token keyword"&gt;in&lt;/span&gt; f&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        values &lt;span class="token operator"&gt;=&lt;/span&gt; line&lt;span class="token punctuation"&gt;.&lt;/span&gt;strip&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;split&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        w &lt;span class="token operator"&gt;=&lt;/span&gt; values&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;
        vectors &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;asarray&lt;span class="token punctuation"&gt;(&lt;/span&gt;values&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'float32'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        embeddings&lt;span class="token punctuation"&gt;[&lt;/span&gt;w&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; vectors
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;After loading the vectors into memory from a file we store them as a &lt;code&gt;numpy.array&lt;/code&gt; using the same indexes as our vocabulary. The created array is of shape &lt;code&gt;(5000, 50)&lt;/code&gt;. At every row index, it contains the &lt;code&gt;50&lt;/code&gt;-dimensional vector representing the word at the same index in our vocabulary.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embedding_matrix &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;random&lt;span class="token punctuation"&gt;.&lt;/span&gt;uniform&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; embedding_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token keyword"&gt;for&lt;/span&gt; w&lt;span class="token punctuation"&gt;,&lt;/span&gt; i &lt;span class="token keyword"&gt;in&lt;/span&gt; word_index&lt;span class="token punctuation"&gt;.&lt;/span&gt;items&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    v &lt;span class="token operator"&gt;=&lt;/span&gt; embeddings&lt;span class="token punctuation"&gt;.&lt;/span&gt;get&lt;span class="token punctuation"&gt;(&lt;/span&gt;w&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;if&lt;/span&gt; v &lt;span class="token keyword"&gt;is&lt;/span&gt; &lt;span class="token operator"&gt;not&lt;/span&gt; &lt;span class="token boolean"&gt;None&lt;/span&gt; &lt;span class="token operator"&gt;and&lt;/span&gt; i &lt;span class="token operator"&gt;&amp;lt;&lt;/span&gt; vocab_size&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        embedding_matrix&lt;span class="token punctuation"&gt;[&lt;/span&gt;i&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; v
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Finally, we can use a custom initializer function and pass it in the &lt;code&gt;params&lt;/code&gt; object to our &lt;code&gt;cnn_model_fn&lt;/code&gt; , without any modifications.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;my_initializer&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;shape&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;None&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32&lt;span class="token punctuation"&gt;,&lt;/span&gt; partition_info&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;None&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token keyword"&gt;assert&lt;/span&gt; dtype &lt;span class="token keyword"&gt;is&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32
    &lt;span class="token keyword"&gt;return&lt;/span&gt; embedding_matrix
params &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; my_initializer&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
cnn\_pretrained\_classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;Estimator&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    model_fn&lt;span class="token operator"&gt;=&lt;/span&gt;cnn_model_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'cnn_pretrained'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    params&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;cnn_pretrained_classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="running-tensorboard"&gt;Running TensorBoard&lt;/h3&gt;  
&lt;p&gt;Now we can launch TensorBoard and see how the different models we’ve trained compare against each other in terms of training time and performance.&lt;/p&gt;  
&lt;p&gt;In a terminal, we run&lt;/p&gt;  
&lt;pre class=" language-bash"&gt;&lt;code class="prism  language-bash"&gt;&lt;span class="token operator"&gt;&amp;gt;&lt;/span&gt; tensorboard --logdir&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;{&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;We can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds.&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/pr_curves.png" alt="PR curve"&gt;&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/loss.png" alt="loss"&gt;&lt;/p&gt;  
&lt;h3 id="getting-predictions"&gt;Getting Predictions&lt;/h3&gt;  
&lt;p&gt;To obtain predictions on new sentences we can use the &lt;code&gt;predict&lt;/code&gt; method in the &lt;code&gt;Estimator&lt;/code&gt; instances, which will load the latest checkpoint for each model and evaluate on the unseen examples. But before passing the data into the model we have to clean up, tokenize and map each token to the corresponding index as we see below.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;text_to_index&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentence&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token comment"&gt;# Remove punctuation characters except for the apostrophe&lt;/span&gt;
    translator &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token builtin"&gt;str&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;maketrans&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; string&lt;span class="token punctuation"&gt;.&lt;/span&gt;punctuation&lt;span class="token punctuation"&gt;.&lt;/span&gt;replace&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;"'"&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    tokens &lt;span class="token operator"&gt;=&lt;/span&gt; sentence&lt;span class="token punctuation"&gt;.&lt;/span&gt;translate&lt;span class="token punctuation"&gt;(&lt;/span&gt;translator&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;lower&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;split&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;+&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;word_index&lt;span class="token punctuation"&gt;[&lt;/span&gt;t&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;+&lt;/span&gt; index_offset &lt;span class="token keyword"&gt;if&lt;/span&gt; t &lt;span class="token keyword"&gt;in&lt;/span&gt; word_index &lt;span class="token keyword"&gt;else&lt;/span&gt; &lt;span class="token number"&gt;2&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; t &lt;span class="token keyword"&gt;in&lt;/span&gt; tokens&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;print_predictions&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentences&lt;span class="token punctuation"&gt;,&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    indexes &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;text_to_index&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentence&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; sentence &lt;span class="token keyword"&gt;in&lt;/span&gt; sentences&lt;span class="token punctuation"&gt;]&lt;/span&gt;
    x &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;indexes&lt;span class="token punctuation"&gt;,&lt;/span&gt;
                               maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                               padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                               value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    length &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; indexes&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predict_input_fn &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;inputs&lt;span class="token punctuation"&gt;.&lt;/span&gt;numpy_input_fn&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;"x"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; x&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;"len"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; length&lt;span class="token punctuation"&gt;}&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; shuffle&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;False&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predictions &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;p&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'logistic'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; p &lt;span class="token keyword"&gt;in&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;predict&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;predict_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;
    &lt;span class="token keyword"&gt;print&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;predictions&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;It is worth noting that the checkpoint itself is not sufficient to make predictions; the actual code used to build the estimator is necessary as well in order to map the saved weights to the corresponding tensors. It’s a good practice to associate saved checkpoints with the branch of code with which they were created.&lt;/p&gt;  
&lt;p&gt;If you are interested in exporting the models to disk in a fully recoverable way, you might want to look into the &lt;a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators"&gt;SavedModel&lt;/a&gt; class, which is especially useful for serving your model through an API using &lt;a href="https://github.com/tensorflow/serving"&gt;TensorFlow Serving&lt;/a&gt;.&lt;/p&gt;  
&lt;h3 id="summary"&gt;Summary&lt;/h3&gt;  
&lt;p&gt;In this blog post, we explored how to use estimators for text classification, in particular for the IMDB Reviews Dataset. We trained and visualized our own embeddings, as well as loaded pre-trained ones. We started from a simple baseline and made our way to convolutional neural networks and LSTMs.&lt;/p&gt;  
&lt;p&gt;For more details, be sure to check out:&lt;/p&gt;  
&lt;ul&gt;  
&lt;li&gt;A &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb"&gt;Jupyter notebook&lt;/a&gt; that can run locally, or on Colaboratory.&lt;/li&gt;  
&lt;li&gt;The complete &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py"&gt;source code&lt;/a&gt; for this blog post.&lt;/li&gt;  
&lt;li&gt;The TensorFlow &lt;a href="https://www.tensorflow.org/programmers_guide/embedding"&gt;Embedding&lt;/a&gt; guide.&lt;/li&gt;  
&lt;li&gt;The TensorFlow &lt;a href="https://www.tensorflow.org/tutorials/word2vec"&gt;Vector Representation of Words&lt;/a&gt; tutorial.&lt;/li&gt;  
&lt;li&gt;The &lt;em&gt;NLTK&lt;/em&gt; &lt;a href="http://www.nltk.org/book/ch03.html"&gt;Processing Raw Text&lt;/a&gt; chapter on how to design language pipelines.&lt;/li&gt;  
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt;  
</content:encoded></item><item><title>Requests for Research</title><description>&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentdataaugmentationfornlp"&gt;Task-independent data augmentation for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#fewshotlearningfornlp"&gt;Few-shot learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#transferlearningfornlp"&gt;Transfer learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#crosslinguallearning"&gt;Cross-lingual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentarchitectureimprovements"&gt;Task-independent architecture improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be hard to find compelling topics to work on and know what questions are interesting to ask when you are just starting as a researcher&lt;/p&gt;</description><link>http://ruder.io/requests-for-research/</link><guid isPermaLink="false">8115c479-d16e-4b22-9a0f-daf8ed5e1693</guid><category>deep learning</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 04 Mar 2018 15:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentdataaugmentationfornlp"&gt;Task-independent data augmentation for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#fewshotlearningfornlp"&gt;Few-shot learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#transferlearningfornlp"&gt;Transfer learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#crosslinguallearning"&gt;Cross-lingual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentarchitectureimprovements"&gt;Task-independent architecture improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be hard to find compelling topics to work on and know what questions are interesting to ask when you are just starting as a researcher in a new field. Machine learning research in particular moves so fast these days that it is difficult to find an opening.&lt;/p&gt;

&lt;p&gt;This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research. It gathers a collection of research topics that are interesting to me, with a focus on NLP and transfer learning. As such, they might obviously not be of interest to everyone. If you are interested in Reinforcement Learning, OpenAI provides a &lt;a href="https://blog.openai.com/requests-for-research-2/"&gt;selection of interesting RL-focused research topics&lt;/a&gt;. In case you'd like to collaborate with others or are interested in a broader range of topics, have a look at the &lt;a href="https://ai-on.org/"&gt;Artificial Intelligence Open Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most of these topics are not thoroughly thought out yet; in many cases, the general description is quite vague and subjective and many directions are possible. In addition, most of these are &lt;em&gt;not&lt;/em&gt; low-hanging fruit, so serious effort is necessary to come up with a solution. I am happy to provide feedback with regard to any of these, but will not have time to provide more detailed guidance unless you have a working proof-of-concept. I will update this post periodically with new research directions and advances in already listed ones. Note that this collection does not attempt to review the extensive literature but only aims to give a glimpse of a topic; consequently, the references won't be comprehensive.&lt;/p&gt;

&lt;p&gt;I hope that this collection will pique your interest and serve as inspiration for your own research agenda.&lt;/p&gt;

&lt;h2 id="taskindependentdataaugmentationfornlp"&gt; Task-independent data augmentation for NLP&lt;/h2&gt;

&lt;p&gt;Data augmentation aims to create additional training data by producing variations of existing training examples through transformations, which can mirror those encountered in the real world. In Computer Vision (CV), common augmentation techniques are &lt;a href="https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation"&gt;mirroring, random cropping, shearing, etc&lt;/a&gt;. Data augmentation is super useful in CV. For instance, it has been used to great effect in AlexNet (Krizhevsky et al., 2012) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] to combat overfitting and in most state-of-the-art models since. In addition, data augmentation makes intuitive sense as it makes the training data more diverse and should thus increase a model's generalization ability.&lt;/p&gt;

&lt;p&gt;However, in NLP, data augmentation is not widely used. In my mind, this is for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data in NLP is discrete. This prevents us from applying simple transformations directly to the input data. Most recently proposed augmentation methods in CV focus on such transformations, e.g. domain randomization (Tobin et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;].  &lt;/li&gt;
&lt;li&gt;Small perturbations may change the meaning. Deleting a negation may change a sentence's sentiment, while modifying a word in a paragraph might inadvertently change the answer to a question about that paragraph. This is not the case in CV where perturbing individual pixels does not change whether an image is a cat or dog and even stark changes such as interpolation of different images can be useful (Zhang et al., 2017) [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;].&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Existing approaches that I am aware of are either rule-based (Li et al., 2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] or task-specific, e.g. for parsing (Wang and Eisner, 2016) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] or zero-pronoun resolution (Liu et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;]. Xie et al. (2017) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] replace words with samples from different distributions for language modelling and Machine Translation. Recent work focuses on creating adversarial examples either by replacing words or characters (Samanta and Mehta, 2017; Ebrahimi et al., 2017) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;], concatenation (Jia and Liang, 2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;], or adding adversarial perturbations (Yasunaga et al., 2017) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;]. An adversarial setup is also used by Li et al. (2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] who train a system to produce sequences that are indistinguishable from human-generated dialogue utterances.&lt;/p&gt;

&lt;p&gt;Back-translation (Sennrich et al., 2015; Sennrich et al., 2016) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] is a common data augmentation method in Machine Translation (MT) that allows us to incorporate monolingual training data. For instance, when training a EN\(\rightarrow\)FR system, monolingual French text is translated to English using an FR\(\rightarrow\)EN system; the synthetic parallel data can then be used for training. Back-translation can also be used for paraphrasing (Mallinson et al., 2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;]. Paraphrasing has been used for data augmentation for QA (Dong et al., 2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;], but I am not aware of its use for other tasks.&lt;/p&gt;

&lt;p&gt;Another method that is close to paraphrasing is generating sentences from a continuous space using a variational autoencoder (Bowman et al., 2016; Guu et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;]. If the representations are disentangled as in (Hu et al., 2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;], then we are also not too far from style transfer (Shen et al., 2017) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;There are a few research directions that would be interesting to pursue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Evaluation study:&lt;/strong&gt; Evaluate a range of existing data augmentation methods as well as techniques that have not been widely used for augmentation such as paraphrasing and style transfer on a diverse range of tasks including text classification and sequence labelling. Identify what types of data augmentation are robust across task and which are task-specific. This could be packaged as a software library to make future benchmarking easier (think &lt;a href="https://github.com/tensorflow/cleverhans"&gt;CleverHans&lt;/a&gt; for NLP).  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data augmentation with style transfer:&lt;/strong&gt; Investigate if style transfer can be used to modify various attributes of training examples for more robust learning.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn the augmentation:&lt;/strong&gt; Similar to Dong et al. (2017) we could learn either to paraphrase or to generate transformations for a particular task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn a word embedding space for data augmentation:&lt;/strong&gt; A typical word embedding space clusters synonyms and antonyms together; using nearest neighbours in this space for replacement is thus infeasible. Inspired by recent work (Mrkšić et al., 2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;], we could specialize the word embedding space to make it more suitable for data augmentation.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial data augmentation:&lt;/strong&gt; Related to recent work in interpretability (Ribeiro et al., 2016) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;], we could change the most salient words in an example, i.e. those that a model depends on for a prediction. This still requires a semantics-preserving replacement method, however.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="fewshotlearningfornlp"&gt; Few-shot learning for NLP&lt;/h2&gt;

&lt;p&gt;Zero-shot, one-shot and few-shot learning are one of the most interesting recent research directions IMO. Following the key insight from Vinyals et al. (2016) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;] that a few-shot learning model should be explicitly trained to perform few-shot learning, we have seen several recent advances (Ravi and Larochelle, 2017; Snell et al., 2017) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Learning from few labeled samples is one of the hardest problems IMO and one of the core capabilities that separates the current generation of ML models from more generally applicable systems. Zero-shot learning has only been investigated in the context of &lt;a href="http://ruder.io/word-embeddings-2017/index.html#oovhandling"&gt;learning word embeddings for unknown words&lt;/a&gt; AFAIK. Dataless classification (Song and Roth, 2014; Song et al., 2016) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;] is an interesting related direction that embeds labels and documents in a joint space, but requires interpretable labels with good descriptions. &lt;/p&gt;

&lt;p&gt;Potential research directions are the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Standardized benchmarks:&lt;/strong&gt; Create standardized benchmarks for few-shot learning for NLP. Vinyals et al. (2016) introduce a one-shot language modelling task for the Penn Treebank. The task, while useful, is dwarfed by the extensive evaluation on CV benchmarks and has not seen much use AFAIK. A few-shot learning benchmark for NLP should contain a large number of classes and provide a standardized split for reproducibility. Good candidate tasks would be topic classification or fine-grained entity recognition.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation study&lt;/strong&gt;: After creating such a benchmark, the next step would be to evaluate how well existing few-shot learning models from CV perform for NLP.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Novel methods for NLP&lt;/strong&gt;: Given a dataset for benchmarking and an empirical evaluation study, we could then start developing novel methods that can perform few-shot learning for NLP.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="transferlearningfornlp"&gt; Transfer learning for NLP&lt;/h2&gt;

&lt;p&gt;Transfer learning has had a large impact on computer vision (CV) and has greatly lowered the entry threshold for people wanting to apply CV algorithms to their own problems. CV practicioners are no longer required to perform extensive feature-engineering for every new task, but can simply fine-tune a model pretrained on a large dataset with a small number of examples.&lt;/p&gt;

&lt;p&gt;In NLP, however, we have so far only been pretraining the first layer of our models via pretrained embeddings. Recent approaches (Peters et al., 2017, 2018) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] add pretrained language model embedddings, but these still require custom architectures for every task. In my opinion, in order to unlock the true potential of transfer learning for NLP, we need to pretrain the entire model and fine-tune it on the target task, akin to fine-tuning ImageNet models. Language modelling, for instance, is a great task for pretraining and could be to NLP what ImageNet classification is to CV (Howard and Ruder, 2018) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Here are some potential research directions in this context:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify useful pretraining tasks:&lt;/strong&gt; The choice of the pretraining task is very important as even fine-tuning a model on a related task might only provide limited success (Mou et al., 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;]. Other tasks such as those explored in recent work on learning general-purpose sentence embeddings (Conneau et al., 2017; Subramanian et al., 2018; Nie et al., 2017) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;] might be complementary to language model pretraining or suitable for other target tasks.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-tuning of complex architectures:&lt;/strong&gt; Pretraining is most useful when a model can be applied to many target tasks. However, it is still unclear how to pretrain more complex architectures, such as those used for pairwise classification tasks (Augenstein et al., 2018) or reasoning tasks such as QA or reading comprehension.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="multitasklearning"&gt; Multi-task learning&lt;/h2&gt;

&lt;p&gt;Multi-task learning (MTL) has become more commonly used in NLP. See &lt;a href="http://ruder.io/multi-task/"&gt;here&lt;/a&gt; for a general overview of multi-task learning and &lt;a href="http://ruder.io/multi-task-learning-nlp/"&gt;here&lt;/a&gt; for MTL objectives for NLP. However, there is still much we don't understand about multi-task learning in general.&lt;/p&gt;

&lt;p&gt;The main questions regarding MTL give rise to many interesting research directions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify effective auxiliary tasks:&lt;/strong&gt; One of the main questions is which tasks are useful for multi-task learning. Label entropy has been shown to be a predictor of MTL success (Alonso and Plank, 2017) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;], but this does not tell the whole story. In recent work (Augenstein et al., 2018) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;], we have found that auxiliary tasks with more data and more fine-grained labels are more useful. It would be useful if future MTL papers would not only propose a new model or auxiliary task, but also try to understand why a certain auxiliary task might be better than another closely related one.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alternatives to hard parameter sharing:&lt;/strong&gt; Hard parameter sharing is still the default modus operandi for MTL, but places a strong constraint on the model to compress knowledge pertaining to different tasks with the same parameters, which often makes learning difficult. We need better ways of doing MTL that are easy to use and work reliably across many tasks. Recently proposed methods such as cross-stitch units (Misra et al., 2017; Ruder et al., 2017) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] and a label embedding layer (Augenstein et al., 2018) are promising steps in this direction.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Artificial auxiliary tasks:&lt;/strong&gt; The best auxiliary tasks are those, which are tailored to the target task and do not require any additional data. I have outlined a list of potential &lt;em&gt;artificial&lt;/em&gt; auxiliary tasks &lt;a href="http://ruder.io/multi-task-learning-nlp/"&gt;here&lt;/a&gt;. However, it is not clear which of these work reliably across a number of diverse tasks or what variations or task-specific modifications are useful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="crosslinguallearning"&gt; Cross-lingual learning&lt;/h2&gt;

&lt;p&gt;Creating models that perform well across languages and that can transfer knowledge from resource-rich to resource-poor languages is one of the most important research directions IMO. There has been much progress in learning cross-lingual representations that project different languages into a shared embedding space. Refer to Ruder et al. (2017) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] for a survey.&lt;/p&gt;

&lt;p&gt;Cross-lingual representations are commonly evaluated either intrinsically on similarity benchmarks or extrinsically on downstream tasks, such as text classification. While recent methods have advanced the state-of-the-art for many of these settings, we do not have a good understanding of the tasks or languages for which these methods fail and how to mitigate these failures in a task-independent manner, e.g. by injecting task-specific constraints (Mrkšić et al., 2017).&lt;/p&gt;

&lt;h2 id="taskindependentarchitectureimprovements"&gt; Task-independent architecture improvements&lt;/h2&gt;

&lt;p&gt;Novel architectures that outperform the current state-of-the-art and are tailored to specific tasks are regularly introduced, superseding the previous architecture. I have outlined &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/"&gt;best practices for different NLP tasks&lt;/a&gt; before, but without comparing such architectures on different tasks, it is often hard to gain insights from specialized architectures and tell which components would also be useful in other settings. &lt;/p&gt;

&lt;p&gt;A particularly promising recent model is the Transformer (Vaswani et al., 2017) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. While the complete model might not be appropriate for every task, components such as multi-head attention or position-based encoding could be building blocks that are generally useful for many NLP tasks.&lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope you've found this collection of research directions useful. If you have suggestions on how to tackle some of these problems or ideas for related research topics, feel free to comment below.&lt;/p&gt;

&lt;h2 id="references"&gt; References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105). &lt;a href="http://ruder.io/requests-for-research/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., &amp;amp; Abbeel, P. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. arXiv Preprint arXiv:1703.06907. Retrieved from &lt;a href="http://arxiv.org/abs/1703.06907"&gt;http://arxiv.org/abs/1703.06907&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Zhang, H., Cisse, M., Dauphin, Y. N., &amp;amp; Lopez-Paz, D. (2017). mixup: Beyond Empirical Risk Minimization, 1–11. Retrieved from &lt;a href="http://arxiv.org/abs/1710.09412"&gt;http://arxiv.org/abs/1710.09412&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp;amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. NIPS 2016. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04080"&gt;http://arxiv.org/abs/1606.04080&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Li, Y., Cohn, T., &amp;amp; Baldwin, T. (2017). Robust Training under Linguistic Adversity. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (Vol. 2, pp. 21–27). &lt;a href="http://ruder.io/requests-for-research/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Wang, D., &amp;amp; Eisner, J. (2016). The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages. Tacl, 4, 491–505. Retrieved from &lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212%0Ahttps://transacl.org/ojs/index.php/tacl/article/view/917"&gt;https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212%0Ahttps://transacl.org/ojs/index.php/tacl/article/view/917&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Liu, T., Cui, Y., Yin, Q., Zhang, W., Wang, S., &amp;amp; Hu, G. (2017). Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 102–111). &lt;a href="http://ruder.io/requests-for-research/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Samanta, S., &amp;amp; Mehta, S. (2017). Towards Crafting Text Adversarial Samples. arXiv preprint arXiv:1707.02812. &lt;a href="http://ruder.io/requests-for-research/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Ebrahimi, J., Rao, A., Lowd, D., &amp;amp; Dou, D. (2017). HotFlip: White-Box Adversarial Examples for NLP. Retrieved from &lt;a href="http://arxiv.org/abs/1712.06751"&gt;http://arxiv.org/abs/1712.06751&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Yasunaga, M., Kasai, J., &amp;amp; Radev, D. (2017). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.04903"&gt;http://arxiv.org/abs/1711.04903&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Jia, R., &amp;amp; Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2015). Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709. &lt;a href="http://ruder.io/requests-for-research/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891. &lt;a href="http://ruder.io/requests-for-research/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Mallinson, J., Sennrich, R., &amp;amp; Lapata, M. (2017). Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers (Vol. 1, pp. 881-893). &lt;a href="http://ruder.io/requests-for-research/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Dong, L., Mallinson, J., Reddy, S., &amp;amp; Lapata, M. (2017). Learning to Paraphrase for Question Answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Li, J., Monroe, W., Shi, T., Ritter, A., &amp;amp; Jurafsky, D. (2017). Adversarial Learning for Neural Dialogue Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1701.06547"&gt;http://arxiv.org/abs/1701.06547&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp;amp; Bengio, S. (2016). Generating Sentences from a Continuous Space. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Retrieved from &lt;a href="http://arxiv.org/abs/1511.06349"&gt;http://arxiv.org/abs/1511.06349&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., &amp;amp; Xing, E. P. (2017). Toward Controlled Generation of Text. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/requests-for-research/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Guu, K., Hashimoto, T. B., Oren, Y., &amp;amp; Liang, P. (2017). Generating Sentences by Editing Prototypes. &lt;a href="http://ruder.io/requests-for-research/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Shen, T., Lei, T., Barzilay, R., &amp;amp; Jaakkola, T. (2017). Style Transfer from Non-Parallel Text by Cross-Alignment. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1705.09655"&gt;http://arxiv.org/abs/1705.09655&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Mrkšić, N., Vulić, I., Séaghdha, D. Ó., Leviant, I., Reichart, R., Gašić, M., … Young, S. (2017). Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints. TACL. Retrieved from &lt;a href="http://arxiv.org/abs/1706.00374"&gt;http://arxiv.org/abs/1706.00374&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ribeiro, M. T., Singh, S., &amp;amp; Guestrin, C. (2016, August). Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). ACM. &lt;a href="http://ruder.io/requests-for-research/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Ravi, S., &amp;amp; Larochelle, H. (2017). Optimization as a Model for Few-Shot Learning. In ICLR 2017. &lt;a href="http://ruder.io/requests-for-research/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Snell, J., Swersky, K., &amp;amp; Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/requests-for-research/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Song, Y., &amp;amp; Roth, D. (2014). On dataless hierarchical text classification. Proceedings of AAAI, 1579–1585. Retrieved from &lt;a href="http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf"&gt;http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Song, Y., Upadhyay, S., Peng, H., &amp;amp; Roth, D. (2016). Cross-Lingual Dataless Classification for Many Languages. Ijcai, 2901–2907. &lt;a href="http://ruder.io/requests-for-research/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Augenstein, I., Ruder, S., &amp;amp; Søgaard, A. (2018). Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces. In Proceedings of NAACL 2018. &lt;a href="http://ruder.io/requests-for-research/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Alonso, H. M., &amp;amp; Plank, B. (2017). When is multitask learning effective? Multitask learning for semantic sequence prediction under varying data conditions. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1612.02251"&gt;http://arxiv.org/abs/1612.02251&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Misra, I., Shrivastava, A., Gupta, A., &amp;amp; Hebert, M. (2016). Cross-stitch Networks for Multi-task Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. &lt;a href="http://doi.org/10.1109/CVPR.2016.433"&gt;http://doi.org/10.1109/CVPR.2016.433&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv preprint arXiv:1705.08142. &lt;a href="http://ruder.io/requests-for-research/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/requests-for-research/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp;amp; Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings of NAACL. &lt;a href="http://ruder.io/requests-for-research/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Howard, J., &amp;amp; Ruder, S. (2018). Fine-tuned Language Models for Text Classification. arXiv preprint arXiv:1801.06146. &lt;a href="http://ruder.io/requests-for-research/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp;amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Subramanian, S., Trischler, A., Bengio, Y., &amp;amp; Pal, C. J. (2018). Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Proceedings of ICLR 2018. &lt;a href="http://ruder.io/requests-for-research/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Ruder, S., Vulić, I., &amp;amp; Søgaard, A. (2017). A Survey of Cross-lingual Word Embedding Models. arXiv Preprint arXiv:1706.04902. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04902"&gt;http://arxiv.org/abs/1706.04902&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/requests-for-research/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Mou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L., &amp;amp; Jin, Z. (2016). How Transferable are Neural Networks in NLP Applications? Proceedings of 2016 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Xie, Z., Wang, S. I., Li, J., Levy, D., Nie, A., Jurafsky, D., &amp;amp; Ng, A. Y. (2017). Data Noising as Smoothing in Neural Network Language Models. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/requests-for-research/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Nie, A., Bennett, E. D., &amp;amp; Goodman, N. D. (2017). DisSent: Sentence Representation Learning from Explicit Discourse Relations. arXiv Preprint arXiv:1710.04334. Retrieved from &lt;a href="http://arxiv.org/abs/1710.04334"&gt;http://arxiv.org/abs/1710.04334&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Optimization for Deep Learning Highlights in 2017</title><description>An overview of the most exciting highlights and research directions in optimization for Deep Learning in 2017.</description><link>http://ruder.io/deep-learning-optimization-2017/</link><guid isPermaLink="false">24033557-01b5-4d3a-aecf-9497f83f90a9</guid><category>deep learning</category><category>optimization</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 03 Dec 2017 15:36:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/12/snapshot_ensembles.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/12/snapshot_ensembles.png" alt="Optimization for Deep Learning Highlights in 2017"&gt;&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#improvingadam"&gt;Improving Adam&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#decouplingweightdecay"&gt;Decoupling weight decay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fixingtheexponentialmovingaverage"&gt;Fixing the exponential moving average&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#tuningthelearningrate"&gt;Tuning the learning rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#warmrestarts"&gt;Warm restarts&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#sgdwithrestarts"&gt;SGD with restarts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#snapshotensembles"&gt;Snapshot ensembles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#adamwithrestarts"&gt;Adam with restarts&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#learningtooptimize"&gt;Learning to optimize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#understandinggeneralization"&gt;Understanding generalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deep Learning ultimately is about finding a minimum that generalizes well -- with bonus points for finding one fast and reliably. Our workhorse, stochastic gradient descent (SGD), is a 60-year old algorithm (Robbins and Monro, 1951) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;], that is as essential to the current generation of Deep Learning algorithms as back-propagation.&lt;/p&gt;

&lt;p&gt;Different optimization algorithms have been proposed in recent years, which use different equations to update a model's parameters. Adam (Kingma and Ba, 2015) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] was introduced in 2015 and is arguably today still the most commonly used one of these algorithms. This indicates that from the Machine Learning practitioner's perspective, best practices for optimization for Deep Learning have largely remained the same.&lt;/p&gt;

&lt;p&gt;New ideas, however, have been developed over the course of this year, which may shape the way will optimize our models in the future. In this blog post, I will touch on the most exciting highlights and most promising directions in optimization for Deep Learning in my opinion. Note that this blog post assumes a familiarity with SGD and with adaptive learning rate methods such as Adam. To get up to speed, refer to &lt;a href="http://ruder.io/optimizing-gradient-descent/index.html"&gt;this blog post&lt;/a&gt; for an overview of existing gradient descent optimization algorithms.&lt;/p&gt;

&lt;h2 id="improvingadam"&gt;Improving Adam&lt;/h2&gt;

&lt;p&gt;Despite the apparent supremacy of adaptive learning rate methods such as Adam, state-of-the-art results for many tasks in computer vision and NLP such as object recognition (Huang et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] or machine translation (Wu et al., 2016) [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] have still been achieved by plain old SGD with momentum. Recent theory (Wilson et al., 2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] provides some justification for this, suggesting that adaptive learning rate methods converge to different (and less optimal) minima than SGD with momentum. It is empirically shown that the minima found by adaptive learning rate methods perform generally worse compared to those found by SGD with momentum on object recognition, character-level language modeling, and constituency parsing. This seems counter-intuitive given that Adam comes with nice convergence guarantees and that its adaptive learning rate should give it an edge over the regular SGD. However, Adam and other adaptive learning rate methods are not without their own flaws.&lt;/p&gt;

&lt;h3 id="decouplingweightdecay"&gt; Decoupling weight decay&lt;/h3&gt;

&lt;p&gt;One factor that partially accounts for Adam's poor generalization ability compared with SGD with momentum on some datasets is weight decay. Weight decay is most commonly used in image classification problems and decays the weights \(\theta_t\) after every parameter update by multiplying them by a decay rate \(w_t\) that is slightly less than \(1\):&lt;/p&gt;

&lt;p&gt;\(\theta_{t+1} = w_t \: \theta_t \)&lt;/p&gt;

&lt;p&gt;This prevents the weights from growing too large. As such, weight decay can also be understood as an \(\ell_2\) regularization term that depends on the weight decay rate \(w_t\) added to the loss:&lt;/p&gt;

&lt;p&gt;\(\mathcal{L}_\text{reg} = \dfrac{w_t}{2} \|\theta_t \|^2_2 \)&lt;/p&gt;

&lt;p&gt;Weight decay is commonly implemented in many neural network libraries either as the above regularization term or directly to modify the gradient. As the gradient is modified in both the momentum and Adam update equations (via multiplication with other decay terms), weight decay no longer equals \(\ell_2\) regularization. Loshchilov and Hutter (2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] thus propose to decouple weight decay from the gradient update by adding it after the parameter update as in the original definition. &lt;br&gt;
The SGD with momentum and weight decay (SGDW) update then looks like the following:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
v_t &amp;amp;= \gamma v_{t-1} + \eta g_t \\ &lt;br&gt;
\theta_{t+1} &amp;amp;= \theta_t - v_t - \eta w_t \theta_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;where \(\eta\) is the learning rate and the third term in the second equation is the decoupled weight decay. Similarly, for Adam with weight decay (AdamW) we obtain:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
m_t &amp;amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ &lt;br&gt;
v_t &amp;amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\ &lt;br&gt;
\hat{m}_t &amp;amp;= \dfrac{m_t}{1 - \beta^t_1} \\
\hat{v}_t &amp;amp;= \dfrac{v_t}{1 - \beta^t_2} \\
\theta_{t+1} &amp;amp;= \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t - \eta w_t \theta_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;where \(m_t\) and \(\hat{m}_t\) and \(v_t\) and \(\hat{v}_t\) are the biased and bias-corrected estimates of the first and second moments respectively and \(\beta_1\) and \(\beta_2\) are their decay rates, with the same weight decay term added to it. The authors show that this substantially improves Adam’s generalization performance and allows it to compete with SGD with momentum on image classification datasets.&lt;/p&gt;

&lt;p&gt;In addition, it decouples the choice of the learning rate from the choice of the weight decay, which enables better hyperparameter optimization as the hyperparameters no longer depend on each other. It also separates the implementation of the optimizer from the implementation of the weight decay, which contributes to cleaner and more reusable code (see e.g. the &lt;a href="https://github.com/fastai/fastai/pull/46/files"&gt;fast.ai AdamW/SGDW implementation&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id="fixingtheexponentialmovingaverage"&gt;Fixing the exponential moving average&lt;/h3&gt;

&lt;p&gt;Several recent papers (Dozat and Manning, 2017; Laine and Aila, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] empirically find that a lower \(\beta_2\) value, which controls the contribution of the exponential moving average of past squared gradients in Adam, e.g. \(0.99\) or \(0.9\) vs. the default \(0.999\) worked better in their respective applications, indicating that there might be an issue with the exponential moving average.&lt;/p&gt;

&lt;p&gt;An &lt;a href="https://openreview.net/forum?id=ryQu7f-RZ"&gt;ICLR 2018 submission&lt;/a&gt; formalizes this issue and pinpoints the exponential moving average of past squared gradients as another reason for the poor generalization behaviour of adaptive learning rate methods. Updating the parameters via an exponential moving average of past squared gradients is at the heart of adaptive learning rate methods such as Adadelta, RMSprop, and Adam. The contribution of the exponential average is well-motivated: It should prevent the learning rates to become infinitesimally small as training progresses, the key flaw of the Adagrad algorithm. However, this short-term memory of the gradients becomes an obstacle in other scenarios.&lt;/p&gt;

&lt;p&gt;In settings where Adam converges to a suboptimal solution, it has been observed that some minibatches provide large and informative gradients, but as these minibatches only occur rarely, exponential averaging diminishes their influence, which leads to poor convergence. The authors provide an example for a simple convex optimization problem where the same behaviour can be observed for Adam.&lt;/p&gt;

&lt;p&gt;To fix this behaviour, the authors propose a new algorithm, AMSGrad that uses the maximum of past squared gradients rather than the exponential average to update the parameters. The full AMSGrad update without bias-corrected estimates can be seen below:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
m_t &amp;amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ &lt;br&gt;
v_t &amp;amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\ &lt;br&gt;
\hat{v}_t &amp;amp;= \text{max}(\hat{v}_{t-1}, v_t) \\
\theta_{t+1} &amp;amp;= \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} m_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;The authors observe improved performance compared to Adam on small datasets and on CIFAR-10.&lt;/p&gt;

&lt;h2 id="tuningthelearningrate"&gt;Tuning the learning rate&lt;/h2&gt;

&lt;p&gt;In many cases, it is not our models that require improvement and tuning, but our hyperparameters. Recent examples for language modelling demonstrate that tuning LSTM parameters (Melis et al., 2017) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;] and regularization parameters (Merity et al., 2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] can yield state-of-the-art results compared to more complex models.&lt;/p&gt;

&lt;p&gt;An important hyperparameter for optimization in Deep Learning is the learning rate \(\eta\). In fact, SGD has been shown to require a learning rate annealing schedule to converge to a good minimum in the first place. It is often thought that adaptive learning rate methods such as Adam are more robust to different learning rates, as they update the learning rate themselves. Even for these methods, however, there can be a large difference between a good and the optimal learning rate (psst... it's &lt;a href="https://twitter.com/karpathy/status/801621764144971776"&gt;\(3e-4\)&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;Zhang et al. (2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] show that SGD with a tuned learning rate annealing schedule and momentum parameter is not only competitive with Adam, but also converges faster. On the other hand, while we might think that the adaptivity of Adam's learning rates might mimic learning rate annealing, an explicit annealing schedule can still be beneficial: If we add SGD-style learning rate annealing to Adam, it converges faster and outperforms SGD on Machine Translation (Denkowski and Neubig, 2017) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;In fact, learning rate annealing schedule engineering seems to be the new feature engineering as we can often find highly-tuned learning rate annealing schedules that improve the final convergence behaviour of our model. An interesting example of this is Vaswani et al. (2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;]. While it is usual to see a model's hyperparameters being subjected to large-scale hyperparameter optimization, it is interesting to see a learning rate annealing schedule as the focus of the same attention to detail: The authors use Adam with \(\beta_1=0.9\), a non-default \(\beta_2=0.98\), \(\epsilon = 10^{-9}\), and arguably one of the most elaborate annealing schedules for the learning rate \(\eta\):&lt;/p&gt;

&lt;p&gt;\(\eta  = d_\text{model}^{-0.5} \cdot \min(step\text{_}num^{-0.5}, step\text{_}num \cdot warmup\text{_}steps^{-1.5}) \)&lt;/p&gt;

&lt;p&gt;where \(d_\text{model}\) is the number of parameters of the model and \(warmup\text{_}steps = 4000\). &lt;/p&gt;

&lt;p&gt;Another recent paper by Smith et al. (2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] demonstrates an interesting connection between the learning rate and the batch size, two hyperparameters that are typically thought to be independent of each other: They show that decaying the learning rate is equivalent to increasing the batch size, while the latter allows for increased parallelism. Conversely, we can reduce the number of model updates and thus speed up training by increasing the learning rate and scaling the batch size. This has ramifications for large-scale Deep Learning, which can now repurpose existing training schedules with no hyperparameter tuning.&lt;/p&gt;

&lt;h2 id="warmrestarts"&gt;Warm restarts&lt;/h2&gt;

&lt;h3 id="sgdwithrestarts"&gt; SGD with restarts&lt;/h3&gt;

&lt;p&gt;Another effective recent development is SGDR (Loshchilov and Hutter, 2017) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;], an SGD alternative that uses warm restarts instead of learning rate annealing. In each restart, the learning rate is initialized to some value and is scheduled to decrease. Importantly, the restart is warm as the optimization does not start from scratch but from the parameters to which the model converged during the last step. The key factor is that the learning rate is decreased with an aggressive cosine annealing schedule, which rapidly lowers the learning rate and looks like the following:&lt;/p&gt;

&lt;p&gt;\(\eta_t = \eta_{min}^i + \dfrac{1}{2}(\eta_{max}^i - \eta_{min}^i)(1 + \text{cos}(\dfrac{T_{cur}}{T_i}\pi)) \)&lt;/p&gt;

&lt;p&gt;where \(\eta_{min}^i\) and \(\eta_{max}^i\) are ranges for the learning rate during the \(i\)-th run, \(T_{cur}\) indicates how many epochs passed since the last restart, and \(T_i\) specifies the epoch of the next restart. The warm restart schedules for \(T_i=50\), \(T_i=100\), and \(T_i=200\) compared with regular learning rate annealing are shown in Figure 1.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/warm_restarts.png" style="width: 100%" title="Learning rate schedules with warm restarts" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 1: Learning rate schedules with warm restarts (Loshchilov and Hutter,  
 2017)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The high initial learning rate after a restart is used to essentially catapult the parameters out of the minimum to which they previously converged and to a different area of the loss surface. The aggressive annealing then enables the model to rapidly converge to a new and better solution. The authors empirically find that SGD with warm restarts requires 2 to 4 times fewer epochs than learning rate annealing and achieves comparable or better performance.&lt;/p&gt;

&lt;p&gt;Learning rate annealing with warm restarts is also known as cyclical learning rates and has been originally proposed by Smith (2017) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;]. Two more articles by students of &lt;a href="http://www.fast.ai/"&gt;fast.ai&lt;/a&gt; (which has recently started to teach this method) that discuss warm restarts and cyclical learning rates can be found &lt;a href="https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b"&gt;here&lt;/a&gt; and &lt;a href="http://teleported.in/posts/cyclic-learning-rate/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="snapshotensembles"&gt; Snapshot ensembles&lt;/h3&gt;

&lt;p&gt;Snapshot ensembles (Huang et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] are a clever, recent technique that uses warm restarts to assemble an ensemble essentially for free when training a single model. The method trains a single model until convergence with the cosine annealing schedule that we have seen above. It then saves the model parameters, performs a warm restart, and then repeats these steps \(M\) times. In the end, all saved model snapshots are ensembled. The common SGD optimization behaviour on an error surface compared to the behaviour of snapshot ensembling can be seen in Figure 2.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/snapshot_ensembles.png" style="width: 100%" title="Learning rate schedules with warm restarts" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 2: SGD vs. snapshot ensemble (Huang et al., 2017)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The success of ensembling in general relies on the diversity of the individual models in the ensemble. Snapshot ensembling thus relies on the cosine annealing schedule's ability to enable the model to converge to a different local optimum after every restart. The authors demonstrate that this holds in practice, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN.&lt;/p&gt;

&lt;h3 id="adamwithrestarts"&gt;Adam with restarts&lt;/h3&gt;

&lt;p&gt;Warm restarts did not work originally with Adam due to its dysfunctional weight decay, which we have seen before. After fixing weight decay, Loshchilov and Hutter (2017) similarly extend Adam to work with warm restarts. They set \(\eta_{min}^i=0\) and \(\eta_{max}^i=1\), which yields:&lt;/p&gt;

&lt;p&gt;\(\eta_t = 0.5 + 0.5 \: \text{cos}(\dfrac{T_{cur}}{T_i}\pi))\)&lt;/p&gt;

&lt;p&gt;They recommend to start with an initially small \(T_i\) (between \(1%\) and \(10%\) of the total number of epochs) and multiply it by a factor of \(T_{mult}\) (e.g. \(T_{mult}=2\)) at every restart.&lt;/p&gt;

&lt;h2 id="learningtooptimize"&gt;Learning to optimize&lt;/h2&gt;

&lt;p&gt;One of the most interesting papers of last year (and &lt;a href="https://www.reddit.com/r/MachineLearning/comments/5n53k7/d_results_from_the_best_paper_awards/"&gt;reddit's "Best paper name of 2016" winner&lt;/a&gt;) was a paper by Andrychowicz et al. (2016) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] where they train an LSTM optimizer to provide the updates to the main model during training. Unfortunately, learning a separate LSTM optimizer or even using a pre-trained LSTM optimizer for optimization greatly increases the complexity of model training.&lt;/p&gt;

&lt;p&gt;Another very influential learning-to-learn paper from this year uses an LSTM to generate model architectures in a domain-specific language (Zoph and Quoc, 2017) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;]. While the search process requires vast amounts of resources, the discovered architectures can be used as-is to replace their existing counterparts. This search process has proved effective and found architectures that achieve state-of-the-art results on language modeling and results competitive with the state-of-the-art on CIFAR-10.&lt;/p&gt;

&lt;p&gt;The same search principle can be applied to any other domain where key processes have been previously defined by hand. One such domain are optimization algorithms for Deep Learning. As we have seen before, optimization algorithms are more similar than they seem: All of them use a combination of an exponential moving average of past gradients (as in momentum) and of an exponential moving average of past squared gradients (as in Adadelta, RMSprop, and Adam) (Ruder, 2016) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Bello et al. (2017) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] define a domain-specific language that consists of primitives useful for optimization such as these exponential moving averages. They then sample an update rule from the space of possible update rules, use this update rule to train a model, and update the RNN controller based on the performance of the trained model on the test set. The full procedure can be seen in Figure 3. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/neural_optimizer_search.png" style="width: 100%" title="Neural Optimizer Search" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 3: Neural Optimizer Search (Bello et al., 2017)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In particular, they discover two update equations, PowerSign and AddSign. The update equation for PowerSign is the following:&lt;/p&gt;

&lt;p&gt;\( \theta_{t+1} = \theta_{t} - \alpha^{f(t)*
\text{sign}(g_t)*\text{sign}(m_t)}*g_t \)&lt;/p&gt;

&lt;p&gt;where \(\alpha\) is a hyperparameter that is often set to \(e\) or \(2\), \(f(t)\) is either \(1\) or a decay function that performs linear, cyclical or decay with restarts based on time step \(t\), and \(m_t\) is the moving average of past gradients. The common configuration uses \(\alpha=e\) and no decay.  We can observe that the update scales the gradient by \(\alpha^{f(t)}\) or \(1/\alpha^{f(t)}\) depending on whether the direction of the gradient and its moving average agree. This indicates that this momentum-like agreement between past gradients and the current one is a key piece of information for optimizing Deep Learning models.&lt;/p&gt;

&lt;p&gt;AddSign in turn is defined as follows:&lt;/p&gt;

&lt;p&gt;\( \theta_{t+1} = \theta_{t} - \alpha + f(t) * \text{sign}(g_t) * \text{sign}(m_t)) * g_t\)&lt;/p&gt;

&lt;p&gt;with \(\alpha\) often set to \(1\) or \(2\). Similar to the above, this time the update scales \(\alpha + f(t)\) or \(\alpha - f(t)\) again depending on the agreement of the direction of the gradients. The authors show that PowerSign and AddSign outperform Adam, RMSprop, and SGD with momentum on CIFAR-10 and transfer well to other tasks such as ImageNet classification and machine translation.&lt;/p&gt;

&lt;h2 id="understandinggeneralization"&gt;Understanding generalization&lt;/h2&gt;

&lt;p&gt;Optimization is closely tied to generalization as the minimum to which a model converges defines how well the model generalizes. Advances in optimization are thus closely correlated with theoretical advances in understanding the generalization behaviour of such minima and more generally of gaining a deeper understanding of generalization in Deep Learning.&lt;/p&gt;

&lt;p&gt;However, our understanding of the generalization behaviour of deep neural networks is still very shallow. Recent work showed that the number of possible local minima grows exponentially with the number of parameters (Kawaguchi, 2016) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;]. Given the huge number of parameters of current Deep Learning architectures, it still seems almost magical that such models converge to solutions that generalize well, in particular given that they can completely memorize random inputs (Zhang et al., 2017) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Keskar et al. (2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] identify the sharpness of a minimum as a source for poor generalization: In particular, they show that sharp minima found by batch gradient descent have high generalization error. This makes intuitive sense, as we generally would like our functions to be smooth and a sharp minima indicates a high irregularity in the corresponding error surface. However, more recent work suggests that sharpness may not be such a good indicator after all by showing that local minima that generalize well can be made arbitrarily sharp (Dinh et al., 2017) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;]. A &lt;a href="https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important/answer/Eric-Jang?srid=dWc3"&gt;Quora answer by Eric Jang&lt;/a&gt; also discusses these articles.&lt;/p&gt;

&lt;p&gt;An &lt;a href="https://openreview.net/forum?id=r1iuQjxCZ"&gt;ICLR 2018 submission&lt;/a&gt; demonstrates through a series of ablation analyses that a model's reliance on single directions in activation space, i.e. the activation of single units or feature maps is a good predictor of its generalization performance. They show that this holds across models trained on different datasets and for different degrees of label corruption. They find that dropout does not help to resolve this, while batch normalization discourages single direction reliance.&lt;/p&gt;

&lt;p&gt;While these findings indicate that there is still much we do not know in terms of Optimization for Deep Learning, it is important to remember that convergence guarantees and a large body of work exists for convex optimization and that existing ideas and insights can also be applied to non-convex optimization to some extent. The large-scale optimization tutorial at NIPS 2016 provides an excellent overview of more theoretical work in this area (see the &lt;a href="https://www.di.ens.fr/~fbach/fbach_tutorial_vr_nips_2016.pdf"&gt;slides part 1&lt;/a&gt;, &lt;a href="http://suvrit.de/talks/vr_nips16_sra.pdf"&gt;part 2&lt;/a&gt;, and the &lt;a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Large-Scale-Optimization-Beyond-Stochastic-Gradient-Descent-and-Convexity"&gt;video&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope that I was able to provide an impression of some of the compelling developments in optimization for Deep Learning over the past year. I've undoubtedly failed to mention many other approaches that are equally important and noteworthy. Please let me know in the comments below what I missed, where I made a mistake or misrepresented a method, or which aspect of optimization for Deep Learning you find particularly exciting or underexplored.&lt;/p&gt;

&lt;h2 id="hackernews"&gt; Hacker News&lt;/h2&gt;

&lt;p&gt;You can find the discussion of this post on HN &lt;a href="https://news.ycombinator.com/item?id=15839564"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="references"&gt;References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Robbins, H., &amp;amp; Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, 400-407. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Zhang, J., Mitliagkas, I., &amp;amp; Ré, C. (2017). YellowFin and the Art of Momentum Tuning. In arXiv preprint arXiv:1706.03471. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Denkowski, M., &amp;amp; Neubig, G. (2017). Stronger Baselines for Trustable Results in Neural Machine Translation. In Workshop on Neural Machine Translation (WNMT). Retrieved from &lt;a href="https://arxiv.org/abs/1706.09733"&gt;https://arxiv.org/abs/1706.09733&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Smith, S. L., Kindermans, P.-J., &amp;amp; Le, Q. V. (2017). Don’t Decay the Learning Rate, Increase the Batch Size. In arXiv preprint arXiv:1711.00489. Retrieved from &lt;a href="http://arxiv.org/abs/1711.00489"&gt;http://arxiv.org/abs/1711.00489&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Loshchilov, I., &amp;amp; Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts. In Proceedings of ICLR 2017. &lt;a href="https://doi.org/10.1002/fut"&gt;https://doi.org/10.1002/fut&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Bello, I., Zoph, B., Vasudevan, V., &amp;amp; Le, Q. V. (2017). Neural Optimizer Search with Reinforcement Learning. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Kawaguchi, K. (2016). Deep Learning without Poor Local Minima. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1605.07110"&gt;http://arxiv.org/abs/1605.07110&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp;amp; Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., &amp;amp; Tang, P. T. P. (2017). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In Proceedings of ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1609.04836"&gt;http://arxiv.org/abs/1609.04836&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Dinh, L., Pascanu, R., Bengio, S., &amp;amp; Bengio, Y. (2017). Sharp Minima Can Generalize For Deep Nets. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Dozat, T., &amp;amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01734"&gt;http://arxiv.org/abs/1611.01734&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., &amp;amp; Recht, B. (2017). The Marginal Value of Adaptive Gradient Methods in Machine Learning. arXiv Preprint arXiv:1705.08292. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08292"&gt;http://arxiv.org/abs/1705.08292&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Laine, S., &amp;amp; Aila, T. (2017). Temporal Ensembling for Semi-Supervised Learning. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Huang, G., Liu, Z., Weinberger, K. Q., &amp;amp; van der Maaten, L. (2017). Densely Connected Convolutional Networks. In Proceedings of CVPR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Loshchilov, I., &amp;amp; Hutter, F. (2017). Fixing Weight Decay Regularization in Adam. arXiv Preprint arXi1711.05101. Retrieved from &lt;a href="http://arxiv.org/abs/1711.05101"&gt;http://arxiv.org/abs/1711.05101&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Melis, G., Dyer, C., &amp;amp; Blunsom, P. (2017). On the State of the Art of Evaluation in Neural Language Models. In arXiv preprint arXiv:1707.05589. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Merity, S., Shirish Keskar, N., &amp;amp; Socher, R. (2017). Regularizing and Optimizing LSTM Language Models. arXiv Preprint arXiv:1708.02182. Retrieved from &lt;a href="https://arxiv.org/pdf/1708.02182.pdf"&gt;https://arxiv.org/pdf/1708.02182.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Smith, Leslie N. "Cyclical learning rates for training neural networks." In Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on, pp. 464-472. IEEE, 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., &amp;amp; de Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04474"&gt;http://arxiv.org/abs/1606.04474&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Zoph, B., &amp;amp; Le, Q. V. (2017). Neural Architecture Search with Reinforcement Learning. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv Preprint arXiv:1609.04747. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Word embeddings in 2017: Trends and future directions</title><description>This post gives an overview of the deficiencies of pre-trained word embeddings in 2017 and how recent approaches have tried to resolve them.</description><link>http://ruder.io/word-embeddings-2017/</link><guid isPermaLink="false">f4b25277-c418-46b1-97cd-4773a7365d23</guid><category>word embeddings</category><category>natural language processing</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sat, 21 Oct 2017 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/10/semantic_change.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/10/semantic_change.png" alt="Word embeddings in 2017: Trends and future directions"&gt;&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#subwordlevelembeddings"&gt;Subword-level embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#oovhandling"&gt;OOV handling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#multisenseembeddings"&gt;Multi-sense embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#beyondwordsaspoints"&gt;Beyond words as points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#phrasesandmultiwordexpressions"&gt;Phrases and multi-word expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#bias"&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#temporaldimension"&gt;Temporal dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#lackoftheoreticalunderstanding"&gt;Lack of theoretical understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#taskanddomainspecificembeddings"&gt;Task and domain-specific embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#transfer learning"&gt;Transfer learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#embeddingsformultiplelanguages"&gt;Embeddings for multiple languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#embeddingsbasedonothercontexts"&gt;Embeddings based on other contexts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The word2vec method based on skip-gram with negative sampling (Mikolov et al., 2013) [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;] was published in 2013 and had a large impact on the field, mainly through its accompanying software package, which enabled efficient training of dense word representations and a straightforward integration into downstream models. In some respects, we have come far since then: Word embeddings have established themselves as an integral part of Natural Language Processing (NLP) models. In other aspects, we might as well be in 2013 as we have not found ways to pre-train word embeddings that have managed to supersede the original word2vec.&lt;/p&gt;

&lt;p&gt;This post will focus on the deficiencies of word embeddings and how recent approaches have tried to resolve them. If not otherwise stated, this post discusses &lt;em&gt;pre-trained&lt;/em&gt; word embeddings, i.e. word representations that have been learned on a large corpus using word2vec and its variants. Pre-trained word embeddings are most effective if not millions of training examples are available (and thus transferring knowledge from a large unlabelled corpus is useful), which is true for most tasks in NLP. For an introduction to word embeddings, refer to &lt;a href="http://ruder.io/word-embeddings-1/"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="subwordlevelembeddings"&gt;Subword-level embeddings&lt;/h2&gt;

&lt;p&gt;Word embeddings have been augmented with subword-level information for many applications such as named entity recognition (Lample et al., 2016) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], part-of-speech tagging (Plank et al., 2016) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;], dependency parsing (Ballesteros et al., 2015; Yu &amp;amp; Vu, 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;], and language modelling (Kim et al., 2016) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;]. Most of these models employ a CNN or a BiLSTM that takes as input the characters of a word and outputs a &lt;em&gt;character-based&lt;/em&gt; word representation.&lt;/p&gt;

&lt;p&gt;For incorporating character information into pre-trained embeddings, however, character n-grams features have been shown to be more powerful than composition functions over individual characters (Wieting et al., 2016; Bojanowski et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;]. Character n-grams -- by far not a novel feature for text categorization (Cavnar et al., 1994) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] -- are particularly efficient and also form the basis of Facebook's fastText classifier (Joulin et al., 2016) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;]. Embeddings learned using fastText are &lt;a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"&gt;available in 294 languages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Subword units based on byte-pair encoding have been found to be particularly useful for machine translation (Sennrich et al., 2016) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;] where they have replaced words as the standard input units. They are also useful for tasks with many unknown words such as entity typing (Heinzerling &amp;amp; Strube, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;], but have not been shown to be helpful yet for standard NLP tasks, where this is not a major concern. While they can be learned easily, it is difficult to see their advantage over character-based representations for most tasks (Vania &amp;amp; Lopez, 2017) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Another choice for using pre-trained embeddings that integrate character information is to leverage a state-of-the-art language model (Jozefowicz et al., 2016) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] trained on a large in-domain corpus, e.g. the 1 Billion Word Benchmark (a pre-trained Tensorflow model can be found &lt;a href="https://github.com/tensorflow/models/tree/master/research/lm_1b"&gt;here&lt;/a&gt;). While language modelling has been found to be useful for different tasks as auxiliary objective (Rei, 2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;], pre-trained language model embeddings have also been used to augment word embeddings (Peters et al., 2017) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;]. As we start to better understand how to pre-train and initialize our models, pre-trained language model embeddings are poised to become more effective. They might even supersede word2vec as the go-to choice for initializing word embeddings by virtue of having become more expressive and easier to train due to better frameworks and more computational resources over the last years.&lt;/p&gt;

&lt;h2 id="oovhandling"&gt;OOV handling&lt;/h2&gt;

&lt;p&gt;One of the main problems of using pre-trained word embeddings is that they are unable to deal with out-of-vocabulary (OOV) words, i.e. words that have not been seen during training. Typically, such words are set to the UNK token and are assigned the same vector, which is an ineffective choice if the number of OOV words is large. Subword-level embeddings as discussed in the last section are one way to mitigate this issue. Another way, which is effective for reading comprehension (Dhingra et al., 2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] is to assign OOV words their pre-trained word embedding, if one is available. &lt;/p&gt;

&lt;p&gt;Recently, different approaches have been proposed for generating embeddings for OOV words on-the-fly. Herbelot and Baroni (2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] initialize the embedding of OOV words as the sum of their context words and then rapidly refine only the OOV embedding with a high learning rate. Their approach is successful for a dataset that explicitly requires to model nonce words, but it is unclear if it can be scaled up to work reliably for more typical NLP tasks. Another interesting approach for generating OOV word embeddings is to train a character-based model to explicitly re-create pre-trained embeddings (Pinter et al., 2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;]. This is particularly useful in low-resource scenarios, where a large corpus is inaccessible and only pre-trained embeddings are available.&lt;/p&gt;

&lt;h2 id="evaluation"&gt; Evaluation&lt;/h2&gt;

&lt;p&gt;Evaluation of pre-trained embeddings has been a contentious issue since their inception as the commonly used evaluation via word similarity or analogy datasets has been shown to only correlate weakly with downstream performance (Tsvetkov et al., 2015) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;]. The &lt;a href="https://sites.google.com/site/repevalacl16/"&gt;RepEval Workshop at ACL 2016&lt;/a&gt; exclusively focused on better ways to evaluate pre-trained embeddings. As it stands, the consensus seems to be that -- while pre-trained embeddings can be evaluated on intrinsic tasks such as word similarity for comparison against previous approaches -- the best way to evaluate them is extrinsic evaluation on downstream tasks.&lt;/p&gt;

&lt;h2 id="multisenseembeddings"&gt;Multi-sense embeddings&lt;/h2&gt;

&lt;p&gt;A commonly cited criticism of word embeddings is that they are unable to capture polysemy. &lt;a href="http://wwwusers.di.uniroma1.it/~collados/Slides_ACL16Tutorial_SemanticRepresentation.pdf"&gt;A tutorial at ACL 2016&lt;/a&gt; outlined the work in recent years that focused on learning separate embeddings for multiple senses of a word (Neelakantan et al., 2014; Iacobacci et al., 2015; Pilehvar &amp;amp; Collier, 2016) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. However, most existing approaches for learning multi-sense embeddings solely evaluate on word similarity. Pilehvar et al. (2017) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] are one of the first to show results on topic categorization as a downstream task; while multi-sense embeddings outperform randomly initialized word embeddings in their experiments, they are outperformed by pre-trained word embeddings.&lt;/p&gt;

&lt;p&gt;Given the stellar results Neural Machine Translation systems using word embeddings have achieved in recent years (Johnson et al., 2016) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;], it seems that the current generation of models is expressive enough to contextualize and disambiguate words in context without having to rely on a dedicated disambiguation pipeline or multi-sense embeddings. However, we still need better ways to understand whether our models are actually able to sufficiently disambiguate words and how to improve this disambiguation behaviour if necessary.&lt;/p&gt;

&lt;h2 id="beyondwordsaspoints"&gt;Beyond words as points&lt;/h2&gt;

&lt;p&gt;While we might not need separate embeddings for every sense of each word for good downstream performance, reducing each word to a point in a vector space is unarguably overly simplistic and causes us to miss out on nuances that might be useful for downstream tasks. An interesting direction is thus to employ other representations that are better able to capture these facets. Vilnis &amp;amp; McCallum (2015) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] propose to model each word as a probability distribution rather than a point vector, which allows us to represent probability mass and uncertainty across certain dimensions. Athiwaratkun &amp;amp; Wilson (2017) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] extend this approach to a multimodal distribution that allows to deal with polysemy, entailment, uncertainty, and enhances interpretability.&lt;/p&gt;

&lt;p&gt;Rather than altering the representation, the embedding space can also be changed to better represent certain features. Nickel and Kiela (2017) [&lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;], for instance, embed words in a hyperbolic space, to learn hierarchical representations. Finding other ways to represent words that incorporate linguistic assumptions or better deal with the characteristics of downstream tasks is a compelling research direction.&lt;/p&gt;

&lt;h2 id="phrasesandmultiwordexpressions"&gt;Phrases and multi-word expressions&lt;/h2&gt;

&lt;p&gt;In addition to not being able to capture multiple senses of words, word embeddings also fail to capture the meanings of phrases and multi-word expressions, which can be a function of the meaning of their constituent words, or have an entirely new meaning. Phrase embeddings have been proposed already in the original word2vec paper (Mikolov et al., 2013) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;] and there has been consistent work on learning better compositional and non-compositional phrase embeddings (Yu &amp;amp; Dredze, 2015; Hashimoto &amp;amp; Tsuruoka, 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;]. However, similar to multi-sense embeddings, explicitly modelling phrases has so far not shown significant improvements on downstream tasks that would justify the additional complexity. Analogously, a better understanding of how phrases are modelled in neural networks would pave the way to methods that augment the capabilities of our models to capture compositionality and non-compositionality of expressions.&lt;/p&gt;

&lt;h2 id="bias"&gt;Bias&lt;/h2&gt;

&lt;p&gt;Bias in our models is becoming a larger issue and we are only starting to understand its implications for training and evaluating our models. Even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent (Bolukbasi et al., 2016) [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;]. Understanding what other biases word embeddings capture and finding better ways to remove theses biases will be key to developing fair algorithms for natural language processing.&lt;/p&gt;

&lt;h2 id="temporaldimension"&gt;Temporal dimension&lt;/h2&gt;

&lt;p&gt;Words are a mirror of the zeitgeist and their meanings are subject to continuous change; current representations of words might differ substantially from the way these words where used in the past and will be used in the future. An interesting direction is thus to take into account the temporal dimension and the diachronic nature of words. This can allows us to reveal laws of semantic change (Hamilton et al., 2016; Bamler &amp;amp; Mandt, 2017; Dubossarsky et al., 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;], to model temporal word analogy or relatedness (Szymanski, 2017; Rosin et al., 2017) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;], or to capture the dynamics of semantic relations (Kutuzov et al., 2017) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="lackoftheoreticalunderstanding"&gt; Lack of theoretical understanding&lt;/h2&gt;

&lt;p&gt;Besides the insight that word2vec with skip-gram negative sampling implicitly factorizes a PMI matrix (Levy &amp;amp; Goldberg, 2014) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;], there has been comparatively little work on gaining a better theoretical understanding of the word embedding space and its properties, e.g. that summation captures analogy relations. Arora et al. (2016) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;] propose a new generative model for word embeddings, which treats corpus generation as a random walk of a discourse vector and establishes some theoretical motivations regarding the analogy behaviour. Gittens et al. (2017) [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;] provide a more thorough theoretical justification of additive compositionality and show that skip-gram word vectors are optimal in an information-theoretic sense. Mimno &amp;amp; Thompson (2017) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] furthermore reveal an interesting relation between word embeddings and the embeddings of context words, i.e. that they are not evenly dispersed across the vector space, but occupy a narrow cone that is diametrically opposite to the context word embeddings. Despite these additional insights, our understanding regarding the location and properties of word embeddings is still lacking and more theoretical work is necessary. &lt;/p&gt;

&lt;h2 id="taskanddomainspecificembeddings"&gt; Task and domain-specific embeddings&lt;/h2&gt;

&lt;p&gt;One of the major downsides of using pre-trained embeddings is that the news data used for training them is often very different from the data on which we would like to use them. In most cases, however, we do not have access to millions of unlabelled documents in our target domain that would allow for pre-training good embeddings from scratch. We would thus like to be able to adapt embeddings pre-trained on large news corpora, so that they capture the characteristics of our target domain, but still retain all relevant existing knowledge. Lu &amp;amp; Zheng (2017) [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;] proposed a regularized skip-gram model for learning such cross-domain embeddings. In the future, we will need even better ways to adapt pre-trained embeddings to new domains or to incorporate the knowledge from multiple relevant domains.&lt;/p&gt;

&lt;p&gt;Rather than adapting to a new domain, we can also use existing knowledge encoded in semantic lexicons to augment pre-trained embeddings with information that is relevant for our task. An effective way to inject such relations into the embedding space is retro-fitting (Faruqui et al., 2015) [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;], which has been expanded to other resources such as ConceptNet (Speer et al., 2017) [&lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;] and extended with an intelligent selection of positive and negative examples (Mrkšić et al., 2017) [&lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. Injecting additional prior knowledge into word embeddings such as monotonicity (You et al., 2017) [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;], word similarity (Niebler et al., 2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;], task-related grading or intensity, or logical relations is an important research direction that will allow to make our models more robust.&lt;/p&gt;

&lt;p&gt;Word embeddings are useful for a wide variety of applications beyond NLP such as information retrieval, recommendation, and link prediction in knowledge bases, which all have their own task-specific approaches. Wu et al. (2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;] propose a general-purpose model that is compatible with many of these applications and can serve as a strong baseline.&lt;/p&gt;

&lt;h2 id="transferlearning"&gt; Transfer learning&lt;/h2&gt;

&lt;p&gt;Rather than adapting word embeddings to any particular task, recent work has sought to create &lt;em&gt;contextualized&lt;/em&gt; word vectors by augmenting word embeddings with embeddings based on the hidden states of models pre-trained for certain tasks, such as machine translation (McCann et al., 2017) [&lt;sup id="fnref:57"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:57" rel="footnote"&gt;57&lt;/a&gt;&lt;/sup&gt;] or language modelling (Peters et al., 2018) [&lt;sup id="fnref:58"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:58" rel="footnote"&gt;58&lt;/a&gt;&lt;/sup&gt;]. Together with fine-tuning pre-trained models (Howard and Ruder, 2018) [&lt;sup id="fnref:59"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:59" rel="footnote"&gt;59&lt;/a&gt;&lt;/sup&gt;], this is one of the most promising research directions.&lt;/p&gt;

&lt;h2 id="embeddingsformultiplelanguages"&gt;Embeddings for multiple languages&lt;/h2&gt;

&lt;p&gt;As NLP models are being increasingly employed and evaluated on multiple languages, creating multilingual word embeddings is becoming a more important issue and has received increased interest over recent years. A promising direction is to develop methods that learn cross-lingual representations with as few parallel data as possible, so that they can be easily applied to learn representations even for low-resource languages. For a recent survey in this area, refer to Ruder et al. (2017) [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="embeddingsbasedonothercontexts"&gt;Embeddings based on other contexts&lt;/h2&gt;

&lt;p&gt;Word embeddings are typically learned only based on the window of surrounding context words. Levy &amp;amp; Goldberg (2014) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] have shown that dependency structures can be used as context to capture more syntactic word relations; Köhn (2015) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;] finds that such dependency-based embeddings perform best for a particular multilingual evaluation method that clusters embeddings along different syntactic features. &lt;/p&gt;

&lt;p&gt;Melamud et al. (2016) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;] observe that different context types work well for different downstream tasks and that simple concatenation of word embeddings learned with different context types can yield further performance gains. Given the recent success of incorporating graph structures into neural models for different tasks as -- for instance -- exhibited by graph-convolutional neural networks (Bastings et al., 2017; Marcheggiani &amp;amp; Titov, 2017) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;], we can conjecture that incorporating such structures for learning embeddings for downstream tasks may also be beneficial.&lt;/p&gt;

&lt;p&gt;Besides selecting context words differently, additional context may also be used in other ways: Tissier et al. (2017) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] incorporate co-occurrence information from dictionary definitions into the negative sampling process to move related works closer together and prevent them from being used as negative samples. We can think of topical or relatedness information derived from other contexts such as article headlines or Wikipedia intro paragraphs that could similarly be used to make the representations more applicable to a particular downstream task.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It is nice to see that as a community we are progressing from applying word embeddings to every possible problem to gaining a more principled, nuanced, and practical understanding of them. This post was meant to highlight some of the current trends and future directions for learning word embeddings that I found most compelling. I've undoubtedly failed to mention many other areas that are equally important and noteworthy. Please let me know in the comments below what I missed, where I made a mistake or misrepresented a method, or just which aspect of word embeddings you find particularly exciting or unexplored.&lt;/p&gt;

&lt;h1 id="hackernews"&gt;Hacker News&lt;/h1&gt;

&lt;p&gt;Refer to the &lt;a href="https://news.ycombinator.com/item?id=15521957"&gt;discussion on Hacker News&lt;/a&gt; for some more insights on word embeddings.&lt;/p&gt;

&lt;h2 id="otherblogpostsonwordembeddings"&gt;Other blog posts on word embeddings&lt;/h2&gt;

&lt;p&gt;If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;On word embeddings - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;On word embeddings - Part 2: Approximating the softmax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/secret-word2vec/index.html"&gt;On word embeddings - Part 3: The secret ingredients of word2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/cross-lingual-embeddings/index.html"&gt;Unofficial Part 4: A survey of cross-lingual embedding models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="references"&gt;References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Cavnar, W. B., Trenkle, J. M., &amp;amp; Mi, A. A. (1994). N-Gram-Based Text Categorization. Ann Arbor MI 48113.2, 161–175. &lt;a href="https://doi.org/10.1.1.53.9367"&gt;https://doi.org/10.1.1.53.9367&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Wieting, J., Bansal, M., Gimpel, K., &amp;amp; Livescu, K. (2016). Charagram: Embedding Words and Sentences via Character n-grams. Retrieved from &lt;a href="http://arxiv.org/abs/1607.02789"&gt;http://arxiv.org/abs/1607.02789&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Bojanowski, P., Grave, E., Joulin, A., &amp;amp; Mikolov, T. (2017). Enriching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1607.04606"&gt;http://arxiv.org/abs/1607.04606&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Joulin, A., Grave, E., Bojanowski, P., &amp;amp; Mikolov, T. (2016). Bag of Tricks for Efficient Text Classification. arXiv Preprint arXiv:1607.01759. Retrieved from &lt;a href="http://arxiv.org/abs/1607.01759"&gt;http://arxiv.org/abs/1607.01759&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 1756–1765). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv Preprint arXiv:1602.02410. Retrieved from &lt;a href="http://arxiv.org/abs/1602.02410"&gt;http://arxiv.org/abs/1602.02410&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp;amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In NAACL-HLT 2016. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Yu, X., &amp;amp; Vu, N. T. (2017). Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 672–678). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from &lt;a href="http://arxiv.org/abs/1508.06615"&gt;http://arxiv.org/abs/1508.06615&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1508.07909"&gt;http://arxiv.org/abs/1508.07909&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Heinzerling, B., &amp;amp; Strube, M. (2017). BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages. Retrieved from &lt;a href="http://arxiv.org/abs/1710.02187"&gt;http://arxiv.org/abs/1710.02187&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Dhingra, B., Liu, H., Salakhutdinov, R., &amp;amp; Cohen, W. W. (2017). A Comparative Study of Word Embeddings for Reading Comprehension. arXiv preprint arXiv:1703.00993. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Herbelot, A., &amp;amp; Baroni, M. (2017). High-risk learning: acquiring new word vectors from tiny data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Pinter, Y., Guthrie, R., &amp;amp; Eisenstein, J. (2017). Mimicking Word Embeddings using Subword RNNs. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.06961"&gt;http://arxiv.org/abs/1707.06961&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Ballesteros, M., Dyer, C., &amp;amp; Smith, N. A. (2015). Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs. In Proceedings of EMNLP 2015. &lt;a href="https://doi.org/10.18653/v1/D15-1041"&gt;https://doi.org/10.18653/v1/D15-1041&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Neelakantan, A., Shankar, J., Passos, A., &amp;amp; Mccallum, A. (2014). Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space. In Proceedings fo (pp. 1059–1069). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Iacobacci, I., Pilehvar, M. T., &amp;amp; Navigli, R. (2015). SensEmbed: Learning Sense Embeddings for Word and Relational Similarity. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (pp. 95–105). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Pilehvar, M. T., &amp;amp; Collier, N. (2016). De-Conflated Semantic Representations. In Proceedings of EMNLP. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Tsvetkov, Y., Faruqui, M., Ling, W., Lample, G., &amp;amp; Dyer, C. (2015). Evaluation of Word Vector Representations by Subspace Alignment. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015, 2049–2054. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Pilehvar, M. T., Camacho-Collados, J., Navigli, R., &amp;amp; Collier, N. (2017). Towards a Seamless Integration of Word Senses into Downstream NLP Applications. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1857–1869). &lt;a href="https://doi.org/10.18653/v1/P17-1170"&gt;https://doi.org/10.18653/v1/P17-1170&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Vilnis, L., &amp;amp; McCallum, A. (2015). Word Representations via Gaussian Embedding. ICLR. Retrieved from &lt;a href="http://arxiv.org/abs/1412.6623"&gt;http://arxiv.org/abs/1412.6623&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Athiwaratkun, B., &amp;amp; Wilson, A. G. (2017). Multimodal Word Distributions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., &amp;amp; Kalai, A. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In 30th Conference on Neural Information Processing Systems (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1607.06520"&gt;http://arxiv.org/abs/1607.06520&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Hamilton, W. L., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (pp. 1489–1501). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Bamler, R., &amp;amp; Mandt, S. (2017). Dynamic Word Embeddings via Skip-Gram Filtering. In Proceedings of ICML 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08359"&gt;http://arxiv.org/abs/1702.08359&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Dubossarsky, H., Grossman, E., &amp;amp; Weinshall, D. (2017). Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models. In Conference on Empirical Methods in Natural Language Processing (pp. 1147–1156). Retrieved from &lt;a href="http://aclweb.org/anthology/D17-1119"&gt;http://aclweb.org/anthology/D17-1119&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Szymanski, T. (2017). Temporal Word Analogies : Identifying Lexical Replacement with Diachronic Word Embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 448–453). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Rosin, G., Radinsky, K., &amp;amp; Adar, E. (2017). Learning Word Relatedness over Time. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="https://arxiv.org/pdf/1707.08081.pdf"&gt;https://arxiv.org/pdf/1707.08081.pdf&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Kutuzov, A., Velldal, E., &amp;amp; Øvrelid, L. (2017). Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.08660"&gt;http://arxiv.org/abs/1707.08660&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from &lt;a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization"&gt;http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Arora, S., Li, Y., Liang, Y., Ma, T., &amp;amp; Risteski, A. (2016). A Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4, 385–399. Retrieved from &lt;a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204"&gt;https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Gittens, A., Achlioptas, D., &amp;amp; Mahoney, M. W. (2017). Skip-Gram – Zipf + Uniform = Vector Additivity. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 69–76). &lt;a href="https://doi.org/10.18653/v1/P17-1007"&gt;https://doi.org/10.18653/v1/P17-1007&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Mimno, D., &amp;amp; Thompson, L. (2017). The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2863–2868). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Yu, M., &amp;amp; Dredze, M. (2015). Learning Composition Models for Phrase Embeddings. Transactions of the ACL, 3, 227–242. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Hashimoto, K., &amp;amp; Tsuruoka, Y. (2016). Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings. ACL, 205–215. Retrieved from &lt;a href="http://arxiv.org/abs/1603.06067"&gt;http://arxiv.org/abs/1603.06067&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Lu, W., &amp;amp; Zheng, V. W. (2017). A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2888–2894). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Faruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., &amp;amp; Smith, N. A. (2015). Retrofitting Word Vectors to Semantic Lexicons. In NAACL 2015. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Mrkšić, N., Vulić, I., Séaghdha, D. Ó., Leviant, I., Reichart, R., Gašić, M., … Young, S. (2017). Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints. TACL. Retrieved from &lt;a href="http://arxiv.org/abs/1706.00374"&gt;http://arxiv.org/abs/1706.00374&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Ruder, S., Vulić, I., &amp;amp; Søgaard, A. (2017). A Survey of Cross-lingual Word Embedding Models Sebastian. arXiv preprint arXiv:1706.04902. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04902"&gt;http://arxiv.org/abs/1706.04902&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Dependency-Based Word Embeddings. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), 302–308. &lt;a href="https://doi.org/10.3115/v1/P14-2050"&gt;https://doi.org/10.3115/v1/P14-2050&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Köhn, A. (2015). What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015, (2014), 2067–2073. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Melamud, O., McClosky, D., Patwardhan, S., &amp;amp; Bansal, M. (2016). The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of NAACL-HLT 2016 (pp. 1030–1040). Retrieved from &lt;a href="http://arxiv.org/abs/1601.00893"&gt;http://arxiv.org/abs/1601.00893&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., &amp;amp; Sima’an, K. (2017). Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Marcheggiani, D., &amp;amp; Titov, I. (2017). Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Vania, C., &amp;amp; Lopez, A. (2017). From Characters to Words to in Between: Do We Capture Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 2016–2027). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;You, S., Ding, D., Canini, K., Pfeifer, J., &amp;amp; Gupta, M. (2017). Deep Lattice Networks and Partial Monotonic Functions. In 31st Conference on Neural Information Processing Systems (NIPS 2017). Retrieved from &lt;a href="http://arxiv.org/abs/1709.06680"&gt;http://arxiv.org/abs/1709.06680&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Nickel, M., &amp;amp; Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. arXiv Preprint arXiv:1705.08039. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08039"&gt;http://arxiv.org/abs/1705.08039&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Niebler, T., Becker, M., Pölitz, C., &amp;amp; Hotho, A. (2017). Learning Semantic Relatedness From Human Feedback Using Metric Learning. In Proceedings of ISWC 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1705.07425"&gt;http://arxiv.org/abs/1705.07425&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Wu, L., Fisch, A., Chopra, S., Adams, K., Bordes, A., &amp;amp; Weston, J. (2017). StarSpace: Embed All The Things! arXiv preprint arXiv:1709.03856. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Speer, R., Chin, J., &amp;amp; Havasi, C. (2017). ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In AAAI 31 (pp. 4444–4451). Retrieved from &lt;a href="http://arxiv.org/abs/1612.03975"&gt;http://arxiv.org/abs/1612.03975&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Tissier, J., Gravier, C., &amp;amp; Habrard, A. (2017). Dict2Vec : Learning Word Embeddings using Lexical Dictionaries. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://aclweb.org/anthology/D17-1024"&gt;http://aclweb.org/anthology/D17-1024&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:57"&gt;&lt;p&gt;Mccann, B., Bradbury, J., Xiong, C., &amp;amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:57" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:58"&gt;&lt;p&gt;Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp;amp; Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings of NAACL-HLT 2018. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:58" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:59"&gt;&lt;p&gt;Howard, J., &amp;amp; Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of ACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1801.06146"&gt;http://arxiv.org/abs/1801.06146&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:59" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Cover image credit: Hamilton et al. (2016)&lt;/p&gt;</content:encoded></item><item><title>Multi-Task Learning Objectives for Natural Language Processing</title><description>An overview of auxiliary tasks and objectives that have been used for multi-task learning for natural language processing.</description><link>http://ruder.io/multi-task-learning-nlp/</link><guid isPermaLink="false">2e84d768-9785-4ce7-bf05-7f4773fc34b2</guid><category>natural language processing</category><category>nlp</category><category>multi-task learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 24 Sep 2017 13:42:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/09/soft_parameter_sharing.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/09/soft_parameter_sharing.png" alt="Multi-Task Learning Objectives for Natural Language Processing"&gt;&lt;p&gt;In a &lt;a href="http://ruder.io/multi-task/index.html"&gt;previous blog post&lt;/a&gt;, I discussed how multi-task learning (MTL) can be used to improve the performance of a model by leveraging a related task. Multi-task learning consists of two main components: a) The architecture used for learning and b) the auxiliary task(s) that are trained jointly. Both facets still have a lot of room for improvement. In addition, multi-task learning has the potential to be a key technique on the path to more robust models that learn from limited data: Training a model to acquire proficiency in performing a wide range of NLP tasks would allow us to induce representations, which should be useful for transferring knowledge to many other tasks, as outlined in &lt;a href="http://ruder.io/transfer-learning/index.html"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the way to this goal, we first need to learn more about the relationships between our tasks, what we can learn from each, and how to combine them most effectively. Most of the existing theory in MTL has focused on homogeneous tasks, i.e. tasks that are variations of the same classification or regression problem, such as classifying individual MNIST digits. These guarantees, however, do not hold for the heterogeneous tasks to which MTL is most often applied in Natural Language Processing (NLP) and Computer Vision.&lt;/p&gt;

&lt;p&gt;There have been some recent studies looking into when multi-task learning between different NLP tasks works but we still do not understand very well which tasks are useful. To this end, as inspiration, I will give an overview in the following of different approaches for multi-task learning for NLP. I will focus on the second component of multi-task learning; instead of discussing &lt;em&gt;how&lt;/em&gt; a model is trained, as most architectures only differ in which layers they share, I will concentrate on the auxiliary tasks and objectives that are used for learning.&lt;/p&gt;

&lt;p&gt;This post has two main parts: In the first part, I will talk about artificial tasks that can be used as auxiliary objectives for MTL. In the second part, I will focus on common NLP tasks and discuss which other NLP tasks have benefited them.&lt;/p&gt;

&lt;h1 id="artificialauxiliaryobjectives"&gt;Artificial auxiliary objectives&lt;/h1&gt;

&lt;p&gt;Multi-task learning is all about coming up with ways to add a suitable bias to your model. Incorporating artificial auxiliary tasks that cleverly complement your target task is arguably one of the most ingenious and fun ways to do MTL. It is a feature-engineering of sorts: instead of engineering the features, you are engineering the auxiliary task you optimize. Similarly to feature engineering, domain expertise is therefore required as we will see in the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language modelling&lt;/strong&gt; Language modelling has been shown to be beneficial for many NLP tasks and can be incorporated in various ways. Word embeddings pre-trained by word2vec have been shown to beneficial -- as is known, word2vec approximates the language modelling objective; languages models have been used to pre-train MT and sequence-to-sequence models [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;]; contextual language model embeddings have also been found useful for many tasks [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;]. In this context, we can also treat language modelling as an auxiliary task that is learned together with the main task. Rei (2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] shows that this improves performance on several sequence labelling tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conditioning the initial state&lt;/strong&gt; &amp;nbsp; The initial state of a recurrent neural network is typically initialized to a \(0\) vector. According to a &lt;a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf"&gt;lecture by Hinton in 2013&lt;/a&gt;, it is beneficial to learn the initial state just like any other sets of weights. While a learned state will be more helpful than a \(0\) vector it will be independent of the sequence and thus unable to adapt. Weng et al. (2017) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] propose to add a suitable bias to the initial encoder and decoder states for NMT by training it to predict the words in the sentence. In this sense, this objective can essentially be seen as a &lt;em&gt;language modelling objective for the initial state&lt;/em&gt; and might thus be helpful for other tasks. Similarly, we can think of other task-specific biases that could be encoded in the initial state to aid learning: A sentiment model might benefit from knowing about the general audience response to a movie or whether a user is more likely to be sarcastic while a parser might be able to leverage prior knowledge of the domain's tree depth or complexity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adversarial loss&lt;/strong&gt; &amp;nbsp; An auxiliary adversarial loss was first found to be useful for domain adaptation [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;], where it is used to learn domain-invariant representations by rendering the model unable to distinguish between different domains. This is typically done by adding a gradient reversal layer that reverses the sign of the gradient during back-propagation, which in turn leads to a maximization rather than a minimization of the adversarial loss. It is not to be confused with adversarial examples [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;], which significantly increase the model's loss typically via small perturbations to its input; adversarial training [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], which trains a model to correctly classify such examples; or Generative Adversarial Networks, which are trained to generate some output representation. An adversarial loss can be added to many tasks in order to learn task-independent representations [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;]. It can also be used to ignore certain features of the input that have been found to be detrimental to generalization, such as data-specific properties that are unlikely to generalize. Finally, an adversarial auxiliary task might also help to combat bias and ensure more privacy by encouraging the model to learn representations, which do not contain information that would allow the reconstruction of sensitive user attributes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Predicting data statistics&lt;/strong&gt; &amp;nbsp; An auxiliary loss can also be to predict certain underlying statistics of the training data. In contrast to the adversarial loss, which tries to make the model oblivious to certain features, this auxiliary task explicitly encourages the model to predict certain data statistics. Plank et al. (2016) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;] predict the log frequency of a word as an auxiliary task for language modelling. Intuitively, this makes the representation predictive of frequency, which encourages the model to not share representations between common and rare words, which benefits the handling of rare tokens. Another facet of this auxiliary task is to predict attributes of the user, such as their gender, which has been shown to be beneficial for predicting mental health conditions [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;] or other demographic information [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;]. We can think of other statistics that might be beneficial for a model to encode, such as the frequency of POS tags, parsing structures, or entities, the preferences of users, a sentence's coverage for summarization, or even a user's website usage patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning the inverse&lt;/strong&gt; &amp;nbsp; Another auxiliary task that might be useful in many circumstances is to learn the inverse of the task together with the main task. A popular example of this framework is CycleGAN [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;], which can &lt;a href="https://github.com/junyanz/CycleGAN"&gt;generate photos from paintings&lt;/a&gt;. An inverse auxiliary loss, however, is applicable to many other tasks: MT might be the most intuitive, as every translation direction such as English-&gt;French directly provides data for the inverse direction, as Xia et al. (2016) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] demonstrate. Xia et al. (2017) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;] show that this has applications not only to MT, but also to image classification (with image generation as its inverse) and sentiment classification (paired with sentence generation). For multimodal translation, Elliott and Kádár (2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] jointly learn an inverse task by predicting image representations. It is not difficult to think of inverse complements for many other tasks: Entailment has hypothesis generation; video captioning has video generation; speech recognition has speech synthesis, etc. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Predicting what should be there&lt;/strong&gt; &amp;nbsp; For many tasks, where a model has to pick up on certain features of the training data, we can focus the model's attention on these characteristics by encouraging it explicitly to predict them. For sentiment analysis, for instance, Yu and Jiang (2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;] predict whether the sentence contains a positive or negative domain-independent sentiment word, which sensitizes the model towards the sentiment of the words in the sentence. For name error detection, Cheng et al. (2015) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;] predict if a sentence contains a name. We can envision similar auxiliary tasks that might be useful for other tasks: Predicting whether certain entities occur in a sentence might be useful for relation extraction; predicting whether a headline contains certain lurid terms might help for clickbait detection, while predicting whether an emotion word occurs in the sentence might benefit emotion detection. In summary, this auxiliary task should be helpful whenever a task includes certain highly predictive terms or features.&lt;/p&gt;

&lt;h1 id="jointtrainingofexistingnlptasks"&gt;Joint training of existing NLP tasks&lt;/h1&gt;

&lt;p&gt;In this second section, we will now look at existing NLP tasks, which have been used to improve the performance of a main task. While certain tasks such as chunking and semantic tagging have been found to be useful for many tasks [&lt;sup id="fnref:60"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:60" rel="footnote"&gt;60&lt;/a&gt;&lt;/sup&gt;], the choice whether to use a particular auxiliary task largely depends on characteristics of the main task. In the following, I will thus highlight different strategies and rationals that were used to select auxiliary tasks for many common tasks in NLP:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Speech recognition&lt;/strong&gt; &amp;nbsp; Recent multi-task learning approaches for automatic speech recognition (ASR) typically use additional supervision signals that are available in the speech recognition pipeline as auxiliary tasks to train an ASR model end-to-end. Phonetic recognition and frame-level state classification can be used as auxiliary tasks to induce helpful intermediate representations. Toshniwal et al. (2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] find that positioning the auxiliary loss at an intermediate layer improves performance. Similarly, Arık et al. (2017) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;] predict the phoneme duration and frequency profile as auxiliary tasks for speech synthesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt; &amp;nbsp; The main benefit MTL has brought to machine translation (MT) is by jointly training translation models from and to different languages: Dong et al. (2015) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] jointly train the decoders; Zoph and Knight (2016) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] jointly train the encoders, while Johnson et al. (2016) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] jointly train both encoders and decoders; Malaviya et al. (2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] train one model to translate from 1017 languages into English.&lt;/p&gt;

&lt;p&gt;Other tasks have also shown to be useful for MT: Luong et al. (2015) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] show gains using parsing and image captioning as auxiliary tasks; Niehues and Cho (2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] combine NMT with POS tagging and NER; Wu et al. (2017) [&lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;] jointly model the target word sequence and its dependency tree structure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multilingual tasks&lt;/strong&gt; &amp;nbsp; Similarly to MT, it can often be beneficial to jointly train models for different languages: Gains have been shown for dependency parsing [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;], named entity recognition [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;], part-of-speech tagging [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;], document classification [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;], discourse segmentation [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;], and sequence tagging [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language grounding&lt;/strong&gt; &amp;nbsp; For grounding language in images or videos, it is often useful to enable the model to learn causal relationships in the data. For video captioning, Pasunuru and Bansal (2017) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] jointly learn to predict the next frame in the video and to predict entailment, while Hermann et al. (2017) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;] also predict the next frame in a video and the words that represent the visual state for language learning in a simulated environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Semantic parsing&lt;/strong&gt; &amp;nbsp; For a task where multiple label sets or formalisms are available such as for semantic parsing, an interesting MTL strategy is to learn these formalisms together: To this end, Guo et al. (2016) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] jointly train on multi-typed treebanks; Peng et al. (2017) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] learn three semantic dependency graph formalisms simultaneously; Fan et al. (2017) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] jointly learn different Alexa-based semantic parsing formalisms; and Zhao and Huang (2017) [&lt;sup id="fnref:57"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:57" rel="footnote"&gt;57&lt;/a&gt;&lt;/sup&gt;] jointly train a syntactic and a discourse parser. For more shallow semantic parsing such as frame-semantic argument identification, Swayamdipta et al. (2017) [&lt;sup id="fnref:61"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:61" rel="footnote"&gt;61&lt;/a&gt;&lt;/sup&gt;] predict whether an n-gram is syntactically meaningful, i.e. a syntactic constituent. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation learning&lt;/strong&gt; &amp;nbsp; For learning general-purpose representations, the challenge often is in defining the objective. Most existing representation learning models have been based on a single loss function, such as predicting the next word [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;] or sentence [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;] or training on a certain task such as entailment [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] or MT [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. Rather than learning representations based on a single loss, intuitively, representations should become more general as more tasks are used to learn them. As an example of this strategy, Hashimoto et al. (2017) [&lt;sup id="fnref:59"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:59" rel="footnote"&gt;59&lt;/a&gt;&lt;/sup&gt;] jointly train a model on multiple NLP tasks, while Jernite et al. (2017) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;] propose several discourse-based artificial auxiliary tasks for sentence representation learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question answering&lt;/strong&gt; &amp;nbsp; For question answering (QA) and reading comprehension, it is beneficial to learn the different parts of a more complex end-to-end model together: Choi et al. (2017) [&lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;] jointly learn a sentence selection and answer generation model, while Wang et al. (2017) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] jointly train a ranking and reader model for open-domain QA. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Information retrieval&lt;/strong&gt; &amp;nbsp; For relation extraction, information related to different relations or roles can often be shared. To this end, Jiang (2009) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] jointly learn linear models between different relation types; Yang and Mitchell (2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;] jointly predict semantic role labels and relations; Katiyar and Cardie (2017) [&lt;sup id="fnref:58"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:58" rel="footnote"&gt;58&lt;/a&gt;&lt;/sup&gt;] jointly extract entities and relations; and Liu et al. (2015) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] jointly train domain classification and web search ranking. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chunking&lt;/strong&gt; &amp;nbsp; Chunking has been shown to benefit from being jointly trained with low-level tasks such as POS tagging [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt; &amp;nbsp; Besides the tasks mentioned above, various other tasks have been shown to benefit from MTL: Balikas and Moura (2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] jointly train coarse-grained and fine-grained sentiment analysis; Luo et al. (2017) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;] jointly predict charges and extract articles; Augenstein and Søgaard (2017) [&lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;] use several auxiliary tasks for keyphrase boundary detection; and Isonuma et al. (2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;] pair sentence extraction with document classification.&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I hope this blog post was able to provide you with some insight with regard to which strategies are employed to select auxiliary tasks and objectives for multi-task learning in NLP. As I mentioned &lt;a href="http://ruder.io/multi-task/index.html"&gt;before&lt;/a&gt;, multi-task learning can be very broadly defined. I have tried to provide as broad of an overview as possible but I still likely have omitted many relevant approaches. If you are aware of an approach that provides a valuable perspective that is not represented here, please let me know in the comments below.&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Weng, R., Huang, S., Zheng, Z., Dai, X., &amp;amp; Chen, J. (2017). Neural Machine Translation with Word Predictions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Ramachandran, P., Liu, P. J., &amp;amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 1756–1765). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17, 1–35. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf"&gt;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp;amp; Fergus, R. (2014). Intriguing properties of neural networks. In ICLR 2014. Retrieved from &lt;a href="http://arxiv.org/abs/1312.6199"&gt;http://arxiv.org/abs/1312.6199&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Miyato, T., Dai, A. M., &amp;amp; Goodfellow, I. (2016). Virtual Adversarial Training for Semi-Supervised Text Classification. Retrieved from &lt;a href="http://arxiv.org/abs/1605.07725"&gt;http://arxiv.org/abs/1605.07725&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Liu, P., Qiu, X., &amp;amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1704.05742"&gt;http://arxiv.org/abs/1704.05742&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Toshniwal, S., Tang, H., Lu, L., &amp;amp; Livescu, K. (2017). Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition. Retrieved from &lt;a href="http://arxiv.org/abs/1704.01631"&gt;http://arxiv.org/abs/1704.01631&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Phoneme duration and frequency profile &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., … Shoeybi, M. (2017). Deep Voice: Real-time Neural Text-to-Speech. In ICML 2017.

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Dong, D., Wu, H., He, W., Yu, D., &amp;amp; Wang, H. (2015). Multi-Task Learning for Multiple Language Translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (pp. 1723–1732). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Zoph, B., &amp;amp; Knight, K. (2016). Multi-Source Neural Translation. NAACL, 30–34. Retrieved from &lt;a href="http://arxiv.org/abs/1601.00710"&gt;http://arxiv.org/abs/1601.00710&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Malaviya, C., Neubig, G., &amp;amp; Littell, P. (2017). Learning Language Representations for Typology Prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.09569"&gt;http://arxiv.org/abs/1707.09569&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., &amp;amp; Kaiser, L. (2015). Multi-task Sequence to Sequence Learning. In arXiv preprint arXiv:1511.06114. Retrieved from &lt;a href="http://arxiv.org/abs/1511.06114"&gt;http://arxiv.org/abs/1511.06114&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Niehues, J., &amp;amp; Cho, E. (2017). Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning. In WMT 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1708.00993"&gt;http://arxiv.org/abs/1708.00993&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Elliott, D., &amp;amp; Kádár, Á. (2017). Imagination improves Multimodal Translation. Retrieved from &lt;a href="http://arxiv.org/abs/1705.04350"&gt;http://arxiv.org/abs/1705.04350&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Yu, J., &amp;amp; Jiang, J. (2016). Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 236–246. Retrieved from &lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1023.pdf"&gt;http://www.aclweb.org/anthology/D/D16/D16-1023.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Balikas, G., &amp;amp; Moura, S. (2017). Multitask Learning for Fine-Grained Twitter Sentiment Analysis. In International ACM SIGIR Conference on Research and Development in Information Retrieval 2017. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Duong, L., Cohn, T., Bird, S., &amp;amp; Cook, P. (2015). Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), 845–850. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Gillick, D., Brunk, C., Vinyals, O., &amp;amp; Subramanya, A. (2016). Multilingual Language Processing From Bytes. NAACL, 1296–1306. Retrieved from &lt;a href="http://arxiv.org/abs/1512.00103"&gt;http://arxiv.org/abs/1512.00103&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Fang, M., &amp;amp; Cohn, T. (2017). Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Nikolaos Pappas and Andrei Popescu-Belis (2017). Multilingual Hierarchical Attention Networks for Document Classification. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. Retrieved from &lt;a href="https://arxiv.org/pdf/1707.00896.pdf"&gt;https://arxiv.org/pdf/1707.00896.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Braud, C., Lacroix, O., &amp;amp; Søgaard, A. (2017). Cross-lingual and cross-domain discourse segmentation of entire documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Yang, Z., Salakhutdinov, R., &amp;amp; Cohen, W. (2016). Multi-Task Cross-Lingual Sequence Tagging from Scratch. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Ammar, W., Mulcaire, G., Ballesteros, M., Dyer, C., &amp;amp; Smith, N. A. (2016). One Parser, Many Languages. Transactions of the Association for Computational Linguistics, Vol. 4, Pp. 431–444, 2016, 4, 431–444. Retrieved from &lt;a href="http://arxiv.org/abs/1602.01595"&gt;http://arxiv.org/abs/1602.01595&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Jiang, J. (2009). Multi-task transfer learning for weakly-supervised relation extraction. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, (August), 1012–1020. &lt;a href="https://doi.org/10.3115/1690219.1690288"&gt;https://doi.org/10.3115/1690219.1690288&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Pasunuru, R., &amp;amp; Bansal, M. (2017). Multi-Task Video Captioning with Video and Entailment Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Guo, J., Che, W., Wang, H., &amp;amp; Liu, T. (2016). Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01161"&gt;http://arxiv.org/abs/1606.01161&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Peng, H., Thomson, S., Smith, N. A., &amp;amp; Allen, P. G. (2017). Deep Multitask Learning for Semantic Dependency Parsing. In ACL 2017. Retrieved from &lt;a href="https://arxiv.org/pdf/1704.06855.pdf"&gt;https://arxiv.org/pdf/1704.06855.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Fan, X., Monti, E., Mathias, L., &amp;amp; Dreyer, M. (2017). Transfer Learning for Neural Semantic Parsing. ACL Repl4NLP 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04326"&gt;http://arxiv.org/abs/1706.04326&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp;amp; Fidler, S. (2015). Skip-Thought Vectors, (786). Retrieved from &lt;a href="http://arxiv.org/abs/1506.06726"&gt;http://arxiv.org/abs/1506.06726&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp;amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Mccann, B., Bradbury, J., Xiong, C., &amp;amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Jernite, Y., Bowman, S. R., &amp;amp; Sontag, D. (2017). Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Retrieved from &lt;a href="http://arxiv.org/abs/1705.00557"&gt;http://arxiv.org/abs/1705.00557&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Liu, X., Gao, J., He, X., Deng, L., Duh, K., &amp;amp; Wang, Y.-Y. (2015). Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval. NAACL-2015, 912–921. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. &lt;a href="https://doi.org/10.1145/1390156.1390177"&gt;https://doi.org/10.1145/1390156.1390177&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Søgaard, A., &amp;amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Zhu, J., Park, T., Efros, A. A., Ai, B., &amp;amp; Berkeley, U. C. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Xia, Y., He, D., Qin, T., Wang, L., Yu, N., Liu, T.-Y., &amp;amp; Ma, W.-Y. (2016). Dual Learning for Machine Translation. In Advances in Neural Information Processing Systems 29 (NIPS 2016) (pp. 1–9). Retrieved from &lt;a href="http://arxiv.org/abs/1611.00179"&gt;http://arxiv.org/abs/1611.00179&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Xia, Y., Qin, T., Chen, W., Bian, J., Yu, N., &amp;amp; Liu, T. (2017). Dual Supervised Learning. In ICML. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., … Phil Blunsom. (2017). Grounded Language Learning in a Simulated 3D World. Retrieved from &lt;a href="https://arxiv.org/pdf/1706.06551.pdf"&gt;https://arxiv.org/pdf/1706.06551.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Luo, B., Feng, Y., Xu, J., Zhang, X., &amp;amp; Zhao, D. (2017). Learning to Predict Charges for Criminal Cases with Legal Basis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.09168"&gt;http://arxiv.org/abs/1707.09168&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Augenstein, I., &amp;amp; Søgaard, A. (2017). Multi-Task Learning of Keyphrase Boundary Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1704.00514"&gt;http://arxiv.org/abs/1704.00514&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Benton, A., Mitchell, M., &amp;amp; Hovy, D. (2017). Multi-Task Learning for Mental Health using Social Media Text. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Retrieved from &lt;a href="http://m-mitchell.com/publications/multitask-clinical.pdf"&gt;http://m-mitchell.com/publications/multitask-clinical.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Cheng, H., Fang, H., &amp;amp; Ostendorf, M. (2015). Open-Domain Name Error Detection using a Multi-Task RNN. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 737–746). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;Roy, D. (2017). Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 478–483). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Choi, E., Hewlett, D., Uszkoreit, J., Lacoste, A., &amp;amp; Berant, J. (2017). Coarse-to-Fine Question Answering for Long Documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 209–220). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Yang, B., &amp;amp; Mitchell, T. (2017). A Joint Sequential and Relational Model for Frame-Semantic Parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Isonuma, M., Fujino, T., Mori, J., Matsuo, Y., &amp;amp; Sakata, I. (2017). Extractive Summarization Using Multi-Task Learning with Document Classification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2091–2100). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Wu, S., Zhang, D., Yang, N., Li, M., &amp;amp; Zhou, M. (2017). Sequence-to-Dependency Neural Machine Translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 698–707). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., &amp;amp; Zhang, W. (2017). R^3: Reinforced Reader-Ranker for Open-Domain Question Answering. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:57"&gt;&lt;p&gt;Zhao, K., &amp;amp; Huang, L. (2017). Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:57" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:58"&gt;&lt;p&gt;Katiyar, A., &amp;amp; Cardie, C. (2017). Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 917–928). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:58" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:59"&gt;&lt;p&gt;Hashimoto, K., Xiong, C., Tsuruoka, Y., &amp;amp; Socher, R. (2017). A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01587"&gt;http://arxiv.org/abs/1611.01587&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:59" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:60"&gt;&lt;p&gt;Bingel, J., &amp;amp; Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08303"&gt;http://arxiv.org/abs/1702.08303&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:60" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:61"&gt;&lt;p&gt;Swayamdipta, S., Thomson, S., Dyer, C., &amp;amp; Smith, N. A. (2017). Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold. Retrieved from &lt;a href="http://arxiv.org/abs/1706.09528"&gt;http://arxiv.org/abs/1706.09528&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:61" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more</title><description>This post gives an overview of highlights of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen.</description><link>http://ruder.io/highlights-emnlp-2017/</link><guid isPermaLink="false">9205b2f6-88bb-4db7-ac6d-9449953f1348</guid><category>nlp</category><category>natural language processing</category><category>word embeddings</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Fri, 22 Sep 2017 16:06:14 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/09/emnlp_landscape.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/09/emnlp_landscape.jpg" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Update 22.07.18: Paper ids in the ACL anthology were changed for some reason in many cases and had to be updated in this post.&lt;/p&gt;

&lt;p&gt;I spent the past week at the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen, Denmark. The conference handbook can be found &lt;a href="http://emnlp2017.net/downloads/handbook.pdf"&gt;here&lt;/a&gt; and the proceedings can be found &lt;a href="http://aclweb.org/anthology/D/D17/"&gt;here&lt;/a&gt;. Videos of the conference talks and presentations can be found &lt;a href="https://ku.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%229042b495-7b6b-4169-a5a1-d250cc0ee4ec%22"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The program consisted of two days of workshops and tutorials and three days of main conference. The conference was superbly organized, had a great venue, and a social event with fireworks.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/emnlp_fireworks-1.jpg" style="width: 60%; height: 60%" title="EMNLP 2017 fireworks" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 1: Fireworks at the social event&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;225 long papers, 107 papers, and 9 TACL papers had been accepted, with a clear uptick of submissions compared to last year. The number of long and short paper submissions to EMNLP this year was even higher than those at ACL for the first time within the last 13 years, as can be seen in Figure 2.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/long_short_paper_submissions_emnlp_2017.jpg" style="width: 60%; height: 60%" title="Long and short paper submissions EMNLP 2017" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 2: Long and short paper submissions at ACL and EMNLP from 2004-2017&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In the following, I will outline my highlights and list some research papers that caught my eye.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exciting datasets&lt;/strong&gt; &amp;nbsp; Evaluating your approach on CoNLL-2003 or PTB is appropriate for comparing against previous state-of-the-art, but kind of boring. The two following papers introduce datasets that allow you to test your model in more exciting settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1275.pdf"&gt;Durrett et al.&lt;/a&gt; release a new domain adaptation dataset. The dataset evaluates models on their ability to identify products being bought and sold in online cybercrime forums.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf"&gt;Kutuzov et al.&lt;/a&gt; evaluate their word embedding model on a new dataset that focuses on predicting insurgent armed groups based on geographical locations.&lt;/li&gt;
&lt;li&gt;While he did not introduce a new dataset, Nando de Freitas made the point during &lt;a href="http://emnlp2017.net/invited-speakers.html"&gt;his keynote&lt;/a&gt; that the best environment for learning and evaluating language is simulation.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/nando_de_freitas_vision.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 3: Nando de Freitas’ vision for AI research&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Return of the clusters&lt;/strong&gt; &amp;nbsp; Brown clusters, an agglomerative, hierarchical clustering of word types based on contexts that was introduced in 1992 seem to come in vogue again. They were found to be particularly helpful for cross-lingual applications, while clusters were key features in several approaches:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1269.pdf"&gt;Mayhew et al.&lt;/a&gt; found that Brown cluster features were an important signal for cross-lingual NER.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1309.pdf"&gt;Botha et al.&lt;/a&gt; use word clusters as a key feature in their small, efficient feed-forward neural networks.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1069.pdf"&gt;Mekala et al.&lt;/a&gt;’s new document representations cluster word embeddings, which give it an edge for text classification.&lt;/li&gt;
&lt;li&gt;In his talk at the &lt;a href="https://sites.google.com/view/sclem2017/home"&gt;SCLeM workshop&lt;/a&gt;, Noah Smith cites the benefits of using Brown clusters as features for tasks such as POS tagging and sentiment analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/noah_smith_benefits_clustering.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 4: Noah Smith on the benefits of clustering in his invited talk at the SCLeM workshop&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Distant supervision&lt;/strong&gt; &amp;nbsp; Distant supervision can be leveraged to collect large amounts of noisy training data, which can be useful in many applications. Some papers used novel forms of distant supervision to create new corpora or to train a model more effectively:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1126.pdf"&gt;Lan et al.&lt;/a&gt; use urls in tweets to collect a large corpus of paraphrase data. Paraphrase data is usually hard to create, so this approach facilitates the process significantly and enables a continuously expanding collection of paraphrases.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1169.pdf"&gt;Felbo et al.&lt;/a&gt; show that training on fine-grained emoji detection is more effective for pre-training sentiment and emotion models. Previous approaches primarily pre-trained on positive and negative emoticons or emotion hashtags.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Data selection&lt;/strong&gt; &amp;nbsp; The current generation of deep learning models is excellent at learning from data. However, we often do not pay much attention to the actual data our model is using. In many settings, we can improve upon the model by selecting the most relevant data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1063.pdf"&gt;Fang et al.&lt;/a&gt; reframe active learning as reinforcement learning and explicitly learn a data selection policy. Active learning is one of the best ways to create a model with as few annotations as possible; any improvement to this process is beneficial.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1147.pdf"&gt;Van der Wees et al.&lt;/a&gt; introduce dynamic data selection for NMT, which varies the selected subset of the training data between different training epochs. This approach has the potential to reduce the training time of NMT models at comparable or better performance.&lt;/li&gt;
&lt;li&gt;In &lt;a href="http://aclweb.org/anthology/D/D17/D17-1038.pdf"&gt;my paper with Barbara Plank&lt;/a&gt;, we use Bayesian Optimization to learn data selection policies for transfer learning and investigate how well these transfer across models, domains, and tasks. This approach brings us a step closer towards gaining a better understanding of what constitutes similarity between different tasks and domains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Character-level models&lt;/strong&gt; &amp;nbsp; Characters are nowadays used as standard features in most sequence models. The &lt;a href="https://sites.google.com/view/sclem2017/home"&gt;Subword and Character-level Models in NLP workshop&lt;/a&gt; discussed approaches in more detail, with invited talks on subword language models and character-level NMT. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1298.pdf"&gt;Schmaltz et al.&lt;/a&gt; find that character-based sequence-to-sequence models outperform word-based models and models with character convolutions for sentence correction.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ryancotterell.github.io/"&gt;Ryan Cotterell&lt;/a&gt; gave a great, movie-inspired tutorial on combining the best of FSTs (cowboys) and sequence-to-sequence models (aliens) for string-to-string transduction. While evaluated on morphological segmentation, the tutorial raised awareness in an entertaining way that often the best of both worlds, i.e. a combination of traditional and neural approaches performs best.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/ryan_cotterell_fsts_seq2seq.jpg" style="width: 60%; height: 60%" title="Combining FSTs and seq2seq models" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 5: Ryan Cotterell on combining FSTs and seq2seq models for string-to-string transduction  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Word embeddings&lt;/strong&gt; &amp;nbsp; Research in word embeddings has matured and now mainly tries to 1) address deficits of word2vec, such as its ability of dealing with OOV words; 2) extend it to new settings, e.g. modelling the relations of words over time; and 3) understand the induced representations better:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1010.pdf"&gt;Pinter et al.&lt;/a&gt; propose an approach for generating OOV word embeddings by training a character-based BiLSTM to generate embeddings that are close to pre-trained ones. This approach is promising as it provides us with a more sophisticated way to deal with out-of-vocabulary words than replacing them with an &lt;unk&gt; token. &lt;/unk&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1030.pdf"&gt;Herbelot and Baroni&lt;/a&gt; slightly modify word2vec to allow it to learn embeddings for OOV words from few data.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1121.pdf"&gt;Rosin et al.&lt;/a&gt; propose a model for analyzing &lt;em&gt;when&lt;/em&gt; two words relate to each other.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf"&gt;Kutuzov et al.&lt;/a&gt; propose another model that analyzes how two words relate to each other over time.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1033.pdf"&gt;Hasan and Curry&lt;/a&gt; improve the performance of word embeddings on word similarity tasks by re-embedding them in a manifold.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1312.pdf"&gt;Yang et al.&lt;/a&gt; introduce a simple approach to learning cross-domain word embeddings. Creating embeddings tuned on a small, in-domain corpus is still a challenge, so it is nice to see more approaches addressing this pain point.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1308.pdf"&gt;Mimno and Thompson&lt;/a&gt; try to understand the geometry of word2vec better. They show that the learned word embeddings are positioned diametrically opposite of their context vectors in the embedding space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cross-lingual&lt;/strong&gt; &amp;nbsp; An increasing number of papers evaluate their methods on multiple languages. In addition, there was an excellent &lt;a href="http://emnlp2017.net/tutorials/day2/xling_word_rep.html"&gt;tutorial on cross-lingual word representations&lt;/a&gt;, which summarized and tried to unify much of the existing literature. Slides of the tutorial are available &lt;a href="http://people.ds.cam.ac.uk/iv250/tutorial/xlingrep-tutorial.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1268.pdf"&gt;Malaviya et al.&lt;/a&gt; train a many-to-one NMT to translate 1017 languages into English and use this model to predict information missing from typological databases. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1269.pdf"&gt;Mayhew et al.&lt;/a&gt; introduce a cheap translation method for cross-lingual NER that only requires a bilingual dictionary. They even perform a case study on Uyghur, a truly low-resource language.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/D/D17/D17-1302.pdf"&gt;Kim et al.&lt;/a&gt; present a cross-lingual transfer learning model for POS tagging without parallel data. Parallel data is expensive to create and rarely available for low-resource languages, so this approach fills an important need.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1270.pdf"&gt;Vulic et al.&lt;/a&gt; propose a new cross-lingual transfer method for inducing VerbNets for different languages. The method leverages vector space specialisation, an effective word embedding post-processing technique similar to retro-fitting.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1258.pdf"&gt;Braud et al.&lt;/a&gt; propose a robust, cross-lingual discourse segmentation model that only relies on POS tags. They show that dependency information is less useful than expected; it is important to evaluate our models on multiple languages, so we do not overfit to features that are specific to analytic languages, such as English.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/anders_sogaard_crosslingual.jpg" style="width: 60%; height: 60%" title="Anders Søgaard on cross-lingual embeddings" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 6: Anders Søgaard demonstrating the similarities between different cross-lingual embedding models at the cross-lingual representations tutorial  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Summarization&lt;/strong&gt; &amp;nbsp; &lt;a href="https://summarization2017.github.io/"&gt;The Workshop on New Frontiers of Summarization&lt;/a&gt; brought researchers together to discuss key issues related to automatic summarization. Much of the research on summarization sought to develop new datasets and tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://research.google.com/pubs/author39008.html"&gt;Katja Filippova&lt;/a&gt; gave an interesting talk on sentence compression and passage summarization for Q&amp;amp;A. She described how they went from syntax-based methods to Deep Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4508.pdf"&gt;Volkse et al.&lt;/a&gt; created a new summarization corpus by looking for ‘TL;DR’ on Reddit. This is another example of a creative use of distant supervision, leveraging information that is already contained in the data in order to create a new corpus.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1320.pdf"&gt;Falke and Gurevych&lt;/a&gt; won the best resource paper award for creating a new summary corpus that is based on concept maps rather than textual summaries. The concept map can be explored using a &lt;a href="https://github.com/UKPLab/emnlp2017-graphdocexplore"&gt;graph-based document exploration system&lt;/a&gt;, which is available as a &lt;a href="http://cmaps.ukp.informatik.tu-darmstadt.de/graph-doc-explorer/#!/"&gt;demo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W17-4504"&gt;Pasunuru et al.&lt;/a&gt; use multi-task learning to improve abstractive summarization by leveraging entailment generation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1223.pdf"&gt;Isonuma et al.&lt;/a&gt; also use multi-task learning with document classification in conjunction with curriculum learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4512.pdf"&gt;Li et al.&lt;/a&gt; propose a new task, reader-aware multi-document summarization, which uses comments of articles, along with a dataset for this task.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1064.pdf"&gt;Naranyan et al.&lt;/a&gt; propose another new task, split and rephrase, which aims to split a complex sentence into a sequence of shorter sentences with the same meaning, and also release a new dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4511.pdf"&gt;Ghalandari&lt;/a&gt; revisits the traditional centroid-based method and proposes a new strong baseline for multi-document summarization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; &amp;nbsp; Data and model-inherent bias is an issue that is receiving more attention in the community. Some papers investigate and propose methods to address the bias in certain datasets and evaluations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1109.pdf"&gt;Chaganty et al.&lt;/a&gt; investigate bias in the evaluation of knowledge base population models and propose an importance sampling-based evaluation to mitigate the bias.&lt;/li&gt;
&lt;li&gt;Dan Jurafsky gave a truly insightful &lt;a href="http://emnlp2017.net/invited-speakers.html"&gt;keynote&lt;/a&gt; about his three year-long study analyzing the body camera recordings his team obtained from the Oakland police department for racial bias. Besides describing the first contemporary linguistic study of officer-community member interaction, he also provided entertaining insights on the language of food (cheaper restaurants use terms related to addiction, more expensive venues use language related to indulgence) and the challenges of interdisciplinary publishing. The entire keynote can be viewed &lt;a href="https://ku.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4f45fc4c-e7d6-4743-b843-0aae2ae8b17e&amp;amp;utm_campaign=Revue%20newsletter&amp;amp;utm_medium=Newsletter&amp;amp;utm_source=NLP%20News"&gt;here&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1118.pdf"&gt;Dubossarsky et al.&lt;/a&gt; analyze the bias in word representation models and propose that recently proposed laws of semantic change must be revised. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/D17-1323"&gt;Zhao et al.&lt;/a&gt; won the best paper award for an approach using Lagrangian relaxation to inject constraints based on corpus-level label statistics. An important finding of their work is bias amplification: While some bias is inherent in all datasets, they observed that models trained on the data amplified its bias. While a gendered dataset might only contain women in 30% of examples, the situation at prediction time might thus be even more dire.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/bias_amplification.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 7: Zhao et al.’s proposed method for reducing bias amplification  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Argument mining &amp;amp; debate analysis&lt;/strong&gt; &amp;nbsp; Argument mining is closely related to summarization. In order to summarize argumentative texts, we have to understand claims and their justifications. This research area had the &lt;a href="https://argmining2017.wordpress.com/"&gt;4th Workshop on Argument Mining&lt;/a&gt; dedicated to it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-5102.pdf"&gt;Hidey et al.&lt;/a&gt; analyse the semantic types of claims (e.g. agreement, interpretation) and premises (ethos, logos, pathos) in the Subreddit &lt;a href="https://www.reddit.com/r/changemyview/"&gt;Change My View&lt;/a&gt;. This is another creative use of reddit to create a dataset and analyze linguistic patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-5106.pdf"&gt;Wachsmut et al.&lt;/a&gt; presented an argument web search engine, which can be queried &lt;a href="http://www.args.me/"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D17-1261"&gt;Potash and Rumshinsky&lt;/a&gt; predict the winner of debates, based on audience favorability.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1166.pdf"&gt;Swamy et al.&lt;/a&gt; also forecast winners for the Oscars, the US presidential primaries, and many other contests based on user predictions on Twitter. They create a dataset to test their approach.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1164.pdf"&gt;Zhang et al.&lt;/a&gt; analyze the rhetorical role of questions in discourse.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1142.pdf"&gt;Liu et al.&lt;/a&gt; show that argument-based features are also helpful for predicting review helpfulness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Multi-agent communication&lt;/strong&gt; &amp;nbsp; Multi-agent communication is a niche topic, which has nevertheless received some recent interest, notably in the representation learning community. Most papers deal with a scenario where two agents play a communicative referential game. The task is interesting, as the agents are required to cooperate and have been observed to develop a common pseudo-language in the process.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1311.pdf"&gt;Andreas and Klein&lt;/a&gt; investigate the structure encoded by RNN representations for messages in a communication game. They find that the mistakes are similar to the ones made by humans. In addition, they find that negation is encoded as a linear relationship in the vector space.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1321.pdf"&gt;Kottur et al.&lt;/a&gt; show in their best short paper that language does not emerge naturally when two agents are cooperating, but that they can be coerced to develop compositional expressions.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/multi-agent_setup.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 8: The multi-agent setup in the paper of Kottur et al.  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Relation extraction&lt;/strong&gt; &amp;nbsp; Extracting relations from documents is more compelling than simply extracting entities or concepts. Some papers improve upon existing approaches using better distant supervision or adversarial training:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D17-1189"&gt;Liu et al.&lt;/a&gt; reduce the noise in distantly supervised relation extraction with a soft-label method.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1004.pdf"&gt;Zhang et al.&lt;/a&gt; publish TACRED, a large supervised dataset knowledge base population, as well as a new model.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1187.pdf"&gt;Wu et al.&lt;/a&gt; improve the precision of relation extraction with adversarial training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Document and sentence representations&lt;/strong&gt; &amp;nbsp; Learning better sentence representations is closely related to learning more general word representations. While word embeddings still have to be contextualized, sentence representations are promising as they can be directly applied to many different tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1069.pdf"&gt;Mekala et al.&lt;/a&gt; propose a novel technique for building document vectors from word embeddings, with good results for text classification. They use a combination of adding and concatenating word embeddings to represent multiple topics of a document, based on word clusters. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1070.pdf"&gt;Conneau et al.&lt;/a&gt; learn sentence representations from the SNLI dataset and evaluate them on 12 different tasks. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These were my highlights. Naturally, I was not able to attend every session and see every paper. What were your highlights from the conference or which papers from the &lt;a href="http://aclweb.org/anthology/D/D17/"&gt;proceedings&lt;/a&gt; did you like most? Let me know in the comments below.&lt;/p&gt;</content:encoded></item><item><title>Frequently asked questions (FAQ)</title><description>Frequently asked questions about Deep Learning and Natural Language Processing.</description><link>http://ruder.io/faq/</link><guid isPermaLink="false">86aa940a-ab55-4728-94b3-8b8bc7fded02</guid><dc:creator>Sebastian Ruder</dc:creator><pubDate>Wed, 13 Sep 2017 14:04:03 GMT</pubDate><content:encoded>&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/faq/#whatresourcesshouldiusetogetstartedwithdeeplearning"&gt;What resources should I use to get started with Deep Learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/faq/#whatresourcesshouldiusetogetstartedwithnaturallanguageprocessingnlp"&gt;What resources should I use to get started with Natural Language Processing?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/faq/#whatresearchtopicshouldiworkon"&gt;What research topic should I work on?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/faq/#whatisacommondatasetformytask"&gt;What is a common dataset for my task?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/faq/#whatisthestateoftheartformytask"&gt;What is the state-of-the-art for my task?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I get regularly asked certain questions. Rather than reiterating the answers, I've put them up here so they are easy to find.&lt;/p&gt;

&lt;h2 id="whatresourcesshouldiusetogetstartedwithdeeplearning"&gt;What resources should I use to get started with Deep Learning?&lt;/h2&gt;

&lt;p&gt;Read the Deep Learning book. It is available &lt;a href="http://www.deeplearningbook.org/"&gt;as a pdf&lt;/a&gt; or &lt;a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618"&gt;on Amazon&lt;/a&gt;. If you're new to Machine Learning, take the &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Coursera Machine Learning course by Andrew Ng&lt;/a&gt;. For a more complete overview of ML online courses, have a look at &lt;a href="https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0"&gt;this excellent Medium post&lt;/a&gt;. In terms of Deep Learning online courses, take the courses offered by &lt;a href="http://www.fast.ai/"&gt;fast.ai&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="whatresourcesshouldiusetogetstartedwithnaturallanguageprocessingnlp"&gt;What resources should I use to get started with Natural Language Processing (NLP)?&lt;/h2&gt;

&lt;p&gt;If you're serious about learning NLP, I would recommend to study the fundamentals first by reading &lt;a href="https://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=sr_1_1"&gt;Speech and Language Processing, 2nd Edition by Jurafsky and Martin&lt;/a&gt;. The 3rd edition is in progress and some chapters are available as &lt;a href="https://web.stanford.edu/~jurafsky/slp3/"&gt;pdf&lt;/a&gt;. For learning about Deep Learning for NLP, take the &lt;a href="http://web.stanford.edu/class/cs224n/"&gt;Stanford online course&lt;/a&gt; and read &lt;a href="https://www.jair.org/media/4992/live-4992-9623-jair.pdf"&gt;Yoav Goldberg's primer&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="whatresearchtopicshouldiworkon"&gt; What research topic should I work on?&lt;/h2&gt;

&lt;p&gt;I have collected &lt;a href="http://ruder.io/requests-for-research/"&gt;research directions around transfer learning and NLP&lt;/a&gt; that might be of interest to you.&lt;/p&gt;

&lt;h2 id="whatisacommondatasetformytask"&gt; What is a common dataset for my task?&lt;/h2&gt;

&lt;p&gt;Have a look at &lt;a href="https://github.com/sebastianruder/NLP-progress"&gt;this collection of datasets for many common NLP tasks&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="whatisthestateoftheartformytask"&gt;What is the state-of-the-art for my task?&lt;/h2&gt;

&lt;p&gt;Have a look at &lt;a href="https://github.com/sebastianruder/NLP-progress"&gt;this collection of the state-of-the-art for many common NLP tasks&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title>Learning to select data for transfer learning</title><description>This blog post discusses the motivation and findings of our EMNLP 2017 paper Learning to select data for transfer learning.</description><link>http://ruder.io/learning-select-data/</link><guid isPermaLink="false">2d2ccdff-15b1-493d-888a-bec8acb5a346</guid><category>natural language processing</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Fri, 18 Aug 2017 10:59:23 GMT</pubDate><content:encoded>&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href="http://blog.aylien.com/learning-select-data-transfer-learning/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In Machine Learning, the common assumption is that the data our model is applied to is the same, i.e. comes from the same distribution as the data we used for training. This assumption is revealed to be false as soon as we apply our models to the real world: many of the data sources we encounter will be very different from our original training data. In practice, this causes the performance of our model to deteriorate significantly.&lt;/p&gt;

&lt;p&gt;Domain adaptation is a prominent approach to transfer learning that can help to bridge this discrepancy between the training and test data. Domain adaptation is a type of transfer learning, which I have written about &lt;a href="http://ruder.io/transfer-learning/"&gt;here&lt;/a&gt;. Domain adaptation methods typically seek to identify features that are shared between the domains or learn representations that are general enough to be useful for both domains. In this blog post, I will discuss the motivation for, and the findings of the &lt;a href="https://arxiv.org/abs/1707.05246"&gt;recent paper&lt;/a&gt; that I published with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Planck&lt;/a&gt;. In it, we outline a complementary approach to domain adaptation – rather than learning a model that can &lt;em&gt;adapt&lt;/em&gt; between the domains, we will learn to &lt;em&gt;select data&lt;/em&gt; that is useful for training our model.&lt;/p&gt;

&lt;h1 id="preventingnegativetransfer"&gt;Preventing Negative Transfer&lt;/h1&gt;

&lt;p&gt;The main motivation behind selecting data for transfer learning is to prevent negative transfer. Negative transfer occurs if the information from our source training data is not only unhelpful but actually counter-productive for doing well on our target domain. The classic example for negative transfer comes from sentiment analysis: if we train a model to predict the sentiment of book reviews, we can expect the model to do well on domains that are similar to book reviews. Transferring a model trained on book reviews to reviews of electronics, however, results in negative transfer, as many of the terms our model learned to associate with a certain sentiment for books, e.g. “page-turner”, “gripping”, or — worse — “dangerous” and “electrifying”, will be meaningless or have different connotations for electronics reviews.&lt;/p&gt;

&lt;p&gt;In the classic scenario of adapting from one source to one target domain, the only thing we can do about this is to create a model that is capable of disentangling these shifts in meaning. However, adapting between two very dissimilar domains still fails frequently or leads to painfully poor performance.&lt;/p&gt;

&lt;p&gt;In the real world, we typically have access to multiple data sources. In this case, one thing that we can do is to train our model on the data that is most helpful for our target domain. It is unclear, however, what the best way to determine the helpfulness of source data with respect to a target domain is. Existing work generally relies on measures of similarity between the source and the target domain.&lt;/p&gt;

&lt;h1 id="bayesianoptimizationfordataselection"&gt;Bayesian Optimization for Data Selection&lt;/h1&gt;

&lt;p&gt;Our hypothesis is that the best way to select training data for transfer learning depends on the task and the target domain. In addition, while existing measures only consider data in relation to the target domain, we also argue that some training examples are inherently more helpful than others.&lt;/p&gt;

&lt;p&gt;For these reasons, we propose to learn a data selection measure for transfer learning. We do this using Bayesian Optimization, a framework that has been used successfully to optimize hyperparameters in neural networks and which can be used to optimize any black-box function. We learn this function by defining several features relating to the similarity of the training data to the target domain as well as to its diversity. Over the course of several iterations, the data selection model then learns the importance of each of those features for the relevant task.&lt;/p&gt;

&lt;h1 id="evaluationconclusion"&gt;Evaluation &amp;amp; Conclusion&lt;/h1&gt;

&lt;p&gt;We evaluate our approach on three tasks, sentiment analysis, part-of-speech tagging, and dependency parsing and compare our approach to random selection as well as existing methods that select either the most similar source domain or the most similar training examples.&lt;/p&gt;

&lt;p&gt;For sentiment analysis on reviews, training on the most similar domain is a strong baseline as review categories are clearly delimited. We significantly improve upon this baseline and demonstrate that diversity complements similarity. We even achieve performance competitive with a state-of-the-art domain adaptation approach, despite not performing any adaptation.&lt;/p&gt;

&lt;p&gt;We observe smaller, but consistent improvements for part-of-speech tagging and dependency parsing. Lastly, we evaluate how well learned measures transfer across models, tasks, and domains. We find that learning a data selection measure can be learned with a simpler model, which is used as a proxy for a state-of-the-art model. Transfer across domains is robust, while transfer across tasks holds — as one would expect — for related tasks such as POS tagging and parsing, but fails for dissimilar tasks, e.g. parsing and sentiment analysis.&lt;/p&gt;

&lt;p&gt;In the paper, we demonstrate the importance of selecting relevant data for transfer learning. We show that taking into account task and domain-specific characteristics and learning an appropriate data selection measure outperforms off-the-shelf metrics. We find that diversity complements similarity in selecting appropriate training data and that learned measures can be transferred robustly across models, domains, and tasks.&lt;/p&gt;

&lt;p&gt;This work will be presented at the &lt;a href="http://emnlp2017.net/"&gt;2017 Conference on Empirical Methods in Natural Language Processing&lt;/a&gt;. More details can be found in the paper &lt;a href="https://arxiv.org/abs/1707.05246"&gt;here&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title>Deep Learning for NLP Best Practices</title><description>A collection of best practices for Deep Learning for a wide array of Natural Language Processing tasks. </description><link>http://ruder.io/deep-learning-nlp-best-practices/</link><guid isPermaLink="false">c28facc9-6e69-42e1-b66f-d7449f387e9c</guid><category>nlp</category><category>natural language processing</category><category>deep learning</category><category>machine learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Tue, 25 Jul 2017 19:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/07/attention_bahdanau-1.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/07/attention_bahdanau-1.png" alt="Deep Learning for NLP Best Practices"&gt;&lt;p&gt;Update July 26, 2017: For additional context, the &lt;a href="https://news.ycombinator.com/item?id=14852704"&gt;HackerNews discussion&lt;/a&gt; about this post.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#bestpractices"&gt;Best practices&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#wordembeddings"&gt;Word embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#depth"&gt;Depth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#layerconnections"&gt;Layer connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#dropout"&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#attention"&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#optimization"&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#ensembling"&gt;Ensembling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#hyperparameteroptimization"&gt;Hyperparameter optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#lstmtricks"&gt;LSTM tricks&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#taskspecificbestpractices"&gt;Task-specific best practices&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#classification"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#sequencelabelling"&gt;Sequence labelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#naturallanguagegeneration"&gt;Natural language generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#neuralmachinetranslation"&gt;Neural machine translation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This post is a collection of best practices for using neural networks in Natural Language Processing. It will be updated periodically as new insights become available and in order to keep track of our evolving understanding of Deep Learning for NLP.&lt;/p&gt;

&lt;p&gt;There has been a &lt;a href="https://twitter.com/IAugenstein/status/710837374473920512"&gt;running joke&lt;/a&gt; in the NLP community that an LSTM with attention will yield state-of-the-art performance on any task. While this has been true over the course of the last two years, the NLP community is slowly moving away from this now standard baseline and towards more interesting models.&lt;/p&gt;

&lt;p&gt;However, we as a community do not want to spend the next two years independently (re-)discovering the &lt;em&gt;next&lt;/em&gt; LSTM with attention. We do not want to reinvent tricks or methods that have already been shown to work. While many existing Deep Learning libraries already encode best practices for working with neural networks in general, such as initialization schemes, many other details, particularly task or domain-specific considerations, are left to the practitioner.&lt;/p&gt;

&lt;p&gt;This post is not meant to keep track of the state-of-the-art, but rather to collect best practices that are relevant for a wide range of tasks. In other words, rather than describing one particular architecture, this post aims to collect the features that underly successful architectures. While many of these features will be most useful for pushing the state-of-the-art, I hope that wider knowledge of them will lead to stronger evaluations, more meaningful comparison to baselines, and inspiration by shaping our intuition of what works.&lt;/p&gt;

&lt;p&gt;I assume you are familiar with neural networks as applied to NLP (if not, I recommend Yoav Goldberg's &lt;a href="https://www.jair.org/media/4992/live-4992-9623-jair.pdf"&gt;excellent primer&lt;/a&gt; [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;]) and are interested in NLP in general or in a particular task. The main goal of this article is to get you up to speed with the relevant best practices so you can make meaningful contributions as soon as possible.&lt;/p&gt;

&lt;p&gt;I will first give an overview of best practices that are relevant for most tasks. I will then outline practices that are relevant for the most common tasks, in particular classification, sequence labelling, natural language generation, and neural machine translation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; Treating something as &lt;em&gt;best practice&lt;/em&gt; is notoriously difficult: Best according to what? What if there are better alternatives? This post is based on my (necessarily incomplete) understanding and experience. In the following, I will only discuss practices that have been reported to be beneficial independently by &lt;em&gt;at least&lt;/em&gt; two different groups. I will try to give at least two references for each best practice.&lt;/p&gt;

&lt;h1 id="bestpractices"&gt;Best practices&lt;/h1&gt;

&lt;h2 id="wordembeddings"&gt;Word embeddings&lt;/h2&gt;

&lt;p&gt;Word embeddings are arguably the most widely known best practice in the recent history of NLP. It is well-known that using pre-trained embeddings helps (Kim, 2014) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;]. The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] or part-of-speech (POS) tagging (Plank et al., 2016) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="depth"&gt; Depth&lt;/h2&gt;

&lt;p&gt;While we will not reach the depths of computer vision for a while, neural networks in NLP have become progressively deeper. State-of-the-art approaches now regularly use deep Bi-LSTMs, typically consisting of 3-4 layers, e.g. for POS tagging (Plank et al., 2016) and semantic role labelling (He et al., 2017) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;]. Models for some tasks can be even deeper, cf. Google's NMT model with 8 encoder and 8 decoder layers (Wu et al., 2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. In most cases, however, performance improvements of making the model deeper than 2 layers are minimal (Reimers &amp;amp; Gurevych, 2017) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;These observations hold for most sequence tagging and structured prediction problems. For classification, deep or very deep models perform well only with character-level input and shallow word-level models are still the state-of-the-art (Zhang et al., 2015; Conneau et al., 2016; Le et al., 2017) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="layerconnections"&gt;Layer connections&lt;/h2&gt;

&lt;p&gt;For training deep neural networks, some tricks are essential to avoid the vanishing gradient problem. Different layers and connections have been proposed. Here, we will discuss three: i) highway layers, ii) residual connections, and iii) dense connections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Highway layers&lt;/strong&gt; &amp;nbsp; Highway layers (Srivastava et al., 2015) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] are inspired by the gates of an LSTM. First let us assume a one-layer MLP, which applies an affine transformation followed by a non-linearity \(g\) to its input \(\mathbf{x}\): &lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{b})\)&lt;/p&gt;

&lt;p&gt;A highway layer then computes the following function instead:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = \mathbf{t} \odot g(\mathbf{W} \mathbf{x} + \mathbf{b}) + (1-\mathbf{t}) \odot \mathbf{x} \)&lt;/p&gt;

&lt;p&gt;where \(\odot\) is elementwise multiplication, \(\mathbf{t} = \sigma(\mathbf{W}_T \mathbf{x} + \mathbf{b}_T)\) is called the &lt;em&gt;transform&lt;/em&gt; gate, and \((1-\mathbf{t})\) is called the &lt;em&gt;carry&lt;/em&gt; gate. As we can see, highway layers are similar to the gates of an LSTM in that they adaptively &lt;em&gt;carry&lt;/em&gt; some dimensions of the input directly to the output. &lt;/p&gt;

&lt;p&gt;Highway layers have been used pre-dominantly to achieve state-of-the-art results for language modelling (Kim et al., 2016; Jozefowicz et al., 2016; Zilly et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;], but have also been used for other tasks such as speech recognition (Zhang et al., 2016) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;]. &lt;a href="http://people.idsia.ch/~rupesh/very_deep_learning/"&gt;Sristava's page&lt;/a&gt; contains more information and code regarding highway layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Residual connections&lt;/strong&gt; &amp;nbsp; Residual connections (He et al., 2016) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] have been first proposed for computer vision and were the main factor for winning ImageNet 2016. Residual connections are even more straightforward than highway layers and learn the following function:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{b}) + \mathbf{x}\) &lt;/p&gt;

&lt;p&gt;which simply adds the input of the current layer to its output via a short-cut connection. This simple modification mitigates the vanishing gradient problem, as the model can default to using the identity function if the layer is not beneficial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense connections&lt;/strong&gt; &amp;nbsp; Rather than just adding layers from each layer to the next, dense connections (Huang et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] (best paper award at CVPR 2017) add direct connections from each layer to all subsequent layers. Let us augment the layer output \(h\) and layer input \(x\) with indices \(l\) indicating the current layer. Dense connections then feed the concatenated output from all previous layers as input to the current layer:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h}^l = g(\mathbf{W}[\mathbf{x}^1; \ldots; \mathbf{x}^l] + \mathbf{b})\)&lt;/p&gt;

&lt;p&gt;where \([\cdot; \cdot]\) represents concatenation. Dense connections have been successfully used in computer vision. They have also found to be useful for Multi-Task Learning of different NLP tasks (Ruder et al., 2017) [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;], while a residual variant that uses summation has been shown to consistently outperform residual connections for neural machine translation (Britz et al., 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="dropout"&gt;Dropout&lt;/h2&gt;

&lt;p&gt;While batch normalisation in computer vision has made other regularizers obsolete in most applications, dropout (Srivasta et al., 2014) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] is still the go-to regularizer for deep neural networks in NLP. A dropout rate of 0.5 has been shown to be effective in most scenarios (Kim, 2014). In recent years, variations of dropout such as adaptive (Ba &amp;amp; Frey, 2013) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;] and evolutional dropout (Li et al., 2016) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;] have been proposed, but none of these have found wide adoption in the community. The main problem hindering dropout in NLP has been that it could not be applied to recurrent connections, as the aggregating dropout masks would effectively zero out embeddings over time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recurrent dropout&lt;/strong&gt; &amp;nbsp; Recurrent dropout (Gal &amp;amp; Ghahramani, 2016) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] addresses this issue by applying the same dropout mask across timesteps at layer \(l\). This avoids amplifying the dropout noise along the sequence and leads to effective regularization for sequence models. Recurrent dropout has been used for instance to achieve state-of-the-art results in semantic role labelling (He et al., 2017) and language modelling (Melis et al., 2017) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="multitasklearning"&gt;Multi-task learning&lt;/h2&gt;

&lt;p&gt;If additional data is available, multi-task learning (MTL) can often be used to improve performance on the target task. Have a look &lt;a href="http://ruder.io/multi-task/index.html"&gt;this blog post&lt;/a&gt; for more information on MTL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Auxiliary objectives&lt;/strong&gt; &amp;nbsp; We can often find auxiliary objectives that are useful for the task we care about (Ruder, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;]. While we can already predict surrounding words in order to pre-train word embeddings (Mikolov et al., 2013), we can also use this as an auxiliary objective during training (Rei, 2017) [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]. A similar objective has also been used by  (Ramachandran et al., 2016) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] for sequence-to-sequence models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task-specific layers&lt;/strong&gt; &amp;nbsp; While the standard approach to MTL for NLP is hard parameter sharing, it is beneficial to allow the model to learn task-specific layers. This can be done by placing the output layer of one task at a lower level (Søgaard &amp;amp; Goldberg, 2016) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;]. Another way is to induce private and shared subspaces (Liu et al., 2017; Ruder et al., 2017) [&lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="attention"&gt; Attention&lt;/h2&gt;

&lt;p&gt;Attention is most commonly used in sequence-to-sequence models to attend to encoder states, but can also be used in any sequence model to look back at past states. Using attention, we obtain a context vector \(\mathbf{c}_i\) based on hidden states \(\mathbf{s}_1, \ldots, \mathbf{s}_m\) that can be used together with the current hidden state \(\mathbf{h}_i\) for prediction. The context vector \(\mathbf{c}_i\) at position is calculated as an average of the previous states weighted with the attention scores \(\mathbf{a}_i\):&lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{c}_i &amp;amp;= \sum\limits_j a_{ij}\mathbf{s}_j\\
\mathbf{a}_i &amp;amp;= \text{softmax}(f_{att}(\mathbf{h}_i, \mathbf{s}_j))
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;The attention function \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\) calculates an unnormalized alignment score between the current hidden state \(\mathbf{h}_i\) and the previous hidden state \(\mathbf{s}_j\). In the following, we will discuss four attention variants: i) additive attention, ii) multiplicative attention, iii) self-attention, and iv) key-value attention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additive attention&lt;/strong&gt; &amp;nbsp; The original attention mechanism (Bahdanau et al., 2015) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] uses a one-hidden layer feed-forward network to calculate the attention alignment:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_a[\mathbf{h}_i; \mathbf{s}_j]) \)&lt;/p&gt;

&lt;p&gt;where \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are learned attention parameters. Analogously, we can also use matrices \(\mathbf{W}_1\) and \(\mathbf{W}_2\) to learn separate transformations for \(\mathbf{h}_i\) and \(\mathbf{s}_j\) respectively, which are then summed:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}_j) \)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multiplicative attention&lt;/strong&gt; &amp;nbsp; Multiplicative attention (Luong et al., 2015) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] simplifies the attention operation by calculating the following function:&lt;/p&gt;

&lt;p&gt;\(f_{att}(h_i, s_j) = h_i^\top \mathbf{W}_a s_j \)&lt;/p&gt;

&lt;p&gt;Additive and multiplicative attention are similar in complexity, although multiplicative attention is faster and more space-efficient in practice as it can be implemented more efficiently using matrix multiplication. Both variants perform similar for small dimensionality \(d_h\) of the decoder states, but additive attention performs better for larger dimensions. One way to mitigate this is to scale \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\) by \(1 / \sqrt{d_h}\) (Vaswani et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Attention cannot only be used to attend to encoder or previous hidden states, but also to obtain a distribution over other features, such as the word embeddings of a text as used for reading comprehension (Kadlec et al., 2017) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. However, attention is not directly applicable to classification tasks that do not require additional information, such as sentiment analysis. In such models, the final hidden state of an LSTM or an aggregation function such as max pooling or averaging is often used to obtain a sentence representation. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Self-attention&lt;/strong&gt; &amp;nbsp; Without any additional information, however, we can still extract relevant aspects from the sentence by allowing it to attend to itself using self-attention (Lin et al., 2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;]. Self-attention, also called intra-attention has been used successfully in a variety of tasks including reading comprehension (Cheng et al., 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;], textual entailment (Parikh et al., 2016) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;], and abstractive summarization (Paulus et al., 2017) [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;We can simplify additive attention to compute the unnormalized alignment score for each hidden state \(\mathbf{h}_i\):&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_a \mathbf{h}_i) \)&lt;/p&gt;

&lt;p&gt;In matrix form, for hidden states \(\mathbf{H} = \mathbf{h}_1, \ldots, \mathbf{h}_n\) we can calculate the attention vector \(\mathbf{a}\) and the final sentence representation \(\mathbf{c}\) as follows:&lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{a} &amp;amp;= \text{softmax}(\mathbf{v}_a \text{tanh}(\mathbf{W}_a \mathbf{H}^\top))\\
\mathbf{c} &amp;amp; = \mathbf{H} \mathbf{a}^\top
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;Rather than only extracting one vector, we can perform several hops of attention by using a matrix \(\mathbf{V}_a\) instead of \(\mathbf{v}_a\), which allows us to extract an attention matrix \(\mathbf{A}\): &lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{A} &amp;amp;= \text{softmax}(\mathbf{V}_a \text{tanh}(\mathbf{W}_a \mathbf{H}^\top))\\
\mathbf{C} &amp;amp; = \mathbf{A} \mathbf{H}
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;In practice, we enforce the following orthogonality constraint to penalize redundancy and encourage diversity in the attention vectors in the form of the squared Frobenius norm:&lt;/p&gt;

&lt;p&gt;\(\Omega = \|(\mathbf{A}\mathbf{A}^\top - \mathbf{I} \|^2_F \)&lt;/p&gt;

&lt;p&gt;A similar multi-head attention is also used by Vaswani et al. (2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key-value attention&lt;/strong&gt; &amp;nbsp; Finally, key-value attention (Daniluk et al., 2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] is a recent attention variant that separates form from function by keeping separate vectors for the attention calculation. It has also been found useful for different document modelling tasks (Liu &amp;amp; Lapata, 2017) [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;]. Specifically, key-value attention splits each hidden vector \(\mathbf{h}_i\) into a key \(\mathbf{k}_i\) and a value \(\mathbf{v}_i\): \([\mathbf{k}_i; \mathbf{v}_i] = \mathbf{h}_i\). The keys are used for calculating the attention distribution \(\mathbf{a}_i\) using additive attention:&lt;/p&gt;

&lt;p&gt;\(\mathbf{a}_i = \text{softmax}(\mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 [\mathbf{k}_{i-L}; \ldots; \mathbf{k}_{i-1}] + (\mathbf{W}_2 \mathbf{k}_i)\mathbf{1}^\top)) \)&lt;/p&gt;

&lt;p&gt;where \(L\) is the length of the attention window and \(\mathbf{1}\) is a vector of ones. The  values are then used to obtain the context representation \(\mathbf{c}_i\):&lt;/p&gt;

&lt;p&gt;\(\mathbf{c}_i = [\mathbf{v}_{i-L}; \ldots; \mathbf{v}_{i-1}] \mathbf{a}^\top\) &lt;/p&gt;

&lt;p&gt;The context \(\mathbf{c}_i\) is used together with the current value \(\mathbf{v}_i\) for prediction.&lt;/p&gt;

&lt;h2 id="optimization"&gt; Optimization&lt;/h2&gt;

&lt;p&gt;The optimization algorithm and scheme is often one of the parts of the model that is used as-is and treated as a black-box. Sometimes, even slight changes to the algorithm, e.g. reducing the \(\beta_2\) value in Adam (Dozat &amp;amp; Manning, 2017) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;] can make a large difference to the optimization behaviour.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization algorithm&lt;/strong&gt; &amp;nbsp; Adam (Kingma &amp;amp; Ba, 2015) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] is one of the most popular and widely used optimization algorithms and often the go-to optimizer for NLP researchers. It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam (Wu et al., 2016). Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam (Zhang et al., 2017) [&lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization scheme&lt;/strong&gt; &amp;nbsp; While Adam internally tunes the learning rate for every parameter (Ruder, 2016) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;], we can explicitly use SGD-style annealing with Adam. In particular, we can perform learning rate annealing with restarts: We set a learning rate and train the model until convergence. We then halve the learning rate and restart by loading the previous best model. In Adam's case, this causes the optimizer to forget its per-parameter learning rates and start fresh. Denkowski &amp;amp; Neubig (2017) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing.&lt;/p&gt;

&lt;h2 id="ensembling"&gt;Ensembling&lt;/h2&gt;

&lt;p&gt;Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance. While predicting with an ensemble is expensive at test time, recent advances in distillation allow us to compress an expensive ensemble into a much smaller model (Hinton et al., 2015; Kuncoro et al., 2016; Kim &amp;amp; Rush, 2016) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp;amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect (Huang et al., 2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;]. However, if resources are available, we prefer to ensemble multiple independently trained models to maximize model diversity.&lt;/p&gt;

&lt;h2 id="hyperparameteroptimization"&gt;Hyperparameter optimization&lt;/h2&gt;

&lt;p&gt;Rather than pre-defining or using off-the-shelf hyperparameters, simply tuning the hyperparameters of our model can yield significant improvements over baselines. Recent advances in Bayesian Optimization have made it an ideal tool for the black-box optimization of hyperparameters in neural networks (Snoek et al., 2012) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] and far more efficient than the widely used grid search. Automatic tuning of hyperparameters of an LSTM has led to state-of-the-art results in language modeling, outperforming models that are far more complex (Melis et al., 2017).&lt;/p&gt;

&lt;h2 id="lstmtricks"&gt;LSTM tricks&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Learning the initial state&lt;/strong&gt; &amp;nbsp; We generally initialize the initial LSTM states with a \(0\) vector. Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance and is also &lt;a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf"&gt;recommended by Hinton&lt;/a&gt;. Refer to &lt;a href="https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html"&gt;this blog post&lt;/a&gt; for a Tensorflow implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tying input and output embeddings&lt;/strong&gt; &amp;nbsp; Input and output embeddings account for the largest number of parameters in the LSTM model. If the LSTM predicts words as in language modelling, input and output parameters can be shared (Inan et al., 2016; Press &amp;amp; Wolf, 2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;]. This is particularly useful on small datasets that do not allow to learn a large number of parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gradient norm clipping&lt;/strong&gt; &amp;nbsp; One way to decrease the risk of exploding gradients is to clip their maximum value (Mikolov, 2012) [&lt;sup id="fnref:57"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:57" rel="footnote"&gt;57&lt;/a&gt;&lt;/sup&gt;]. This, however, does not improve performance consistently (Reimers &amp;amp; Gurevych, 2017). Rather than clipping each gradient independently, clipping the global norm of the gradient (Pascanu et al., 2013) [&lt;sup id="fnref:58"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:58" rel="footnote"&gt;58&lt;/a&gt;&lt;/sup&gt;] yields more significant improvements (a Tensorflow implementation can be found &lt;a href="https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow"&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Down-projection&lt;/strong&gt; &amp;nbsp; To reduce the number of output parameters further, the hidden state of the LSTM can be projected to a smaller size. This is useful particularly for tasks with a large number of outputs, such as language modelling (Melis et al., 2017).&lt;/p&gt;

&lt;h1 id="taskspecificbestpractices"&gt;Task-specific best practices&lt;/h1&gt;

&lt;p&gt;In the following, we will discuss task-specific best practices. Most of these perform best for a particular type of task. Some of them might still be applied to other tasks, but should be validated before. We will discuss the following tasks: classification, sequence labelling, natural language generation (NLG), and -- as a special case of NLG -- neural machine translation.&lt;/p&gt;

&lt;h2 id="classification"&gt;Classification&lt;/h2&gt;

&lt;p&gt;More so than for sequence tasks, where CNNs have only recently found application due to more efficient convolutional operations, CNNs have been popular for classification tasks in NLP. The following best practices relate to CNNs and capture some of their optimal hyperparameter choices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CNN filters&lt;/strong&gt; &amp;nbsp; Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp;amp; Wallace, 2015) [&lt;sup id="fnref:59"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:59" rel="footnote"&gt;59&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aggregation function&lt;/strong&gt; &amp;nbsp; 1-max-pooling outperforms average-pooling and \(k\)-max pooling (Zhang &amp;amp; Wallace, 2015). &lt;/p&gt;

&lt;h2 id="sequencelabelling"&gt;Sequence labelling&lt;/h2&gt;

&lt;p&gt;Sequence labelling is ubiquitous in NLP. While many of the existing best practices are with regard to a particular part of the model architecture, the following guidelines discuss choices for the model's output and prediction stage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tagging scheme&lt;/strong&gt; &amp;nbsp; For some tasks, which can assign labels to segments of texts, different tagging schemes are possible. These are: &lt;em&gt;BIO&lt;/em&gt;, which marks the first token in a segment with a &lt;em&gt;B-&lt;/em&gt; tag, all remaining tokens in the span with an &lt;em&gt;I-&lt;/em&gt;tag, and tokens outside of segments with an &lt;em&gt;O-&lt;/em&gt; tag; &lt;em&gt;IOB&lt;/em&gt;, which is similar to BIO, but only uses &lt;em&gt;B-&lt;/em&gt; if the previous token is of the same class but not part of the segment; and &lt;em&gt;IOBES&lt;/em&gt;, which in addition distinguishes between single-token entities (&lt;em&gt;S-&lt;/em&gt;) and the last token in a segment (&lt;em&gt;E-&lt;/em&gt;). Using IOBES and BIO yield similar performance (Lample et al., 2017) &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CRF output layer&lt;/strong&gt; &amp;nbsp; If there are any dependencies between outputs, such as in named entity recognition the final softmax layer can be replaced with a linear-chain conditional random field (CRF). This has been shown to yield consistent improvements for tasks that require the modelling of constraints (Huang et al., 2015; Max &amp;amp; Hovy, 2016; Lample et al., 2016) [&lt;sup id="fnref:60"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:60" rel="footnote"&gt;60&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:61"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:61" rel="footnote"&gt;61&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:62"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:62" rel="footnote"&gt;62&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Constrained decoding&lt;/strong&gt; &amp;nbsp; Rather than using a CRF output layer, constrained decoding can be used as an alternative approach to reject erroneous sequences, i.e. such that do not produce valid BIO transitions (He et al., 2017). Constrained decoding has the advantage that arbitrary constraints can be enforced this way, e.g. task-specific or syntactic constraints.&lt;/p&gt;

&lt;h2 id="naturallanguagegeneration"&gt;Natural language generation&lt;/h2&gt;

&lt;p&gt;Most of the existing best practices can be applied to natural language generation (NLG). In fact, many of the tips presented so far stem from advances in language modelling, the most prototypical NLP task. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modelling coverage&lt;/strong&gt; &amp;nbsp; Repetition is a big problem in many NLG tasks as current models do not have a good way of remembering what outputs they already produced. Modelling coverage explicitly in the model is a good way of addressing this issue. A checklist can be used if it is known in advances, which entities should be mentioned in the output, e.g. ingredients in recipes (Kiddon et al., 2016) [&lt;sup id="fnref:63"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:63" rel="footnote"&gt;63&lt;/a&gt;&lt;/sup&gt;]. If attention is used, we can keep track of a coverage vector \(\mathbf{c}_i\), which is the sum of attention distributions \(\mathbf{a}_t\) over previous time steps (Tu et al., 2016; See et al., 2017) [&lt;sup id="fnref:64"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:64" rel="footnote"&gt;64&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:65"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:65" rel="footnote"&gt;65&lt;/a&gt;&lt;/sup&gt;]:&lt;/p&gt;

&lt;p&gt;\(\mathbf{c}_i = \sum\limits^{i-1}_{t=1} \mathbf{a}_t \)&lt;/p&gt;

&lt;p&gt;This vector captures how much attention we have paid to all words in the source. We can now condition additive attention additionally on this coverage vector in order to encourage our model not to attend to the same words repeatedly:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i,\mathbf{s}_j,\mathbf{c}_i) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}_j + \mathbf{W}_3 \mathbf{c}_i )\)&lt;/p&gt;

&lt;p&gt;In addition, we can add an auxiliary loss that captures the task-specific attention behaviour that we would like to elicit: For NMT, we would like to have a roughly one-to-one alignment; we thus penalize the model if the final coverage vector is more or less than one at every index (Tu et al., 2016). For summarization, we only want to penalize the model if it repeatedly attends to the same location (See et al., 2017). &lt;/p&gt;

&lt;h2 id="neuralmachinetranslation"&gt;Neural machine translation&lt;/h2&gt;

&lt;p&gt;While neural machine translation (NMT) is an instance of NLG, NMT receives so much attention that many methods have been developed specifically for the task. Similarly, many best practices or hyperparameter choices apply exclusively to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Embedding dimensionality&lt;/strong&gt; &amp;nbsp; 2048-dimensional embeddings yield the best performance, but only do so by a small margin. Even 128-dimensional embeddings perform surprisingly well and converge almost twice as quickly (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoder and decoder depth&lt;/strong&gt; &amp;nbsp; The encoder does not need to be deeper than \(2-4\) layers. Deeper models outperform shallower ones, but more than \(4\) layers is not necessary for the decoder (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Directionality&lt;/strong&gt; &amp;nbsp; Bidirectional encoders outperform unidirectional ones by a small margin. 
Sutskever et al., (2014) [&lt;sup id="fnref:67"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:67" rel="footnote"&gt;67&lt;/a&gt;&lt;/sup&gt;] proposed to reverse the source sequence to reduce the number of long-term dependencies. Reversing the source sequence in unidirectional encoders outperforms its non-reversed counter-part (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beam search strategy&lt;/strong&gt; &amp;nbsp; Medium beam sizes around \(10\) with length normalization penalty of \(1.0\) (Wu et al., 2016) yield the best performance (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sub-word translation&lt;/strong&gt; &amp;nbsp; Senrich et al. (2016) [&lt;sup id="fnref:66"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:66" rel="footnote"&gt;66&lt;/a&gt;&lt;/sup&gt;] proposed to split words into sub-words based on a byte-pair encoding (BPE). BPE iteratively merges frequent symbol pairs, which eventually results in frequent character n-grams being merged into a single symbol, thereby effectively eliminating out-of-vocabulary-words. While it was originally meant to handle rare words, a model with sub-word units outperforms full-word systems across the board, with 32,000 being an effective vocabulary size for sub-word units (Denkowski &amp;amp; Neubig, 2017).&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I hope this post was helpful in kick-starting your learning of a new NLP task. Even if you have already been familiar with most of these, I hope that you still learnt something new or refreshed your knowledge of useful tips.&lt;/p&gt;

&lt;p&gt;I am sure that I have forgotten many best practices that deserve to be on this list. Similarly, there are many tasks such as parsing, information extraction, etc., which I do not know enough about to give recommendations. If you have a best practice that should be on this list, do let me know in the comments below. Please provide at least one reference and your handle for attribution. If this gets very collaborative, I might open a GitHub repository rather than collecting feedback here (I won't be able to accept PRs submitted directly to the generated HTML source of this article).&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Srivastava, R. K., Greff, K., &amp;amp; Schmidhuber, J. (2015). Training Very Deep Networks. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from &lt;a href="http://arxiv.org/abs/1508.06615"&gt;http://arxiv.org/abs/1508.06615&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv Preprint arXiv:1602.02410. Retrieved from &lt;a href="http://arxiv.org/abs/1602.02410"&gt;http://arxiv.org/abs/1602.02410&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Zilly, J. G., Srivastava, R. K., Koutnik, J., &amp;amp; Schmidhuber, J. (2017). Recurrent Highway Networks. In International Conference on Machine Learning (ICML 2017). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Zhang, Y., Chen, G., Yu, D., Yao, K., Kudanpur, S., &amp;amp; Glass, J. (2016). Highway Long Short-Term Memory RNNS for Distant Speech Recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In CVPR. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Huang, G., Weinberger, K. Q., &amp;amp; Maaten, L. Van Der. (2016). Densely Connected Convolutional Networks. CVPR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp;amp; Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15, 1929–1958. &lt;a href="http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf"&gt;http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Ba, J., &amp;amp; Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems. Retrieved from file:///Files/A5/A51D0755-5CEF-4772-942D-C5B8157FBE5E.pdf &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Li, Z., Gong, B., &amp;amp; Yang, T. (2016). Improved Dropout for Shallow and Deep Learning. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1602.02220"&gt;http://arxiv.org/abs/1602.02220&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Gal, Y., &amp;amp; Ghahramani, Z. (2016). A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1512.05287"&gt;http://arxiv.org/abs/1512.05287&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from &lt;a href="http://arxiv.org/abs/1408.5882"&gt;http://arxiv.org/abs/1408.5882&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Ruder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. In arXiv preprint arXiv:1706.05098. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Bahdanau, D., Cho, K., &amp;amp; Bengio, Y.. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015. &lt;a href="https://doi.org/10.1146/annurev.neuro.26.041002.131047"&gt;https://doi.org/10.1146/annurev.neuro.26.041002.131047&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Luong, M.-T., Pham, H., &amp;amp; Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1508.04025"&gt;http://arxiv.org/abs/1508.04025&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. arXiv Preprint arXiv:1706.03762. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Lin, Z., Feng, M., Santos, C. N. dos, Yu, M., Xiang, B., Zhou, B., &amp;amp; Bengio, Y. (2017). A Structured Self-Attentive Sentence Embedding. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Daniluk, M., Rockt, T., Welbl, J., &amp;amp; Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. In ICLR 2017.  &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ruder, S. (2016). An overview of gradient descent optimization. arXiv Preprint arXiv:1609.04747. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Denkowski, M., &amp;amp; Neubig, G. (2017). Stronger Baselines for Trustable Results in Neural Machine Translation. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Hinton, G., Vinyals, O., &amp;amp; Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv Preprint arXiv:1503.02531. &lt;a href="https://doi.org/10.1063/1.4931082"&gt;https://doi.org/10.1063/1.4931082&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C., &amp;amp; Smith, N. A. (2016). Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser. Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Kim, Y., &amp;amp; Rush, A. M. (2016). Sequence-Level Knowledge Distillation. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Britz, D., Goldie, A., Luong, T., &amp;amp; Le, Q. (2017). Massive Exploration of Neural Machine Translation Architectures. In arXiv preprint arXiv:1703.03906. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Zhang, X., Zhao, J., &amp;amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems, 649–657. Retrieved from &lt;a href="http://arxiv.org/abs/1509.01626"&gt;http://arxiv.org/abs/1509.01626&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Conneau, A., Schwenk, H., Barrault, L., &amp;amp; Lecun, Y. (2016). Very Deep Convolutional Networks for Natural Language Processing. arXiv Preprint arXiv:1606.01781. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01781"&gt;http://arxiv.org/abs/1606.01781&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Le, H. T., Cerisara, C., &amp;amp; Denis, A. (2017). Do Convolutional Networks need to be Deep for Text Classification ? In arXiv preprint arXiv:1707.04108. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;He, L., Lee, K., Lewis, M., &amp;amp; Zettlemoyer, L. (2017). Deep Semantic Role Labeling: What Works and What’s Next. ACL. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Melis, G., Dyer, C., &amp;amp; Blunsom, P. (2017). On the State of the Art of Evaluation in Neural Language Models. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Ramachandran, P., Liu, P. J., &amp;amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Kadlec, R., Schmid, M., Bajgar, O., &amp;amp; Kleindienst, J. (2016). Text Understanding with the Attention Sum Reader Network. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Cheng, J., Dong, L., &amp;amp; Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. arXiv Preprint arXiv:1601.06733. Retrieved from &lt;a href="http://arxiv.org/abs/1601.06733"&gt;http://arxiv.org/abs/1601.06733&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Parikh, A. P., Täckström, O., Das, D., &amp;amp; Uszkoreit, J. (2016). A Decomposable Attention Model for Natural Language Inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01933"&gt;http://arxiv.org/abs/1606.01933&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Paulus, R., Xiong, C., &amp;amp; Socher, R. (2017). A Deep Reinforced Model for Abstractive Summarization. In arXiv preprint arXiv:1705.04304,. Retrieved from &lt;a href="http://arxiv.org/abs/1705.04304"&gt;http://arxiv.org/abs/1705.04304&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Liu, Y., &amp;amp; Lapata, M. (2017). Learning Structured Text Representations. In arXiv preprint arXiv:1705.09207. Retrieved from &lt;a href="http://arxiv.org/abs/1705.09207"&gt;http://arxiv.org/abs/1705.09207&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Zhang, J., Mitliagkas, I., &amp;amp; Ré, C. (2017). YellowFin and the Art of Momentum Tuning. arXiv preprint arXiv:1706.03471. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Goldberg, Y. (2016). A Primer on Neural Network Models for Natural Language Processing. Journal of Artificial Intelligence Research, 57, 345–420. &lt;a href="https://doi.org/10.1613/jair.4992"&gt;https://doi.org/10.1613/jair.4992&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Melamud, O., McClosky, D., Patwardhan, S., &amp;amp; Bansal, M. (2016). The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of NAACL-HLT 2016 (pp. 1030–1040). Retrieved from &lt;a href="http://arxiv.org/abs/1601.00893"&gt;http://arxiv.org/abs/1601.00893&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Ruder, S., Ghaffari, P., &amp;amp; Breslin, J. G. (2016). A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 999–1005. Retrieved from &lt;a href="http://arxiv.org/abs/1609.02745"&gt;http://arxiv.org/abs/1609.02745&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Reimers, N., &amp;amp; Gurevych, I. (2017). Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks. In arXiv preprint arXiv:1707.06799: Retrieved from &lt;a href="https://arxiv.org/pdf/1707.06799.pdf"&gt;https://arxiv.org/pdf/1707.06799.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Søgaard, A., &amp;amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Liu, P., Qiu, X., &amp;amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1704.05742"&gt;http://arxiv.org/abs/1704.05742&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Dozat, T., &amp;amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01734"&gt;http://arxiv.org/abs/1611.01734&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;Jean, S., Cho, K., Memisevic, R., &amp;amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from &lt;a href="http://www.aclweb.org/anthology/P15-1001"&gt;http://www.aclweb.org/anthology/P15-1001&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh Neural Machine Translation Systems for WMT 16. In Proceedings of the First Conference on Machine Translation (WMT 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1606.02891"&gt;http://arxiv.org/abs/1606.02891&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Inan, H., Khosravi, K., &amp;amp; Socher, R. (2016). Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling. arXiv Preprint arXiv:1611.01462. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Press, O., &amp;amp; Wolf, L. (2017). Using the Output Embedding to Improve Language Models. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2, 157--163. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Snoek, J., Larochelle, H., &amp;amp; Adams, R. P. (2012). Practical Bayesian Optimization of Machine Learning Algorithms. Neural Information Processing Systems Conference (NIPS 2012). &lt;a href="https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf"&gt;https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:57"&gt;&lt;p&gt;Mikolov, T. (2012). Statistical language models based on neural networks (Doctoral dissertation, PhD thesis, Brno University of Technology). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:57" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:58"&gt;&lt;p&gt;Pascanu, R., Mikolov, T., &amp;amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International Conference on Machine Learning, (2), 1310–1318. &lt;a href="https://doi.org/10.1109/72.279181"&gt;https://doi.org/10.1109/72.279181&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:58" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:59"&gt;&lt;p&gt;Zhang, Y., &amp;amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification. arXiv Preprint arXiv:1510.03820, (1). Retrieved from &lt;a href="http://arxiv.org/abs/1510.03820"&gt;http://arxiv.org/abs/1510.03820&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:59" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:60"&gt;&lt;p&gt;Huang, Z., Xu, W., &amp;amp; Yu, K. (2015). Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv preprint arXiv:1508.01991. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:60" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:61"&gt;&lt;p&gt;Ma, X., &amp;amp; Hovy, E. (2016). End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. arXiv Preprint arXiv:1603.01354. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:61" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:62"&gt;&lt;p&gt;Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp;amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. NAACL-HLT 2016. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:62" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:63"&gt;&lt;p&gt;Kiddon, C., Zettlemoyer, L., &amp;amp; Choi, Y. (2016). Globally Coherent Text Generation with Neural Checklist Models. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 329–339. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:63" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:64"&gt;&lt;p&gt;Tu, Z., Lu, Z., Liu, Y., Liu, X., &amp;amp; Li, H. (2016). Modeling Coverage for Neural Machine Translation. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="https://doi.org/10.1145/2856767.2856776"&gt;https://doi.org/10.1145/2856767.2856776&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:64" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:65"&gt;&lt;p&gt;See, A., Liu, P. J., &amp;amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. In ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:65" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:66"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1508.07909"&gt;http://arxiv.org/abs/1508.07909&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:66" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:67"&gt;&lt;p&gt;Sutskever, I., Vinyals, O., &amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 9. Retrieved from &lt;a href="http://arxiv.org/abs/1409.3215%5Cnhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"&gt;http://arxiv.org/abs/1409.3215%5Cnhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:67" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Credit for the cover image goes to Bahdanau et al. (2015).&lt;/p&gt;</content:encoded></item></channel></rss>
