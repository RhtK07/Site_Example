<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Sebastian Ruder</title><description>I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</description><link>http://ruder.io/</link><generator>Ghost 0.7</generator><lastBuildDate>Tue, 22 May 2018 07:52:18 GMT</lastBuildDate><atom:link href="http://ruder.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>An overview of proxy-label approaches for semi-supervised learning</title><description>&lt;p&gt;Note: Parts of this post are based on my ACL 2018 paper &lt;a href="https://arxiv.org/abs/1804.09530"&gt;Strong Baselines for Neural Semi-supervised Learning under Domain Shift&lt;/a&gt; with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Plank&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selftraining"&gt;Self-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#multiviewtraining"&gt;Multi-view training&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Co-training&lt;/li&gt;
&lt;li&gt;Democratic Co-learning&lt;/li&gt;
&lt;li&gt;Tri-training&lt;/li&gt;
&lt;li&gt;Tri-training with disagreement&lt;/li&gt;
&lt;li&gt;Asymmetric tri-training&lt;/li&gt;
&lt;li&gt;Multi-task tri-training&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selfensembling"&gt;Self-ensembling&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Ladder networks&lt;/li&gt;
&lt;li&gt;Virtual Adversarial Training&lt;/li&gt;
&lt;li&gt;\(\Pi\) model&lt;/li&gt;
&lt;li&gt;Temporal&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</description><link>http://ruder.io/semi-supervised/</link><guid isPermaLink="false">da237af8-8674-4d25-af3f-7290029c6247</guid><category>deep learning</category><category>semi-supervised learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Thu, 26 Apr 2018 07:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Note: Parts of this post are based on my ACL 2018 paper &lt;a href="https://arxiv.org/abs/1804.09530"&gt;Strong Baselines for Neural Semi-supervised Learning under Domain Shift&lt;/a&gt; with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Plank&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selftraining"&gt;Self-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#multiviewtraining"&gt;Multi-view training&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Co-training&lt;/li&gt;
&lt;li&gt;Democratic Co-learning&lt;/li&gt;
&lt;li&gt;Tri-training&lt;/li&gt;
&lt;li&gt;Tri-training with disagreement&lt;/li&gt;
&lt;li&gt;Asymmetric tri-training&lt;/li&gt;
&lt;li&gt;Multi-task tri-training&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#selfensembling"&gt;Self-ensembling&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Ladder networks&lt;/li&gt;
&lt;li&gt;Virtual Adversarial Training&lt;/li&gt;
&lt;li&gt;\(\Pi\) model&lt;/li&gt;
&lt;li&gt;Temporal Ensembling&lt;/li&gt;
&lt;li&gt;Mean Teacher&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/semi-supervised/#relatedmethodsandareas"&gt;Related methods and areas&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Distillation&lt;/li&gt;
&lt;li&gt;Learning from weak supervision&lt;/li&gt;
&lt;li&gt;Learning with noisy labels&lt;/li&gt;
&lt;li&gt;Data augmentation&lt;/li&gt;
&lt;li&gt;Ensembling a single model&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unsupervised learning constitutes one of the main challenges for current machine learning models and one of the key elements that is missing for &lt;a href="http://ruder.io/highlights-nips-2016/#generalartificialintelligence"&gt;general artificial intelligence&lt;/a&gt;. While unsupervised learning on its own is still elusive, researchers have a made a lot of progress in &lt;em&gt;combining&lt;/em&gt; unsupervised learning with supervised learning. This branch of machine learning research is called semi-supervised learning.&lt;/p&gt;

&lt;p&gt;Semi-supervised learning has a long history. For a (slightly outdated) overview, refer to Zhu (2005) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] and Chapelle et al. (2006) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;]. Particularly recently, semi-supervised learning has seen some success, considerably reducing the error rate on important benchmarks. Semi-supervised learning also makes an appearance in &lt;a href="https://www.sec.gov/Archives/edgar/data/1018724/000119312518121161/d456916dex991.htm"&gt;Amazon's annual letter to shareholders&lt;/a&gt; where it is credited with reducing the amount of labelled data needed to achieve the same accuracy improvement by \(40\times\).&lt;/p&gt;

&lt;p&gt;In this blog post, I will focus on a particular class of semi-supervised learning algorithms that produce &lt;em&gt;proxy labels&lt;/em&gt; on unlabelled data, which are used as targets together with the labelled data. These proxy labels are produced by the model itself or variants of it without any additional supervision; they thus do not reflect the ground truth but might still provide some signal for learning. In a sense, these labels can be considered &lt;em&gt;noisy&lt;/em&gt; or &lt;em&gt;weak&lt;/em&gt;. I will highlight the connection to learning from noisy labels, weak supervision as well as other related topics in the end of this post.&lt;/p&gt;

&lt;p&gt;This class of models is of particular interest in my opinion, as a) deep neural networks have been shown to be good at dealing with noisy labels and b) these models have achieved state-of-the-art in semi-supervised learning for computer vision. Note that many of these ideas are not new and many related methods have been developed in the past. In one half of this post, I will thus cover classic methods and discuss their relevance for current approaches; in the other half, I will discuss techniques that have recently achieved state-of-the-art performance. Some of the following approaches have been referred to as &lt;em&gt;self-teaching&lt;/em&gt; or &lt;em&gt;bootstrapping&lt;/em&gt; algorithms; I am not aware of a term that captures all of them, so I will simply refer to them as &lt;em&gt;proxy-label&lt;/em&gt; methods.&lt;/p&gt;

&lt;p&gt;I will divide these methods in three groups, which I will discuss in the following: 1) self-training, which uses a model's own predictions as proxy labels; 2) multi-view learning, which uses the predictions of models trained with different &lt;em&gt;views&lt;/em&gt; of the data; and 3) self-ensembling, which ensembles variations of a model's own predictions and uses these as feedback for learning. I will show pseudo-code for the most important algorithms. You can find the LaTeX source &lt;a href="https://github.com/sebastianruder/semi-supervised"&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;There are many interesting and equally important directions for semi-supervised learning that I will not cover in this post, e.g. graph-convolutional neural networks [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="selftraining"&gt;Self-training&lt;/h2&gt;

&lt;p&gt;Self-training (Yarowsky, 1995; McClosky et al., 2006) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] is one of the earliest and simplest approaches to semi-supervised learning and the most straightforward example of how a model's own predictions can be incorporated into training. As the name implies, self-training leverages a model's own predictions on unlabelled data in order to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next.&lt;/p&gt;

&lt;p&gt;Formally, self-training trains a model \(m\) on a labeled training set \(L\) and an unlabeled data set \(U\). At each iteration, the model provides predictions \(m(x)\) in the form of a probability distribution over the \(C\) classes for all unlabeled examples \(x\) in \(U\). If the probability assigned to the most likely class is higher than a predetermined threshold \(\tau\), \(x\) is added to the labeled examples with \(\DeclareMathOperator*{\argmax}{argmax} p(x) = \argmax m(x)\) as pseudo-label. This process is generally repeated for a fixed number of iterations or until no more predictions on unlabelled examples are confident. This instantiation is the most widely used and shown in Algorithm 1. &lt;br&gt;
&lt;img src="http://ruder.io/content/images/2018/03/self-training.png" alt=""&gt;
Classic self-training has shown mixed success. In parsing it proved successful with small datasets (Reichart, and Rappoport, 2007; Huang and Harper, 2009) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] or when a generative component is used together with a reranker when more data is available (McClosky et al., 2006; Suzuki and Isozaki , 2008) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;]. Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], while others report limited success on a variety of NLP tasks (He and Zhou, 2011; Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;The main downside of self-training is that the model is unable to correct its own mistakes. If the model's predictions on unlabelled data are confident but wrong, the erroneous data is nevertheless incorporated into training and the model's errors are amplified. This effect is exacerbated if the domain of the unlabelled data is different from that of the labelled data; in this case, the model's confidence will be a poor predictor of its performance.&lt;/p&gt;

&lt;h2 id="multiviewtraining"&gt; Multi-view training&lt;/h2&gt;

&lt;p&gt;Multi-view training aims to train different models with different &lt;em&gt;views&lt;/em&gt; of the data. Ideally, these views complement each other and the models can collaborate in improving each other's performance. These views can differ in different ways such as in the features they use, in the architectures of the models, or in the data on which the models are trained.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Co-training&lt;/strong&gt; &amp;nbsp; Co-training (Blum and Mitchell, 1998) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] is a classic multi-view training method, which makes comparatively strong assumptions. It requires that the data \(L\) can be represented using two conditionally independent feature sets \(L^1\) and \(L^2\) and that each feature set is sufficient to train a good model. After the initial models \(m_1\) and \(m_2\) are trained on their respective feature sets, at each iteration, only inputs that are confident (i.e. have a probability higher than a threshold \(\tau\)) according to &lt;em&gt;exactly one&lt;/em&gt; of the two models are moved to the training set of &lt;em&gt;the other&lt;/em&gt; model. One model thus provides the labels to the inputs on which the &lt;em&gt;other&lt;/em&gt; model is uncertain. Co-training can be seen in Algorithm 2.
&lt;img src="http://ruder.io/content/images/2018/04/co-training.png" alt=""&gt;
In the original co-training paper (Blum and Mitchell, 1998), co-training is used to classify web pages using the text on the page as one view and the anchor text of hyperlinks on other pages pointing to the page as the other view. As two conditionally independent views are not always available, Chen et al. (2011) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] propose pseudo-multiview regularization (Chen et al., 2011) in order to split the features into two mutually exclusive views so that co-training is effective. To this end, pseudo-multiview regularization constrains the models so that at least one of them has a zero weight for each feature. This is similar to the orthogonality constraint recently used in domain adaptation to encourage shared and private spaces (Bousmalis et al., 2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. A second constraint requires the models to be confident on different subsets of \(U\). Chen et al. (2011) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] use pseudo-multiview regularization to adapt co-training to domain adaptation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Democratic Co-learning&lt;/strong&gt; Rather than treating different feature sets as views, democratic co-learning (Zhou and Goldman, 2004) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] employs models with &lt;em&gt;different inductive biases&lt;/em&gt;. These can be different network architectures in the case of neural networks or completely different learning algorithms. Democratic co-learning first trains each model separately on the complete labelled data \(L\). The models then make predictions on the unlabelled data \(U\). If a majority of models confidently agree on the label of an example, the example is added to the labelled dataset. Confidence is measured in the original formulation by measuring if the sum of the mean confidence intervals \(w\) of the models, which agreed on the label is larger than the sum of the models that disagreed. This process is repeated until no more examples are added. The final prediction is made with a majority vote weighted with the confidence intervals of the models. The full algorithm can be seen below. \(M\) is the set of all models that predict the same label \(j\) for an example \(x\). 
&lt;img src="http://ruder.io/content/images/2018/04/democratic_co-learning-2.png" alt=""&gt;
&lt;strong&gt;Tri-training&lt;/strong&gt; &amp;nbsp; Tri-training (Zhou and Li, 2005) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] is one of the best known multi-view training   methods. It can be seen as an instantiation of democratic co-learning, which leverages the agreement of three independently trained models to reduce the bias of predictions on unlabeled data. The main requirement for tri-training is that the initial models are diverse. This can be achieved using different model architectures as in democratic co-learning. The most common way to obtain diversity for tri-training, however, is to obtain different variations \(S_i\) of the original training data \(L\) using bootstrap sampling. The three models \(m_1\), \(m_2\), and \(m_3\) are then trained on these bootstrap samples, as depicted in Algorithm 4. An unlabeled data point is added to the training set of a model \(m_i\) if the other two models \(m_j\) and \(m_k\) agree on its label. Training stops when the classifiers do not change anymore.
&lt;img src="http://ruder.io/content/images/2018/04/tri-training.png" alt=""&gt;
Despite having been proposed more than 10 years ago, before the advent of Deep Learning, we found in a &lt;a href="https://arxiv.org/abs/1804.09530"&gt;recent paper&lt;/a&gt; (Ruder and Plank, 2018) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] that classic tri-training is a strong baseline for neural semi-supervised with and without domain shift for NLP and that it outperforms even recent state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tri-training with disagreement&lt;/strong&gt; &amp;nbsp; Tri-training &lt;em&gt;with disagreement&lt;/em&gt; (Søgaard, 2010) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. In order to achieve this, it adds a simple modification to the original algorithm (altering line 8 in Algorithm 2), requiring that for an unlabeled data point on which \(m_j\) and \(m_k\) &lt;em&gt;agree&lt;/em&gt;, the other model \(m_i\) &lt;em&gt;disagrees&lt;/em&gt; on the prediction. Tri-training with disagreement is more data-efficient than tri-training and has achieved competitive results on part-of-speech tagging (Søgaard, 2010).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Asymmetric tri-training&lt;/strong&gt; &amp;nbsp; Asymmetic tri-training (Saito et al., 2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] is a recently proposed extension of tri-training that achieved state-of-the-art results for unsupervised domain adaptation in computer vision. For unsupervised domain adaptation, the test data and unlabeled data are from a different domain than the labelled examples. To adapt tri-training to this shift, asymmetric tri-training learns one of the models &lt;em&gt;only&lt;/em&gt; on proxy labels and not on labelled examples (a change to line 10 in Algorithm 4) and uses only this model to classify target domain examples at test time. In addition, all three models share the same feature extractor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-task tri-training&lt;/strong&gt; &amp;nbsp; Tri-training typically relies on training separate models on bootstrap samples of a potentially large amount of training data, which is expensive. Multi-task tri-training (MT-Tri) (Ruder and Plank, 2018) aims to reduce both the time and space complexity of tri-training by leveraging insights from multi-task learning (MTL) (Caruana, 1993) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] to share knowledge across models and accelerate training. Rather than storing and training each model separately, MT-Tri shares the parameters of the models and trains them jointly using MTL. Note that the model does only &lt;em&gt;pseudo&lt;/em&gt; MTL as all three models effectively perform the same task.&lt;/p&gt;

&lt;p&gt;The output softmax layers are model-specific and are only updated for the input of the respective model. As the models leverage a joint representation, diversity is even more crucial. We need to ensure that the features used for prediction in the softmax layers of the different models are as diverse as possible, so that the models can still learn from each other's predictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training. Similar to pseudo-view regularization, we thus use an orthogonality constraint (Bousmalis et al., 2016) on two of the three softmax output layers as an additional loss term. &lt;/p&gt;

&lt;p&gt;The pseudo-code can be seen below. In contrast to classic tri-training, we can train the multi-task model with its three model-specific outputs jointly and &lt;em&gt;without&lt;/em&gt; bootstrap sampling on the labeled source domain data until convergence, as the orthogonality constraint enforces different representations between models \(m_1\) and \(m_2\). From this point, we can leverage the pair-wise agreement of two output layers to add pseudo-labeled examples as training data to the third model. We train the third output layer \(m_3\) only on pseudo-labeled target instances in order to make tri-training more robust to a domain shift. For the final prediction, we use majority voting of all three output layers. For more information about multi-task tri-training, self-training, other tri-training variants, you can refer to our recent &lt;a href="https://arxiv.org/abs/1804.09530"&gt;ACL 2018 paper&lt;/a&gt;. &lt;br&gt;
&lt;img src="http://ruder.io/content/images/2018/04/multi-task_tri-training-1.png" alt=""&gt;&lt;/p&gt;

&lt;h2 id="selfensembling"&gt; Self-ensembling&lt;/h2&gt;

&lt;p&gt;Self-ensembling methods are very similar to multi-view learning approaches in that they combine different variants of a model. Multi-task tri-training, for instance, can also be seen as a self-ensembling method where different variations of a model are used to create a stronger ensemble prediction. In contrast to multi-view learning, diversity is not a key concern. Self-ensembling approaches mostly use a single model under different configurations in order to make the model's predictions more robust. Most of the following methods are very recent and several have achieved state-of-the-art results in computer vision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ladder networks&lt;/strong&gt; &amp;nbsp; The \(\Gamma\) (gamma) version of Ladder Networks (Rasmus et al., 2015) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] aims to make a model more robust to noise. For each unlabelled example, it uses the model's prediction on the clean example as a proxy label for prediction on a perturbed version of the example. This way, the model learns to develop features that are invariant to noise and predictive of the labels on the labelled training data. Ladder networks have been mostly used in computer vision where many forms of perturbation and data augmentation are available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virtual Adversarial Training&lt;/strong&gt; &amp;nbsp; If perturbing the original sample is not possible or desired, we can instead perturb the example in feature space. Rather than randomly perturbing it by e.g. adding dropout, we can apply the &lt;em&gt;worst possible&lt;/em&gt; perturbation for the model, which transforms the input into an adversarial sample. While adversarial training requires access to the labels to perform these perturbations, &lt;em&gt;virtual&lt;/em&gt; adversarial training (Miyato et al., 2017) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] requires no labels and is thus suitable for semi-supervised learning. Virtual adversarial training effectively seeks to make the model robust to perturbations in directions to which it is most sensitive and has achieved good results on text classification datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;\(\Pi\) model&lt;/strong&gt; &amp;nbsp; Rather than treating clean predictions as proxy labels, the \(\Pi\) (pi) model (Laine and Aila, 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;] ensembles the predictions of the model under two different perturbations of the input data and two different dropout conditions \(z\) and \(\tilde{z}\). The full pseudo-code can be seen in Algorithm 6 below. \(g(x)\) is the stochastic input augmentation function. The first loss term encourages the predictions under the two different noise settings to be consistent, with \(\lambda\) determining the contribution, while the second loss term is the standard cross-entropy loss \(H\) with respect to the label \(y\). In contrast to the models we encountered before, we apply the unsupervised loss component to both unlabelled and labelled examples.
&lt;img src="http://ruder.io/content/images/2018/04/pi-model-3.png" alt=""&gt;
&lt;strong&gt;Temporal Ensembling&lt;/strong&gt; &amp;nbsp; Instead of ensembling over the same model under different noise configurations, we can ensemble over different models. As training separate models is expensive, we can instead ensemble the predictions of a model &lt;em&gt;at different timesteps&lt;/em&gt;. We can save the ensembled proxy labels \(Z\) as an exponential moving average of the model's past predictions on all examples as depicted below in order to save space. As we initialize the proxy labels as a zero vector, they are biased towards \(0\). We can correct this bias similar to Adam (Kingma and Ba, 2015) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;] based on the current epoch \(t\) to obtain bias-corrected target vectors \(\tilde{z}\). We then update the model similar to the \(\Pi\) model. 
&lt;img src="http://ruder.io/content/images/2018/04/temporal_ensembling-3.png" alt=""&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Teacher&lt;/strong&gt; &amp;nbsp; Finally, instead of averaging the &lt;em&gt;predictions&lt;/em&gt; of our model over training time, we can average the model weights. Mean teacher (Tarvainen and Valpola, 2017) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] stores an exponential moving average of the model parameters. For every example, this mean teacher model is then used to obtain proxy labels \(\tilde{z}\). The consistency loss and supervised loss are computed as in temporal ensembling.&lt;/p&gt;

&lt;p&gt;Mean teacher has achieved state-of-the-art results for semi-supervised learning for computer vision. For reference, on ImageNet with 10% of the labels, it achieves an error rate of \(9.11\), compared to an error rate of \(3.79\) using &lt;em&gt;all&lt;/em&gt; labels with the state-of-the-art. For more information about self-ensembling methods, have a look at &lt;a href="https://thecuriousaicompany.com/mean-teacher/"&gt;this intuitive blog post&lt;/a&gt; by the Curious AI company. We have run experiments with temporal ensembling for NLP tasks, but did not manage to obtain consistent results. My assumption is that the unsupervised consistency loss is more suitable for continuous inputs. Mean teacher might work better, as averaging weights aka Polyak averaging (Polyak and Juditsky, 1992) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] is a tried method for accelerating optimization.&lt;/p&gt;

&lt;p&gt;Very recently, Oliver et al. (2018) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] raise some questions regarding the true applicability of these methods: They find that the performance difference to a properly tuned supervised baseline is smaller than typically reported, that transfer learning from a labelled dataset (e.g. ImageNet) outperforms the presented methods, and that performance degrades severely under a domain shift. In order to deal with the latter, algorithms such as asymmetric or multi-task tri-training learn different representations for the target distribution. It remains to be seen if these insights translate to other domains; a combination of transfer learning and semi-supervised adaptation to the target domain seems particularly promising.&lt;/p&gt;

&lt;h2 id="relatedmethodsandareas"&gt; Related methods and areas&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Distillation&lt;/strong&gt; &amp;nbsp; Proxy-label approaches can be seen as different forms of distillation (Hinton et al., 2015) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;]. Distillation was originally conceived as a method to compress the information of a large model or an ensemble in a smaller model. In the standard setup, a typically large and fully trained &lt;em&gt;teacher&lt;/em&gt; model provides proxy targets for a &lt;em&gt;student&lt;/em&gt; model, which is generally smaller and faster. Self-learning is akin to distillation without a teacher, where the student is left to learn by themselves and with no-one to correct its mistakes. For multi-view learning, different models work together to teach each other, alternately acting as both teachers and students. Self-ensembling, finally, has one model assuming the dual role of teacher and student: As a teacher, it generates new targets, which are then incorporated by itself as a student for learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning from weak supervision&lt;/strong&gt; &amp;nbsp; Learning from weak supervision, as the name implies, can be seen as a weaker form of supervised learning or alternatively as a stronger form of semi-supervised learning: While supervised learning provides us with labels that we know to be correct and semi-supervised learning only provides us with a small set of labelled examples, weak supervision allows us to obtain labels that we know to be noisy for the unlabelled data as a further signal for learning. Typically, the weak annotator is an unsupervised method that is very different from the model we use for learning the task. For sentiment analysis, this could be a simple lexicon-based method [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]. Many of the presented methods could be extended to the weak supervision setting by incorporating the weak labels as feedback. Self-ensembling methods, for instance, might employ another teacher model that gauges the quality of weakly annotated examples similar to Deghani et al. (2018) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning with noisy labels&lt;/strong&gt; &amp;nbsp; Learning with noisy labels is similar to learning from weak supervision. In both cases, labels are available that cannot be completely trusted. For learning with noisy labels, labels are typically assumed to be permuted with a fixed random permutation. While proxy-label approaches supply the noisy labels themselves, when learning with noisy labels, the labels are part of the data. Similar to learning from weak supervision, we can try to model the noise to assess the quality of the labels (Sukhbaatar et al., 2015) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. Similar to self-ensembling methods, we can enforce consistency between the model's preditions and the proxy labels (Reed et al., 2015) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt; &amp;nbsp; Several self-ensembling methods employ data augmentation to enforce consistency between model predictions under different noise settings. Data augmentation is mostly used in computer vision, but  noise in the form of different dropout masks can also be applied to the model parameters as in the \(\Pi\) model and has also been used in LSTMs (Zolna et al., 2018) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;]. While regularization in the form of dropout, batch normalization, etc. can be used when labels are available in order to make predictions more robust, a consistency loss is required in the case without labels. For supervised learning, adversarial training can be employed to obtain adversarial examples and has been used successfully e.g. for part-of-speech tagging (Yasunaga et al., 2018) [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ensembling a single model&lt;/strong&gt; &amp;nbsp; The discussed self-ensembling methods all employ ensemble predictions not just to make predictions more robust, but as feedback to improve the model itself during training in a self-reinforcing loop. In the supervised setting, this feedback might not be necessary; ensembling a single model is still useful, however, to save time compared to training multiple models. Two methods that have been proposed to ensemble a model from a single training run are checkpoint ensembles and snapshot ensembles. Checkpoint ensembles (Sennrich et al., 2016) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] ensemble the last \(n\) checkpoints of a single training run and have been used to achieve state-of-the-art in machine translation. Snapshot ensembles (Huang et al., 2017) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/semi-supervised/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] ensemble models converged to different minima during a training run and have been used to achieve state-of-the-art in object recognition. &lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this post was able to give you an insight into a part of the semi-supervised learning landscape that seems to be particularly useful to improve the performance of current models. While learning completely without labelled data is unrealistic at this point, semi-supervised learning enables us to augment our small labelled datasets with large amounts of available unlabelled data. Most of the discussed methods are promising in that they treat the model as a black box and can thus be used with any existing supervised learning model. As always, if you have any questions or noticed any mistakes, feel free to write a comment in the comments section below.&lt;/p&gt;

&lt;h2 id="references"&gt; References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Zhu, X. (2005). Semi-Supervised Learning Literature Survey. &lt;a href="http://ruder.io/semi-supervised/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Chapelle, O., Schölkopf, B., &amp;amp; Zien, A. (2006). Semi-Supervised Learning. Interdisciplinary sciences computational life sciences (Vol. 1). &lt;a href="http://doi.org/10.1007/s12539-009-0016-2"&gt;http://doi.org/10.1007/s12539-009-0016-2&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Kipf, T. N., &amp;amp; Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics (pp. 189-196). Association for Computational Linguistics. &lt;a href="http://ruder.io/semi-supervised/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;McClosky, D., Charniak, E., &amp;amp; Johnson, M. (2006). Effective self-training for parsing. Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, 152–159. &lt;a href="http://ruder.io/semi-supervised/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Reichart, R., &amp;amp; Rappoport, A. (2007). Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (pp. 616-623) &lt;a href="http://ruder.io/semi-supervised/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Suzuki, J., &amp;amp; Isozaki, H. (2008). Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. Proceedings of ACL-08: HLT, 665-673. &lt;a href="http://ruder.io/semi-supervised/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Petrov, S., &amp;amp; McDonald, R. (2012). Overview of the 2012 shared task on parsing the web. In Notes of the first workshop on syntactic analysis of non-canonical language (sancl) (Vol. 59). &lt;a href="http://ruder.io/semi-supervised/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;He, Y., &amp;amp; Zhou, D. (2011). Self-training from labeled features for sentiment analysis. Information Processing &amp;amp; Management, 47(4), 606-616. &lt;a href="http://ruder.io/semi-supervised/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Plank, B. (2011). Domain adaptation for parsing. University Library Groniongen][Host]. &lt;a href="http://ruder.io/semi-supervised/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Van Asch, V., &amp;amp; Daelemans, W. (2016). Predicting the Effectiveness of Self-Training: Application to Sentiment Classification. arXiv preprint arXiv:1601.03288. &lt;a href="http://ruder.io/semi-supervised/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;van der Goot, R., Plank, B., &amp;amp; Nissim, M. (2017). To normalize, or not to normalize: The impact of normalization on part-of-speech tagging. arXiv preprint arXiv:1707.05116. &lt;a href="http://ruder.io/semi-supervised/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Huang, Z., &amp;amp; Harper, M. (2009). Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2 (pp. 832-841). Association for Computational Linguistics. &lt;a href="http://ruder.io/semi-supervised/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Zhou, Z.-H., &amp;amp; Li, M. (2005). Tri-Training: Exploiting Unlabled Data Using Three Classifiers. IEEE Trans.Data Eng., 17(11), 1529–1541. &lt;a href="http://doi.org/10.1109/TKDE.2005.186"&gt;http://doi.org/10.1109/TKDE.2005.186&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Søgaard, A. (2010). Simple semi-supervised training of part-of-speech taggers. Proceedings of the ACL 2010 Conference Short Papers. &lt;a href="http://ruder.io/semi-supervised/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Saito, K., Ushiku, Y., &amp;amp; Harada, T. (2017). Asymmetric Tri-training for Unsupervised Domain Adaptation. In ICML 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08400"&gt;http://arxiv.org/abs/1702.08400&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Blum, A., &amp;amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory (pp. 92-100). ACM. &lt;a href="http://ruder.io/semi-supervised/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Chen, M., Weinberger, K. Q., &amp;amp; Blitzer, J. C. (2011). Co-Training for Domain Adaptation. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/semi-supervised/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Chen, M., Weinberger, K. Q., &amp;amp; Chen, Y. (2011). Automatic Feature Decomposition for Single View Co-training. Proceedings of the 28th International Conference on Machine Learning (ICML-11), 953–960. &lt;a href="http://ruder.io/semi-supervised/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., &amp;amp; Erhan, D. (2016). Domain Separation Networks. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/semi-supervised/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Zhou, Y., &amp;amp; Goldman, S. (2004). Democratic Co-Learning. In 16th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2004. &lt;a href="http://ruder.io/semi-supervised/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ruder, S., &amp;amp; Plank, B. (2018). Strong Baselines for Neural Semi-supervised Learning under Domain Shift. In Proceedings of ACL 2018. &lt;a href="http://ruder.io/semi-supervised/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Caruana, R. (1993). Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning. &lt;a href="http://ruder.io/semi-supervised/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Rasmus, A., Valpola, H., Honkala, M., Berglund, M., &amp;amp; Raiko, T. (2015). Semi-Supervised Learning with Ladder Network. arXiv Preprint arXiv:1507.02672. Retrieved from &lt;a href="http://arxiv.org/abs/1507.02672"&gt;http://arxiv.org/abs/1507.02672&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Miyato, T., Dai, A. M., &amp;amp; Goodfellow, I. (2017). Adversarial Training Methods for Semi-supervised Text Classification. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Yasunaga, M., Kasai, J., &amp;amp; Radev, D. (2018). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.04903"&gt;http://arxiv.org/abs/1711.04903&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Laine, S., &amp;amp; Aila, T. (2017). Temporal Ensembling for Semi-Supervised Learning. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/semi-supervised/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Tarvainen, A., &amp;amp; Valpola, H. (2017). Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1703.01780"&gt;http://arxiv.org/abs/1703.01780&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Hinton, G., Vinyals, O., &amp;amp; Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv Preprint arXiv:1503.02531. &lt;a href="https://doi.org/10.1063/1.4931082"&gt;https://doi.org/10.1063/1.4931082&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh neural machine translation systems for WMT 16. arXiv preprint arXiv:1606.02891. &lt;a href="http://ruder.io/semi-supervised/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/semi-supervised/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Polyak, B. T., &amp;amp; Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4), 838-855. &lt;a href="http://ruder.io/semi-supervised/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Dehghani, M., Mehrjou, A., Gouws, S., Kamps, J., &amp;amp; Schölkopf, B. (2018). Fidelity-Weighted Learning. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.02799"&gt;http://arxiv.org/abs/1711.02799&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Kiritchenko, S., Zhu, X., &amp;amp; Mohammad, S. M. (2014). Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, 50, 723-762. &lt;a href="http://ruder.io/semi-supervised/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., &amp;amp; Rabinovich, A. (2015). Training Deep Neural Networks on Noisy Labels with Bootstrapping. ICLR 2015 Workshop Track. Retrieved from &lt;a href="http://arxiv.org/abs/1412.6596"&gt;http://arxiv.org/abs/1412.6596&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., &amp;amp; Fergus, R. (2015). Training Convolutional Networks with Noisy Labels. Workshop Track - ICLR 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1406.2080"&gt;http://arxiv.org/abs/1406.2080&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Zolna, K., Arpit, D., Suhubdy, D., &amp;amp; Bengio, Y. (2018). Fraternal Dropout. In Proceedings of ICLR 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.00066"&gt;http://arxiv.org/abs/1711.00066&lt;/a&gt; &lt;a href="http://ruder.io/semi-supervised/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Oliver, A., Odena, A., Raffel, C., Cubuk, E. D., &amp;amp; Goodfellow, I. J. (2018). Realistic Evaluation of Semi-Supervised Learning Algorithms. arXiv preprint arXiv:1804.09170. &lt;a href="http://ruder.io/semi-supervised/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Text Classification with TensorFlow Estimators</title><description>This post is a tutorial on how to use Estimators in TensorFlow to classify text.</description><link>http://ruder.io/text-classification-tensorflow-estimators/</link><guid isPermaLink="false">4dd8b7a6-a266-4ddb-b6b2-b3444bcc48b3</guid><category>nlp</category><category>tensorflow</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 16 Apr 2018 13:24:29 GMT</pubDate><content:encoded>&lt;p&gt;&lt;head&gt; &lt;br&gt;
  &lt;meta charset="utf-8"&gt;
  &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
  &lt;title&gt;nlp&lt;em&gt;estimators&lt;/em&gt;blog.md&lt;/title&gt;
  &lt;link rel="stylesheet" href="https://stackedit.io/style.css"&gt;
&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This post was written together with the awesome &lt;a href="https://twitter.com/eisenjulian"&gt;Julian Eisenschlos&lt;/a&gt; and was originally published on the &lt;a href="https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe"&gt;TensorFlow blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hello there! Throughout this post we will show you how to classify text using Estimators in TensorFlow. Here’s the outline of what we’ll cover:&lt;/p&gt;

&lt;ul&gt;  
&lt;li&gt;Loading data using Datasets.&lt;/li&gt;  
&lt;li&gt;Building baselines using pre-canned estimators.&lt;/li&gt;  
&lt;li&gt;Using word embeddings.&lt;/li&gt;  
&lt;li&gt;Building custom estimators with convolution and LSTM layers.&lt;/li&gt;  
&lt;li&gt;Loading pre-trained word vectors.&lt;/li&gt;  
&lt;li&gt;Evaluating and comparing models using TensorBoard.&lt;/li&gt;  
&lt;/ul&gt;  

&lt;hr&gt;  

&lt;p&gt;Welcome to Part 4 of a blog series that introduces TensorFlow Datasets and Estimators. You don’t need to read all of the previous material, but take a look if you want to refresh any of the following concepts. &lt;a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html"&gt;Part 1&lt;/a&gt; focused on pre-made Estimators, &lt;a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"&gt;Part 2&lt;/a&gt; discussed feature columns, and &lt;a href="https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html"&gt;Part 3&lt;/a&gt; how to create custom Estimators.&lt;/p&gt;  

&lt;p&gt;Here in Part 4, we will build on top of all the above to tackle a different family of problems in Natural Language Processing (NLP). In particular, this article demonstrates how to solve a text classification task using custom TensorFlow estimators, embeddings, and the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/layers"&gt;tf.layers&lt;/a&gt; module. Along the way, we’ll learn about word2vec and transfer learning as a technique to bootstrap model performance when labeled data is a scarce resource.&lt;/p&gt;  

&lt;p&gt;We will show you relevant code snippets. &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb"&gt;Here&lt;/a&gt;’s the complete Jupyter Notebook  that you can run locally or on &lt;a href="https://colab.research.google.com/drive/1oXjNYSJ3VsRvAsXN4ClmtsVEgPW_CX_c?hl=en#forceEdit=true&amp;offline=true&amp;sandboxMode=true"&gt;Google Colaboratory&lt;/a&gt;. The plain &lt;code&gt;.py&lt;/code&gt;  source file is also available &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py"&gt;here&lt;/a&gt;. Note that the code was written to demonstrate how Estimators work functionally and was not optimized for maximum performance.&lt;/p&gt;  

&lt;h3 id="the-task"&gt;The task&lt;/h3&gt;  

&lt;p&gt;The dataset we will be using is the IMDB &lt;a href="http://ai.stanford.edu/~amaas/data/sentiment/"&gt;Large Movie Review Dataset&lt;/a&gt;, which consists of &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; highly polar movie reviews for training, and &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; for testing. We will use this dataset to train a binary classification model, able to predict whether a review is positive or negative.&lt;/p&gt;  

&lt;p&gt;For illustration, here’s a piece of a negative review (with &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; stars) in the dataset:&lt;/p&gt;  

&lt;blockquote&gt;  
&lt;p&gt;Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film.&lt;/p&gt;  
&lt;/blockquote&gt;  

&lt;p&gt;&lt;em&gt;Keras&lt;/em&gt; provides a convenient handler for importing the dataset which is also available as a serialized numpy array &lt;code&gt;.npz&lt;/code&gt; file to download &lt;a href="https://s3.amazonaws.com/text-datasets/imdb.npz"&gt;here&lt;/a&gt;. For text classification, it is standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. For this reason, each review consists of a series of word indexes that go from &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;4&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (the most frequent word in the dataset &lt;strong&gt;the&lt;/strong&gt;) to &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;4999&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;4&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;span class="mord mathrm"&gt;9&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which corresponds to &lt;strong&gt;orange&lt;/strong&gt;. Index &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;1&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; represents the beginning of the sentence and the index &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is assigned to all unknown (also known as &lt;em&gt;out-of-vocabulary&lt;/em&gt; or &lt;em&gt;OOV&lt;/em&gt;) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.&lt;/p&gt;  

&lt;p&gt;After we’ve loaded the data in memory we pad each of the sentences with &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to a fixed size (here: &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;) so that we have two &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;2&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-dimensional &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25000\times200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.72777em; vertical-align: -0.08333em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mbin"&gt;×&lt;/span&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; arrays for training and testing respectively.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;vocab_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;5000&lt;/span&gt;  
sentence_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;200&lt;/span&gt;  
&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_train&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;(&lt;/span&gt;x_test_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_test&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; imdb&lt;span class="token punctuation"&gt;.&lt;/span&gt;load_data&lt;span class="token punctuation"&gt;(&lt;/span&gt;num_words&lt;span class="token operator"&gt;=&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
x_train &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    x_train_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
x_test &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    x_test_variable&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;h3 id="input-functions"&gt;Input Functions&lt;/h3&gt;  

&lt;p&gt;The Estimator framework uses &lt;em&gt;input functions&lt;/em&gt; to split the data pipeline from the model itself. Several helper methods are available to create them, whether your data is in a &lt;code&gt;.csv&lt;/code&gt; file, or in a &lt;code&gt;pandas.DataFrame&lt;/code&gt;, whether it fits in memory or not. In our case, we can use &lt;code&gt;Dataset.from_tensor_slices&lt;/code&gt; for both the train and test sets.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;x_len_train &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; x_train_variable&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
x\_len\_test &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; x_test_variable&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;parser&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;,&lt;/span&gt; length&lt;span class="token punctuation"&gt;,&lt;/span&gt; y&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    features &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;"x"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; x&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;"len"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; length&lt;span class="token punctuation"&gt;}&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; features&lt;span class="token punctuation"&gt;,&lt;/span&gt; y

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;train_input_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;data&lt;span class="token punctuation"&gt;.&lt;/span&gt;Dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;from_tensor_slices&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train&lt;span class="token punctuation"&gt;,&lt;/span&gt; x_len_train&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_train&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;shuffle&lt;span class="token punctuation"&gt;(&lt;/span&gt;buffer_size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_train_variable&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;batch&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;&lt;span class="token builtin"&gt;map&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;parser&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;repeat&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    iterator &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;make_one_shot_iterator&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; iterator&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_next&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;eval_input_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;data&lt;span class="token punctuation"&gt;.&lt;/span&gt;Dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;from_tensor_slices&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x_test&lt;span class="token punctuation"&gt;,&lt;/span&gt; x_len_test&lt;span class="token punctuation"&gt;,&lt;/span&gt; y_test&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;batch&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    dataset &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;&lt;span class="token builtin"&gt;map&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;parser&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    iterator &lt;span class="token operator"&gt;=&lt;/span&gt; dataset&lt;span class="token punctuation"&gt;.&lt;/span&gt;make_one_shot_iterator&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; iterator&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_next&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional &lt;code&gt;"len"&lt;/code&gt; key that captures the length of the original, unpadded sequence, which we will use later.&lt;/p&gt;  

&lt;h3 id="building-a-baseline"&gt;Building a baseline&lt;/h3&gt;  

&lt;p&gt;It’s good practice to start any machine learning project trying basic baselines. The simpler the better as having a simple and robust baseline is key to understanding exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.&lt;/p&gt;  

&lt;p&gt;With that in mind, let us start by trying out one of the simplest models for text classification. That would be a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. As this model does not care about the order of words in a sentence, we normally refer to it as a &lt;em&gt;Bag-of-Words&lt;/em&gt; approach. Let’s see how we can implement this model using an &lt;code&gt;Estimator&lt;/code&gt;.&lt;/p&gt;  

&lt;p&gt;We start out by defining the feature column that is used as input to our classifier. As we have seen in &lt;a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"&gt;Part 2&lt;/a&gt;, &lt;code&gt;categorical_column_with_identity&lt;/code&gt; is the right choice for this pre-processed text input. If we were feeding raw text tokens other &lt;code&gt;feature_columns&lt;/code&gt; could do a lot of the pre-processing for us. We can now use the pre-made &lt;code&gt;LinearClassifier&lt;/code&gt;.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;column &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;feature_column&lt;span class="token punctuation"&gt;.&lt;/span&gt;categorical_column_with_identity&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'x'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; vocab_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;LinearClassifier&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    feature_columns&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;column&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'bow_sparse'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;Finally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. As we do not aim to maximize performance in this blog post, we only train our models for &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;25,000&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.83888em; vertical-align: -0.19444em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mpunct"&gt;,&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; steps.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;train_and_evaluate&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;train_input_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt; steps&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;25000&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    eval_results &lt;span class="token operator"&gt;=&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;eval_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predictions &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;p&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'logistic'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; p &lt;span class="token keyword"&gt;in&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;predict&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;eval_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;reset_default_graph&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; 
    &lt;span class="token comment"&gt;# Add a PR summary in addition to the summaries that the classifier writes&lt;/span&gt;
    pr &lt;span class="token operator"&gt;=&lt;/span&gt; summary_lib&lt;span class="token punctuation"&gt;.&lt;/span&gt;pr_curve&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'precision_recall'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; predictions&lt;span class="token operator"&gt;=&lt;/span&gt;predictions&lt;span class="token punctuation"&gt;,&lt;/span&gt; labels&lt;span class="token operator"&gt;=&lt;/span&gt;y_test&lt;span class="token punctuation"&gt;.&lt;/span&gt;astype&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;bool&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; num_thresholds&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;21&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;with&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;Session&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;as&lt;/span&gt; sess&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        writer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;summary&lt;span class="token punctuation"&gt;.&lt;/span&gt;FileWriter&lt;span class="token punctuation"&gt;(&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'eval'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sess&lt;span class="token punctuation"&gt;.&lt;/span&gt;graph&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        writer&lt;span class="token punctuation"&gt;.&lt;/span&gt;add_summary&lt;span class="token punctuation"&gt;(&lt;/span&gt;sess&lt;span class="token punctuation"&gt;.&lt;/span&gt;run&lt;span class="token punctuation"&gt;(&lt;/span&gt;pr&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; global_step&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        writer&lt;span class="token punctuation"&gt;.&lt;/span&gt;close&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;One of the benefits of choosing a simple model is that it is much more interpretable. The more complex a model, the harder it is to inspect and the more it tends to work like a black box. In this example, we can load the weights from our model’s last checkpoint and take a look at what tokens correspond to the  biggest weights in absolute value. The results look like what we would expect.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token comment"&gt;# Load the tensor with the model weights&lt;/span&gt;  
weights &lt;span class="token operator"&gt;=&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_variable_value&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'linear/linear_model/x/weights'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;flatten&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token comment"&gt;# Find biggest weights in absolute value&lt;/span&gt;  
extremes &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;concatenate&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sorted_indexes&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;8&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sorted_indexes&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token number"&gt;8&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token comment"&gt;# word_inverted_index is a dictionary that maps from indexes back to tokens&lt;/span&gt;  
extreme_weights &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token builtin"&gt;sorted&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    &lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;weights&lt;span class="token punctuation"&gt;[&lt;/span&gt;i&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; word_inverted_index&lt;span class="token punctuation"&gt;[&lt;/span&gt;i &lt;span class="token operator"&gt;-&lt;/span&gt; index_offset&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; i &lt;span class="token keyword"&gt;in&lt;/span&gt; extremes&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;span class="token comment"&gt;# Create plot&lt;/span&gt;  
y_pos &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;arange&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;extreme_weights&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;bar&lt;span class="token punctuation"&gt;(&lt;/span&gt;y_pos&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;pair&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; pair &lt;span class="token keyword"&gt;in&lt;/span&gt; extreme_weights&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; align&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'center'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; alpha&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.5&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;xticks&lt;span class="token punctuation"&gt;(&lt;/span&gt;y_pos&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;pair&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; pair &lt;span class="token keyword"&gt;in&lt;/span&gt; extreme_weights&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; rotation&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;45&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; ha&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'right'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;ylabel&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'Weight'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;title&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'Most significant tokens'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
plt&lt;span class="token punctuation"&gt;.&lt;/span&gt;show&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/token_weights.png" alt="Model weights" style="width: 60%"&gt;&lt;/p&gt;  

&lt;p&gt;As we can see, tokens with the most positive weight such as ‘refreshing’ are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf&lt;/a&gt; scores.&lt;/p&gt;  

&lt;h3 id="embeddings"&gt;Embeddings&lt;/h3&gt;  

&lt;p&gt;The next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space—when learned from a large enough corpus—has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an &lt;code&gt;embedding_column&lt;/code&gt;. The representation seen by the model is the mean of the embeddings for each token (see the &lt;code&gt;combiner&lt;/code&gt; argument in the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column"&gt;docs&lt;/a&gt;). We can plug in the embedded features into a pre-canned &lt;code&gt;DNNClassifier&lt;/code&gt;.&lt;/p&gt;  

&lt;p&gt;A note for the keen observer: an &lt;code&gt;embedding_column&lt;/code&gt; is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn’t make sense to use an &lt;code&gt;embedding_column&lt;/code&gt; directly in a &lt;code&gt;LinearClassifier&lt;/code&gt; because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.&lt;/p&gt;  

&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embedding_size &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token number"&gt;50&lt;/span&gt;  
word\_embedding\_column &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;feature_column&lt;span class="token punctuation"&gt;.&lt;/span&gt;embedding_column&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    column&lt;span class="token punctuation"&gt;,&lt;/span&gt; dimension&lt;span class="token operator"&gt;=&lt;/span&gt;embedding_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;
classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;DNNClassifier&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    hidden_units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    feature_columns&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;word_embedding_column&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'bow_embeddings'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  

&lt;p&gt;We can use TensorBoard to visualize our &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;50&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;5&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-dimensional word vectors projected into &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;\mathbb{R}^3&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.814108em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.814108em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.814108em;"&gt;&lt;span class="" style="top: -3.063em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathrm mtight"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; using &lt;a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"&gt;t-SNE&lt;/a&gt;. We expect similar words to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviours.&lt;/p&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/embeddings.gif" alt="embeddings"&gt;&lt;/p&gt;  

&lt;h3 id="convolutions"&gt;Convolutions&lt;/h3&gt;  

&lt;p&gt;At this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.&lt;/p&gt;  

&lt;p&gt;Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for &lt;a href="https://www.tensorflow.org/tutorials/layers"&gt;image classification&lt;/a&gt;. The intuition is that certain sequences of words, or &lt;em&gt;n-grams&lt;/em&gt;, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.&lt;/p&gt;  

&lt;p&gt;The following image shows how a filter matrix &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant="double-struck"&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;F \in \mathbb{R}^{d\times m}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.849108em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.888208em; vertical-align: -0.0391em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathit" style="margin-right: 0.13889em;"&gt;F&lt;/span&gt;&lt;span class="mrel"&gt;∈&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathbb"&gt;R&lt;/span&gt;&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.849108em;"&gt;&lt;span class="" style="top: -3.063em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathit mtight"&gt;d&lt;/span&gt;&lt;span class="mbin mtight"&gt;×&lt;/span&gt;&lt;span class="mord mathit mtight"&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; slides across each &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;3&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-gram window of tokens to build a new feature map. Afterwards a &lt;em&gt;pooling&lt;/em&gt; layer is usually applied to combine adjacent results.&lt;/p&gt;  

&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/conv.png" alt="text convolution" style="width: 80%"&gt;&lt;br&gt;  
&lt;small&gt;&lt;/small&gt;&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;  
Source: &lt;a href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9"&gt;Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks&lt;/a&gt; by &lt;strong&gt;Severyn&lt;/strong&gt; et al. [2015]&lt;/p&gt;

&lt;p&gt;Let us look at the full model architecture. The use of dropout layers is a regularization technique that makes the model less likely to overfit.&lt;/p&gt;  

&lt;div class="mermaid"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" id="mermaid-svg-qmCUPkCnZ5TFF69q" height="100%" viewbox="0 0 1415.640625 112.5" style="max-width:1415.640625px;"&gt;&lt;g&gt;&lt;g class="output"&gt;&lt;g class="clusters"/&gt;&lt;g class="edgePaths"&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M191.4375,46.25L216.4375,46.25L241.4375,46.25" marker-end="url(#arrowhead84)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead84" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M335.515625,46.25L360.515625,46.25L385.515625,46.25" marker-end="url(#arrowhead85)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead85" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M539.84375,46.25L564.84375,46.25L589.84375,46.25" marker-end="url(#arrowhead86)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead86" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M796.28125,46.25L821.28125,46.25L846.28125,46.25" marker-end="url(#arrowhead87)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead87" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M1044.65625,46.25L1069.65625,46.25L1094.65625,46.25" marker-end="url(#arrowhead88)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead88" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M1188.734375,46.25L1213.734375,46.25L1238.734375,46.25" marker-end="url(#arrowhead89)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead89" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabels"&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="nodes"&gt;&lt;g class="node" style="opacity: 1;" id="id1" transform="translate(105.71875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-85.71875" y="-26.25" width="171.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-75.71875,-16.25)"&gt;&lt;foreignobject width="151.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Embedding Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id2" transform="translate(288.4765625,46.25)"&gt;&lt;rect rx="5" ry="5" x="-47.0390625" y="-26.25" width="94.078125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-37.0390625,-16.25)"&gt;&lt;foreignobject width="74.08203125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Dropout&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id3" transform="translate(462.6796875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-77.1640625" y="-26.25" width="154.328125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-67.1640625,-16.25)"&gt;&lt;foreignobject width="134.3359375" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Convolution1D&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id4" transform="translate(693.0625,46.25)"&gt;&lt;rect rx="5" ry="5" x="-103.21875" y="-26.25" width="206.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-93.21875,-16.25)"&gt;&lt;foreignobject width="186.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;GlobalMaxPooling1D&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id5" transform="translate(945.46875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-99.1875" y="-26.25" width="198.375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-89.1875,-16.25)"&gt;&lt;foreignobject width="178.37890625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Hidden Dense Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id6" transform="translate(1141.6953125,46.25)"&gt;&lt;rect rx="5" ry="5" x="-47.0390625" y="-26.25" width="94.078125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-37.0390625,-16.25)"&gt;&lt;foreignobject width="74.08203125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Dropout&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id7" transform="translate(1307.1875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-68.453125" y="-26.25" width="136.90625" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-58.453125,-16.25)"&gt;&lt;foreignobject width="116.9140625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Output Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;  
&lt;h3 id="creating-a-custom-estimator"&gt;Creating a custom estimator&lt;/h3&gt;  
&lt;p&gt;As seen in previous blog posts, the &lt;code&gt;tf.estimator&lt;/code&gt; framework provides a high-level API for training machine learning models, defining &lt;code&gt;train()&lt;/code&gt;, &lt;code&gt;evaluate()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. There is a small family of pre-made estimators, like the ones we used earlier, but it’s most likely that you will need to &lt;a href="https://www.tensorflow.org/extend/estimators"&gt;build your own&lt;/a&gt;.&lt;/p&gt;  
&lt;p&gt;Writing a custom estimator means writing a &lt;code&gt;model_fn(features, labels, mode, params)&lt;/code&gt; that returns an &lt;code&gt;EstimatorSpec&lt;/code&gt;. The first step will be mapping the features into our embedding layer:&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;input_layer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;contrib&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;embed_sequence&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    features&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'x'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
    embedding_size&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    initializer&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Then we use &lt;code&gt;tf.layers&lt;/code&gt; to process each output sequentially.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;training &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;(&lt;/span&gt;mode &lt;span class="token operator"&gt;==&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;ModeKeys&lt;span class="token punctuation"&gt;.&lt;/span&gt;TRAIN&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
dropout_emb &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dropout&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;input_layer&lt;span class="token punctuation"&gt;,&lt;/span&gt;  
                                rate&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.2&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                                training&lt;span class="token operator"&gt;=&lt;/span&gt;training&lt;span class="token punctuation"&gt;)&lt;/span&gt;
conv &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;conv1d&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    inputs&lt;span class="token operator"&gt;=&lt;/span&gt;dropout_emb&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    filters&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;32&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    kernel_size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;3&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;"same"&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    activation&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;relu&lt;span class="token punctuation"&gt;)&lt;/span&gt;
pool &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;reduce_max&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_tensor&lt;span class="token operator"&gt;=&lt;/span&gt;conv&lt;span class="token punctuation"&gt;,&lt;/span&gt; axis&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
hidden &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;pool&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;250&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; activation&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;relu&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
dropout &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dropout&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;hidden&lt;span class="token punctuation"&gt;,&lt;/span&gt; rate&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;0.2&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; training&lt;span class="token operator"&gt;=&lt;/span&gt;training&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
logits &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;dropout_hidden&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Finally, we will use a &lt;code&gt;Head&lt;/code&gt; to simplify the writing of our last part of the &lt;code&gt;model_fn&lt;/code&gt;. The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. This is also used in the pre-made estimators and provides us with the benefit of a uniform evaluation function across all of our models. We will use &lt;code&gt;binary_classification_head&lt;/code&gt;, which is a head for single label binary classification that uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; as the loss function under the hood.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;head &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;contrib&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;binary_classification_head&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
optimizer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;.&lt;/span&gt;AdamOptimizer&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;_train_op_fn&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;loss&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;summary&lt;span class="token punctuation"&gt;.&lt;/span&gt;scalar&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'loss'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; loss&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; optimizer&lt;span class="token punctuation"&gt;.&lt;/span&gt;minimize&lt;span class="token punctuation"&gt;(&lt;/span&gt;
        loss&lt;span class="token operator"&gt;=&lt;/span&gt;loss&lt;span class="token punctuation"&gt;,&lt;/span&gt;
        global_step&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;train&lt;span class="token punctuation"&gt;.&lt;/span&gt;get_global_step&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;return&lt;/span&gt; head&lt;span class="token punctuation"&gt;.&lt;/span&gt;create_estimator_spec&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    features&lt;span class="token operator"&gt;=&lt;/span&gt;features&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    labels&lt;span class="token operator"&gt;=&lt;/span&gt;labels&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    mode&lt;span class="token operator"&gt;=&lt;/span&gt;mode&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    logits&lt;span class="token operator"&gt;=&lt;/span&gt;logits&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    train_op_fn&lt;span class="token operator"&gt;=&lt;/span&gt;_train_op_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Running this model is just as easy as before:&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;initializer &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;random_uniform&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; embedding_size&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1.0&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token number"&gt;1.0&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
params &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; initializer&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
cnn_classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;Estimator&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_fn&lt;span class="token operator"&gt;=&lt;/span&gt;model_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt;  
                                        model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'cnn'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
                                        params&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;cnn_classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="lstm-networks"&gt;LSTM Networks&lt;/h3&gt;  
&lt;p&gt;Using the &lt;code&gt;Estimator&lt;/code&gt; API and the same model &lt;code&gt;head&lt;/code&gt;, we can also create a classifier that uses a &lt;em&gt;Long Short-Term Memory&lt;/em&gt; (&lt;em&gt;LSTM&lt;/em&gt;) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.&lt;/p&gt;  
&lt;p&gt;One of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models turn out deeper and more complex, which usually produces slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients, that said, with sufficient tuning they can obtain state-of-the-art results for many problems. As a rule of thumb CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.&lt;/p&gt;  
&lt;p&gt;Each cell processes one token embedding at a time updating its internal state based on a differentiable computation that depends on both the embedding vector &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;x_t&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.43056em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.58056em; vertical-align: -0.15em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathit"&gt;x&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.280556em;"&gt;&lt;span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathit mtight"&gt;t&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.15em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and the previous state &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;h_{t-1}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.69444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.902771em; vertical-align: -0.208331em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathit"&gt;h&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.301108em;"&gt;&lt;span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"&gt;&lt;span class="pstrut" style="height: 2.7em;"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathit mtight"&gt;t&lt;/span&gt;&lt;span class="mbin mtight"&gt;−&lt;/span&gt;&lt;span class="mord mathrm mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist" style="height: 0.208331em;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. In order to get a better understanding of how LSTMs work, you can refer to Chris Olah’s &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;blog post&lt;/a&gt;.&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM Architecture"&gt;&lt;br&gt;  
&lt;small&gt;&lt;/small&gt;&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;  
Source: &lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt; by &lt;strong&gt;Chris Olah&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The complete LSTM model can be expressed by the following simple flowchart:&lt;/p&gt;  
&lt;div class="mermaid"&gt;&lt;svg xmlns="http://www.w3.org/2000/svg" id="mermaid-svg-LwaRGVWE7XOXWZj1" height="100%" viewbox="0 0 578.125 206.9140625" style="max-width:578.125px;"&gt;&lt;g&gt;&lt;g class="output"&gt;&lt;g class="clusters"/&gt;&lt;g class="edgePaths"&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M191.4375,46.25L216.4375,46.25L241.4375,46.25" marker-end="url(#arrowhead102)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead102" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M272.4790948275862,72.5L241.4375,106.66666666666666L241.4375,115.20833333333334L296.328125,123.75L351.21875,115.20833333333334L351.21875,106.66666666666666L320.1771551724138,72.5" marker-end="url(#arrowhead103)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead103" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;g class="edgePath" style="opacity: 1;"&gt;&lt;path class="path" d="M351.21875,46.25L376.21875,46.25L401.21875,46.25" marker-end="url(#arrowhead104)" style="fill:none"/&gt;&lt;defs&gt;&lt;marker id="arrowhead104" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"&gt;&lt;path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/&gt;&lt;/marker&gt;&lt;/defs&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabels"&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform="translate(296.328125,123.75)"&gt;&lt;g transform="translate(-43.1640625,-16.25)" class="label"&gt;&lt;foreignobject width="86.328125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"&gt;Recursion&lt;/span&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="edgeLabel" style="opacity: 1;" transform=""&gt;&lt;g transform="translate(0,0)" class="label"&gt;&lt;foreignobject width="0" height="0"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;&lt;span class="edgeLabel"/&gt;&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="nodes"&gt;&lt;g class="node" style="opacity: 1;" id="id1" transform="translate(105.71875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-85.71875" y="-26.25" width="171.4375" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-75.71875,-16.25)"&gt;&lt;foreignobject width="151.4453125" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Embedding Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id2" transform="translate(296.328125,46.25)"&gt;&lt;rect rx="5" ry="5" x="-54.890625" y="-26.25" width="109.78125" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-44.890625,-16.25)"&gt;&lt;foreignobject width="89.78515625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;LSTM Cell&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g class="node" style="opacity: 1;" id="id3" transform="translate(469.671875,46.25)"&gt;&lt;rect rx="5" ry="5" x="-68.453125" y="-26.25" width="136.90625" height="52.5"/&gt;&lt;g class="label" transform="translate(0,0)"&gt;&lt;g transform="translate(-58.453125,-16.25)"&gt;&lt;foreignobject width="116.9140625" height="32.5"&gt;&lt;div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; white-space: nowrap;"&gt;Output Layer&lt;/div&gt;&lt;/foreignobject&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;  
&lt;p&gt;In the beginning of this post, we padded all documents up to &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; tokens, which is necessary to build a proper tensor. However, when a document contains fewer than &lt;span class="katex--inline"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;200&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html" aria-hidden="true"&gt;&lt;span class="strut" style="height: 0.64444em;"&gt;&lt;/span&gt;&lt;span class="strut bottom" style="height: 0.64444em; vertical-align: 0em;"&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="mord mathrm"&gt;2&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;span class="mord mathrm"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; words, we don’t want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence’s end. We can do this by using the &lt;code&gt;"len"&lt;/code&gt; feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;lstm_cell &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;rnn_cell&lt;span class="token punctuation"&gt;.&lt;/span&gt;BasicLSTMCell&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token number"&gt;100&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
_&lt;span class="token punctuation"&gt;,&lt;/span&gt; final_states &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;nn&lt;span class="token punctuation"&gt;.&lt;/span&gt;dynamic_rnn&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
        lstm_cell&lt;span class="token punctuation"&gt;,&lt;/span&gt; inputs&lt;span class="token punctuation"&gt;,&lt;/span&gt; sequence_length&lt;span class="token operator"&gt;=&lt;/span&gt;features&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'len'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32&lt;span class="token punctuation"&gt;)&lt;/span&gt;
logits &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;layers&lt;span class="token punctuation"&gt;.&lt;/span&gt;dense&lt;span class="token punctuation"&gt;(&lt;/span&gt;inputs&lt;span class="token operator"&gt;=&lt;/span&gt;final_states&lt;span class="token punctuation"&gt;.&lt;/span&gt;h&lt;span class="token punctuation"&gt;,&lt;/span&gt; units&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="pre-trained-vectors"&gt;Pre-trained vectors&lt;/h3&gt;  
&lt;p&gt;Most of the models that we have shown before rely on word embeddings as a first layer. So far, we have initialized this embedding layer randomly. However, &lt;a href="https://arxiv.org/abs/1607.01759"&gt;much&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1301.3781"&gt;previous&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1103.0398"&gt;work&lt;/a&gt; has shown that using embeddings pre-trained on a large unlabeled corpus as initialization is beneficial, particularly when training on only a small number of labeled examples. The most popular pre-trained embedding is &lt;a href="https://www.tensorflow.org/tutorials/word2vec"&gt;word2vec&lt;/a&gt;. Leveraging knowledge from unlabeled data via pre-trained embeddings is an instance of &lt;em&gt;&lt;a href="http://ruder.io/transfer-learning/"&gt;transfer learning&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;  
&lt;p&gt;To this end, we will show you how to use them in an &lt;code&gt;Estimator&lt;/code&gt;. We will use the pre-trained vectors from another popular model, &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;GloVe&lt;/a&gt;.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embeddings &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
&lt;span class="token keyword"&gt;with&lt;/span&gt; &lt;span class="token builtin"&gt;open&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;'glove.6B.50d.txt'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'r'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; encoding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'utf-8'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;as&lt;/span&gt; f&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token keyword"&gt;for&lt;/span&gt; line &lt;span class="token keyword"&gt;in&lt;/span&gt; f&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        values &lt;span class="token operator"&gt;=&lt;/span&gt; line&lt;span class="token punctuation"&gt;.&lt;/span&gt;strip&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;split&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        w &lt;span class="token operator"&gt;=&lt;/span&gt; values&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;
        vectors &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;asarray&lt;span class="token punctuation"&gt;(&lt;/span&gt;values&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'float32'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
        embeddings&lt;span class="token punctuation"&gt;[&lt;/span&gt;w&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; vectors
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;After loading the vectors into memory from a file we store them as a &lt;code&gt;numpy.array&lt;/code&gt; using the same indexes as our vocabulary. The created array is of shape &lt;code&gt;(5000, 50)&lt;/code&gt;. At every row index, it contains the &lt;code&gt;50&lt;/code&gt;-dimensional vector representing the word at the same index in our vocabulary.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;embedding_matrix &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;random&lt;span class="token punctuation"&gt;.&lt;/span&gt;uniform&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; size&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;vocab_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; embedding_size&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;span class="token keyword"&gt;for&lt;/span&gt; w&lt;span class="token punctuation"&gt;,&lt;/span&gt; i &lt;span class="token keyword"&gt;in&lt;/span&gt; word_index&lt;span class="token punctuation"&gt;.&lt;/span&gt;items&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    v &lt;span class="token operator"&gt;=&lt;/span&gt; embeddings&lt;span class="token punctuation"&gt;.&lt;/span&gt;get&lt;span class="token punctuation"&gt;(&lt;/span&gt;w&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;if&lt;/span&gt; v &lt;span class="token keyword"&gt;is&lt;/span&gt; &lt;span class="token operator"&gt;not&lt;/span&gt; &lt;span class="token boolean"&gt;None&lt;/span&gt; &lt;span class="token operator"&gt;and&lt;/span&gt; i &lt;span class="token operator"&gt;&amp;lt;&lt;/span&gt; vocab_size&lt;span class="token punctuation"&gt;:&lt;/span&gt;
        embedding_matrix&lt;span class="token punctuation"&gt;[&lt;/span&gt;i&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;=&lt;/span&gt; v
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;Finally, we can use a custom initializer function and pass it in the &lt;code&gt;params&lt;/code&gt; object to our &lt;code&gt;cnn_model_fn&lt;/code&gt; , without any modifications.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;my_initializer&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;shape&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;None&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; dtype&lt;span class="token operator"&gt;=&lt;/span&gt;tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32&lt;span class="token punctuation"&gt;,&lt;/span&gt; partition_info&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;None&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token keyword"&gt;assert&lt;/span&gt; dtype &lt;span class="token keyword"&gt;is&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;float32
    &lt;span class="token keyword"&gt;return&lt;/span&gt; embedding_matrix
params &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;'embedding_initializer'&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; my_initializer&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
cnn\_pretrained\_classifier &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;Estimator&lt;span class="token punctuation"&gt;(&lt;/span&gt;  
    model_fn&lt;span class="token operator"&gt;=&lt;/span&gt;cnn_model_fn&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    model_dir&lt;span class="token operator"&gt;=&lt;/span&gt;os&lt;span class="token punctuation"&gt;.&lt;/span&gt;path&lt;span class="token punctuation"&gt;.&lt;/span&gt;join&lt;span class="token punctuation"&gt;(&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;'cnn_pretrained'&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt;
    params&lt;span class="token operator"&gt;=&lt;/span&gt;params&lt;span class="token punctuation"&gt;)&lt;/span&gt;
train\_and\_evaluate&lt;span class="token punctuation"&gt;(&lt;/span&gt;cnn_pretrained_classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;h3 id="running-tensorboard"&gt;Running TensorBoard&lt;/h3&gt;  
&lt;p&gt;Now we can launch TensorBoard and see how the different models we’ve trained compare against each other in terms of training time and performance.&lt;/p&gt;  
&lt;p&gt;In a terminal, we run&lt;/p&gt;  
&lt;pre class=" language-bash"&gt;&lt;code class="prism  language-bash"&gt;&lt;span class="token operator"&gt;&amp;gt;&lt;/span&gt; tensorboard --logdir&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;{&lt;/span&gt;model_dir&lt;span class="token punctuation"&gt;}&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;We can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds.&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/pr_curves.png" alt="PR curve"&gt;&lt;/p&gt;  
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/loss.png" alt="loss"&gt;&lt;/p&gt;  
&lt;h3 id="getting-predictions"&gt;Getting Predictions&lt;/h3&gt;  
&lt;p&gt;To obtain predictions on new sentences we can use the &lt;code&gt;predict&lt;/code&gt; method in the &lt;code&gt;Estimator&lt;/code&gt; instances, which will load the latest checkpoint for each model and evaluate on the unseen examples. But before passing the data into the model we have to clean up, tokenize and map each token to the corresponding index as we see below.&lt;/p&gt;  
&lt;pre class=" language-python"&gt;&lt;code class="prism  language-python"&gt;&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;text_to_index&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentence&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    &lt;span class="token comment"&gt;# Remove punctuation characters except for the apostrophe&lt;/span&gt;
    translator &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token builtin"&gt;str&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;maketrans&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; string&lt;span class="token punctuation"&gt;.&lt;/span&gt;punctuation&lt;span class="token punctuation"&gt;.&lt;/span&gt;replace&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token string"&gt;"'"&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;''&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    tokens &lt;span class="token operator"&gt;=&lt;/span&gt; sentence&lt;span class="token punctuation"&gt;.&lt;/span&gt;translate&lt;span class="token punctuation"&gt;(&lt;/span&gt;translator&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;lower&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;.&lt;/span&gt;split&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    &lt;span class="token keyword"&gt;return&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;+&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;word_index&lt;span class="token punctuation"&gt;[&lt;/span&gt;t&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token operator"&gt;+&lt;/span&gt; index_offset &lt;span class="token keyword"&gt;if&lt;/span&gt; t &lt;span class="token keyword"&gt;in&lt;/span&gt; word_index &lt;span class="token keyword"&gt;else&lt;/span&gt; &lt;span class="token number"&gt;2&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; t &lt;span class="token keyword"&gt;in&lt;/span&gt; tokens&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;

&lt;span class="token keyword"&gt;def&lt;/span&gt; &lt;span class="token function"&gt;print_predictions&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentences&lt;span class="token punctuation"&gt;,&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt;  
    indexes &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;text_to_index&lt;span class="token punctuation"&gt;(&lt;/span&gt;sentence&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; sentence &lt;span class="token keyword"&gt;in&lt;/span&gt; sentences&lt;span class="token punctuation"&gt;]&lt;/span&gt;
    x &lt;span class="token operator"&gt;=&lt;/span&gt; sequence&lt;span class="token punctuation"&gt;.&lt;/span&gt;pad_sequences&lt;span class="token punctuation"&gt;(&lt;/span&gt;indexes&lt;span class="token punctuation"&gt;,&lt;/span&gt;
                               maxlen&lt;span class="token operator"&gt;=&lt;/span&gt;sentence_size&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                               padding&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token string"&gt;'post'&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; 
                               value&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token operator"&gt;-&lt;/span&gt;&lt;span class="token number"&gt;1&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    length &lt;span class="token operator"&gt;=&lt;/span&gt; np&lt;span class="token punctuation"&gt;.&lt;/span&gt;array&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token builtin"&gt;min&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;&lt;span class="token builtin"&gt;len&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; sentence_size&lt;span class="token punctuation"&gt;)&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; x &lt;span class="token keyword"&gt;in&lt;/span&gt; indexes&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predict_input_fn &lt;span class="token operator"&gt;=&lt;/span&gt; tf&lt;span class="token punctuation"&gt;.&lt;/span&gt;estimator&lt;span class="token punctuation"&gt;.&lt;/span&gt;inputs&lt;span class="token punctuation"&gt;.&lt;/span&gt;numpy_input_fn&lt;span class="token punctuation"&gt;(&lt;/span&gt;x&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token punctuation"&gt;{&lt;/span&gt;&lt;span class="token string"&gt;"x"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; x&lt;span class="token punctuation"&gt;,&lt;/span&gt; &lt;span class="token string"&gt;"len"&lt;/span&gt;&lt;span class="token punctuation"&gt;:&lt;/span&gt; length&lt;span class="token punctuation"&gt;}&lt;/span&gt;&lt;span class="token punctuation"&gt;,&lt;/span&gt; shuffle&lt;span class="token operator"&gt;=&lt;/span&gt;&lt;span class="token boolean"&gt;False&lt;/span&gt;&lt;span class="token punctuation"&gt;)&lt;/span&gt;
    predictions &lt;span class="token operator"&gt;=&lt;/span&gt; &lt;span class="token punctuation"&gt;[&lt;/span&gt;p&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token string"&gt;'logistic'&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;&lt;span class="token punctuation"&gt;[&lt;/span&gt;&lt;span class="token number"&gt;0&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt; &lt;span class="token keyword"&gt;for&lt;/span&gt; p &lt;span class="token keyword"&gt;in&lt;/span&gt; classifier&lt;span class="token punctuation"&gt;.&lt;/span&gt;predict&lt;span class="token punctuation"&gt;(&lt;/span&gt;input_fn&lt;span class="token operator"&gt;=&lt;/span&gt;predict_input_fn&lt;span class="token punctuation"&gt;)&lt;/span&gt;&lt;span class="token punctuation"&gt;]&lt;/span&gt;
    &lt;span class="token keyword"&gt;print&lt;/span&gt;&lt;span class="token punctuation"&gt;(&lt;/span&gt;predictions&lt;span class="token punctuation"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;  
&lt;p&gt;It is worth noting that the checkpoint itself is not sufficient to make predictions; the actual code used to build the estimator is necessary as well in order to map the saved weights to the corresponding tensors. It’s a good practice to associate saved checkpoints with the branch of code with which they were created.&lt;/p&gt;  
&lt;p&gt;If you are interested in exporting the models to disk in a fully recoverable way, you might want to look into the &lt;a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators"&gt;SavedModel&lt;/a&gt; class, which is especially useful for serving your model through an API using &lt;a href="https://github.com/tensorflow/serving"&gt;TensorFlow Serving&lt;/a&gt;.&lt;/p&gt;  
&lt;h3 id="summary"&gt;Summary&lt;/h3&gt;  
&lt;p&gt;In this blog post, we explored how to use estimators for text classification, in particular for the IMDB Reviews Dataset. We trained and visualized our own embeddings, as well as loaded pre-trained ones. We started from a simple baseline and made our way to convolutional neural networks and LSTMs.&lt;/p&gt;  
&lt;p&gt;For more details, be sure to check out:&lt;/p&gt;  
&lt;ul&gt;  
&lt;li&gt;A &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb"&gt;Jupyter notebook&lt;/a&gt; that can run locally, or on Colaboratory.&lt;/li&gt;  
&lt;li&gt;The complete &lt;a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py"&gt;source code&lt;/a&gt; for this blog post.&lt;/li&gt;  
&lt;li&gt;The TensorFlow &lt;a href="https://www.tensorflow.org/programmers_guide/embedding"&gt;Embedding&lt;/a&gt; guide.&lt;/li&gt;  
&lt;li&gt;The TensorFlow &lt;a href="https://www.tensorflow.org/tutorials/word2vec"&gt;Vector Representation of Words&lt;/a&gt; tutorial.&lt;/li&gt;  
&lt;li&gt;The &lt;em&gt;NLTK&lt;/em&gt; &lt;a href="http://www.nltk.org/book/ch03.html"&gt;Processing Raw Text&lt;/a&gt; chapter on how to design language pipelines.&lt;/li&gt;  
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt;  
</content:encoded></item><item><title>Requests for Research</title><description>&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentdataaugmentationfornlp"&gt;Task-independent data augmentation for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#fewshotlearningfornlp"&gt;Few-shot learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#transferlearningfornlp"&gt;Transfer learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#crosslinguallearning"&gt;Cross-lingual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentarchitectureimprovements"&gt;Task-independent architecture improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be hard to find compelling topics to work on and know what questions are interesting to ask when you are just starting as a researcher&lt;/p&gt;</description><link>http://ruder.io/requests-for-research/</link><guid isPermaLink="false">8115c479-d16e-4b22-9a0f-daf8ed5e1693</guid><category>deep learning</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 04 Mar 2018 15:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentdataaugmentationfornlp"&gt;Task-independent data augmentation for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#fewshotlearningfornlp"&gt;Few-shot learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#transferlearningfornlp"&gt;Transfer learning for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#crosslinguallearning"&gt;Cross-lingual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/requests-for-research/#taskindependentarchitectureimprovements"&gt;Task-independent architecture improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be hard to find compelling topics to work on and know what questions are interesting to ask when you are just starting as a researcher in a new field. Machine learning research in particular moves so fast these days that it is difficult to find an opening.&lt;/p&gt;

&lt;p&gt;This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research. It gathers a collection of research topics that are interesting to me, with a focus on NLP and transfer learning. As such, they might obviously not be of interest to everyone. If you are interested in Reinforcement Learning, OpenAI provides a &lt;a href="https://blog.openai.com/requests-for-research-2/"&gt;selection of interesting RL-focused research topics&lt;/a&gt;. In case you'd like to collaborate with others or are interested in a broader range of topics, have a look at the &lt;a href="https://ai-on.org/"&gt;Artificial Intelligence Open Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most of these topics are not thoroughly thought out yet; in many cases, the general description is quite vague and subjective and many directions are possible. In addition, most of these are &lt;em&gt;not&lt;/em&gt; low-hanging fruit, so serious effort is necessary to come up with a solution. I am happy to provide feedback with regard to any of these, but will not have time to provide more detailed guidance unless you have a working proof-of-concept. I will update this post periodically with new research directions and advances in already listed ones. Note that this collection does not attempt to review the extensive literature but only aims to give a glimpse of a topic; consequently, the references won't be comprehensive.&lt;/p&gt;

&lt;p&gt;I hope that this collection will pique your interest and serve as inspiration for your own research agenda.&lt;/p&gt;

&lt;h2 id="taskindependentdataaugmentationfornlp"&gt; Task-independent data augmentation for NLP&lt;/h2&gt;

&lt;p&gt;Data augmentation aims to create additional training data by producing variations of existing training examples through transformations, which can mirror those encountered in the real world. In Computer Vision (CV), common augmentation techniques are &lt;a href="https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation"&gt;mirroring, random cropping, shearing, etc&lt;/a&gt;. Data augmentation is super useful in CV. For instance, it has been used to great effect in AlexNet (Krizhevsky et al., 2012) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] to combat overfitting and in most state-of-the-art models since. In addition, data augmentation makes intuitive sense as it makes the training data more diverse and should thus increase a model's generalization ability.&lt;/p&gt;

&lt;p&gt;However, in NLP, data augmentation is not widely used. In my mind, this is for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data in NLP is discrete. This prevents us from applying simple transformations directly to the input data. Most recently proposed augmentation methods in CV focus on such transformations, e.g. domain randomization (Tobin et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;].  &lt;/li&gt;
&lt;li&gt;Small perturbations may change the meaning. Deleting a negation may change a sentence's sentiment, while modifying a word in a paragraph might inadvertently change the answer to a question about that paragraph. This is not the case in CV where perturbing individual pixels does not change whether an image is a cat or dog and even stark changes such as interpolation of different images can be useful (Zhang et al., 2017) [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;].&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Existing approaches that I am aware of are either rule-based (Li et al., 2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] or task-specific, e.g. for parsing (Wang and Eisner, 2016) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] or zero-pronoun resolution (Liu et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;]. Xie et al. (2017) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] replace words with samples from different distributions for language modelling and Machine Translation. Recent work focuses on creating adversarial examples either by replacing words or characters (Samanta and Mehta, 2017; Ebrahimi et al., 2017) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;], concatenation (Jia and Liang, 2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;], or adding adversarial perturbations (Yasunaga et al., 2017) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;]. An adversarial setup is also used by Li et al. (2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] who train a system to produce sequences that are indistinguishable from human-generated dialogue utterances.&lt;/p&gt;

&lt;p&gt;Back-translation (Sennrich et al., 2015; Sennrich et al., 2016) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] is a common data augmentation method in Machine Translation (MT) that allows us to incorporate monolingual training data. For instance, when training a EN\(\rightarrow\)FR system, monolingual French text is translated to English using an FR\(\rightarrow\)EN system; the synthetic parallel data can then be used for training. Back-translation can also be used for paraphrasing (Mallinson et al., 2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;]. Paraphrasing has been used for data augmentation for QA (Dong et al., 2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;], but I am not aware of its use for other tasks.&lt;/p&gt;

&lt;p&gt;Another method that is close to paraphrasing is generating sentences from a continuous space using a variational autoencoder (Bowman et al., 2016; Guu et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;]. If the representations are disentangled as in (Hu et al., 2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;], then we are also not too far from style transfer (Shen et al., 2017) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;There are a few research directions that would be interesting to pursue:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Evaluation study:&lt;/strong&gt; Evaluate a range of existing data augmentation methods as well as techniques that have not been widely used for augmentation such as paraphrasing and style transfer on a diverse range of tasks including text classification and sequence labelling. Identify what types of data augmentation are robust across task and which are task-specific. This could be packaged as a software library to make future benchmarking easier (think &lt;a href="https://github.com/tensorflow/cleverhans"&gt;CleverHans&lt;/a&gt; for NLP).  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data augmentation with style transfer:&lt;/strong&gt; Investigate if style transfer can be used to modify various attributes of training examples for more robust learning.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn the augmentation:&lt;/strong&gt; Similar to Dong et al. (2017) we could learn either to paraphrase or to generate transformations for a particular task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn a word embedding space for data augmentation:&lt;/strong&gt; A typical word embedding space clusters synonyms and antonyms together; using nearest neighbours in this space for replacement is thus infeasible. Inspired by recent work (Mrkšić et al., 2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;], we could specialize the word embedding space to make it more suitable for data augmentation.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adversarial data augmentation:&lt;/strong&gt; Related to recent work in interpretability (Ribeiro et al., 2016) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;], we could change the most salient words in an example, i.e. those that a model depends on for a prediction. This still requires a semantics-preserving replacement method, however.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="fewshotlearningfornlp"&gt; Few-shot learning for NLP&lt;/h2&gt;

&lt;p&gt;Zero-shot, one-shot and few-shot learning are one of the most interesting recent research directions IMO. Following the key insight from Vinyals et al. (2016) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;] that a few-shot learning model should be explicitly trained to perform few-shot learning, we have seen several recent advances (Ravi and Larochelle, 2017; Snell et al., 2017) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Learning from few labeled samples is one of the hardest problems IMO and one of the core capabilities that separates the current generation of ML models from more generally applicable systems. Zero-shot learning has only been investigated in the context of &lt;a href="http://ruder.io/word-embeddings-2017/index.html#oovhandling"&gt;learning word embeddings for unknown words&lt;/a&gt; AFAIK. Dataless classification (Song and Roth, 2014; Song et al., 2016) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;] is an interesting related direction that embeds labels and documents in a joint space, but requires interpretable labels with good descriptions. &lt;/p&gt;

&lt;p&gt;Potential research directions are the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Standardized benchmarks:&lt;/strong&gt; Create standardized benchmarks for few-shot learning for NLP. Vinyals et al. (2016) introduce a one-shot language modelling task for the Penn Treebank. The task, while useful, is dwarfed by the extensive evaluation on CV benchmarks and has not seen much use AFAIK. A few-shot learning benchmark for NLP should contain a large number of classes and provide a standardized split for reproducibility. Good candidate tasks would be topic classification or fine-grained entity recognition.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation study&lt;/strong&gt;: After creating such a benchmark, the next step would be to evaluate how well existing few-shot learning models from CV perform for NLP.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Novel methods for NLP&lt;/strong&gt;: Given a dataset for benchmarking and an empirical evaluation study, we could then start developing novel methods that can perform few-shot learning for NLP.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="transferlearningfornlp"&gt; Transfer learning for NLP&lt;/h2&gt;

&lt;p&gt;Transfer learning has had a large impact on computer vision (CV) and has greatly lowered the entry threshold for people wanting to apply CV algorithms to their own problems. CV practicioners are no longer required to perform extensive feature-engineering for every new task, but can simply fine-tune a model pretrained on a large dataset with a small number of examples.&lt;/p&gt;

&lt;p&gt;In NLP, however, we have so far only been pretraining the first layer of our models via pretrained embeddings. Recent approaches (Peters et al., 2017, 2018) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] add pretrained language model embedddings, but these still require custom architectures for every task. In my opinion, in order to unlock the true potential of transfer learning for NLP, we need to pretrain the entire model and fine-tune it on the target task, akin to fine-tuning ImageNet models. Language modelling, for instance, is a great task for pretraining and could be to NLP what ImageNet classification is to CV (Howard and Ruder, 2018) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Here are some potential research directions in this context:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify useful pretraining tasks:&lt;/strong&gt; The choice of the pretraining task is very important as even fine-tuning a model on a related task might only provide limited success (Mou et al., 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;]. Other tasks such as those explored in recent work on learning general-purpose sentence embeddings (Conneau et al., 2017; Subramanian et al., 2018; Nie et al., 2017) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;] might be complementary to language model pretraining or suitable for other target tasks.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-tuning of complex architectures:&lt;/strong&gt; Pretraining is most useful when a model can be applied to many target tasks. However, it is still unclear how to pretrain more complex architectures, such as those used for pairwise classification tasks (Augenstein et al., 2018) or reasoning tasks such as QA or reading comprehension.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="multitasklearning"&gt; Multi-task learning&lt;/h2&gt;

&lt;p&gt;Multi-task learning (MTL) has become more commonly used in NLP. See &lt;a href="http://ruder.io/multi-task/"&gt;here&lt;/a&gt; for a general overview of multi-task learning and &lt;a href="http://ruder.io/multi-task-learning-nlp/"&gt;here&lt;/a&gt; for MTL objectives for NLP. However, there is still much we don't understand about multi-task learning in general.&lt;/p&gt;

&lt;p&gt;The main questions regarding MTL give rise to many interesting research directions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Identify effective auxiliary tasks:&lt;/strong&gt; One of the main questions is which tasks are useful for multi-task learning. Label entropy has been shown to be a predictor of MTL success (Alonso and Plank, 2017) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;], but this does not tell the whole story. In recent work (Augenstein et al., 2018) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;], we have found that auxiliary tasks with more data and more fine-grained labels are more useful. It would be useful if future MTL papers would not only propose a new model or auxiliary task, but also try to understand why a certain auxiliary task might be better than another closely related one.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alternatives to hard parameter sharing:&lt;/strong&gt; Hard parameter sharing is still the default modus operandi for MTL, but places a strong constraint on the model to compress knowledge pertaining to different tasks with the same parameters, which often makes learning difficult. We need better ways of doing MTL that are easy to use and work reliably across many tasks. Recently proposed methods such as cross-stitch units (Misra et al., 2017; Ruder et al., 2017) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] and a label embedding layer (Augenstein et al., 2018) are promising steps in this direction.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Artificial auxiliary tasks:&lt;/strong&gt; The best auxiliary tasks are those, which are tailored to the target task and do not require any additional data. I have outlined a list of potential &lt;em&gt;artificial&lt;/em&gt; auxiliary tasks &lt;a href="http://ruder.io/multi-task-learning-nlp/"&gt;here&lt;/a&gt;. However, it is not clear which of these work reliably across a number of diverse tasks or what variations or task-specific modifications are useful.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="crosslinguallearning"&gt; Cross-lingual learning&lt;/h2&gt;

&lt;p&gt;Creating models that perform well across languages and that can transfer knowledge from resource-rich to resource-poor languages is one of the most important research directions IMO. There has been much progress in learning cross-lingual representations that project different languages into a shared embedding space. Refer to Ruder et al. (2017) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] for a survey.&lt;/p&gt;

&lt;p&gt;Cross-lingual representations are commonly evaluated either intrinsically on similarity benchmarks or extrinsically on downstream tasks, such as text classification. While recent methods have advanced the state-of-the-art for many of these settings, we do not have a good understanding of the tasks or languages for which these methods fail and how to mitigate these failures in a task-independent manner, e.g. by injecting task-specific constraints (Mrkšić et al., 2017).&lt;/p&gt;

&lt;h2 id="taskindependentarchitectureimprovements"&gt; Task-independent architecture improvements&lt;/h2&gt;

&lt;p&gt;Novel architectures that outperform the current state-of-the-art and are tailored to specific tasks are regularly introduced, superseding the previous architecture. I have outlined &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/"&gt;best practices for different NLP tasks&lt;/a&gt; before, but without comparing such architectures on different tasks, it is often hard to gain insights from specialized architectures and tell which components would also be useful in other settings. &lt;/p&gt;

&lt;p&gt;A particularly promising recent model is the Transformer (Vaswani et al., 2017) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/requests-for-research/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. While the complete model might not be appropriate for every task, components such as multi-head attention or position-based encoding could be building blocks that are generally useful for many NLP tasks.&lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope you've found this collection of research directions useful. If you have suggestions on how to tackle some of these problems or ideas for related research topics, feel free to comment below.&lt;/p&gt;

&lt;h2 id="references"&gt; References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105). &lt;a href="http://ruder.io/requests-for-research/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., &amp;amp; Abbeel, P. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. arXiv Preprint arXiv:1703.06907. Retrieved from &lt;a href="http://arxiv.org/abs/1703.06907"&gt;http://arxiv.org/abs/1703.06907&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Zhang, H., Cisse, M., Dauphin, Y. N., &amp;amp; Lopez-Paz, D. (2017). mixup: Beyond Empirical Risk Minimization, 1–11. Retrieved from &lt;a href="http://arxiv.org/abs/1710.09412"&gt;http://arxiv.org/abs/1710.09412&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp;amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. NIPS 2016. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04080"&gt;http://arxiv.org/abs/1606.04080&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Li, Y., Cohn, T., &amp;amp; Baldwin, T. (2017). Robust Training under Linguistic Adversity. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (Vol. 2, pp. 21–27). &lt;a href="http://ruder.io/requests-for-research/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Wang, D., &amp;amp; Eisner, J. (2016). The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages. Tacl, 4, 491–505. Retrieved from &lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212%0Ahttps://transacl.org/ojs/index.php/tacl/article/view/917"&gt;https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212%0Ahttps://transacl.org/ojs/index.php/tacl/article/view/917&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Liu, T., Cui, Y., Yin, Q., Zhang, W., Wang, S., &amp;amp; Hu, G. (2017). Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 102–111). &lt;a href="http://ruder.io/requests-for-research/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Samanta, S., &amp;amp; Mehta, S. (2017). Towards Crafting Text Adversarial Samples. arXiv preprint arXiv:1707.02812. &lt;a href="http://ruder.io/requests-for-research/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Ebrahimi, J., Rao, A., Lowd, D., &amp;amp; Dou, D. (2017). HotFlip: White-Box Adversarial Examples for NLP. Retrieved from &lt;a href="http://arxiv.org/abs/1712.06751"&gt;http://arxiv.org/abs/1712.06751&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Yasunaga, M., Kasai, J., &amp;amp; Radev, D. (2017). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from &lt;a href="http://arxiv.org/abs/1711.04903"&gt;http://arxiv.org/abs/1711.04903&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Jia, R., &amp;amp; Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2015). Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709. &lt;a href="http://ruder.io/requests-for-research/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891. &lt;a href="http://ruder.io/requests-for-research/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Mallinson, J., Sennrich, R., &amp;amp; Lapata, M. (2017). Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers (Vol. 1, pp. 881-893). &lt;a href="http://ruder.io/requests-for-research/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Dong, L., Mallinson, J., Reddy, S., &amp;amp; Lapata, M. (2017). Learning to Paraphrase for Question Answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Li, J., Monroe, W., Shi, T., Ritter, A., &amp;amp; Jurafsky, D. (2017). Adversarial Learning for Neural Dialogue Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1701.06547"&gt;http://arxiv.org/abs/1701.06547&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp;amp; Bengio, S. (2016). Generating Sentences from a Continuous Space. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Retrieved from &lt;a href="http://arxiv.org/abs/1511.06349"&gt;http://arxiv.org/abs/1511.06349&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., &amp;amp; Xing, E. P. (2017). Toward Controlled Generation of Text. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/requests-for-research/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Guu, K., Hashimoto, T. B., Oren, Y., &amp;amp; Liang, P. (2017). Generating Sentences by Editing Prototypes. &lt;a href="http://ruder.io/requests-for-research/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Shen, T., Lei, T., Barzilay, R., &amp;amp; Jaakkola, T. (2017). Style Transfer from Non-Parallel Text by Cross-Alignment. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1705.09655"&gt;http://arxiv.org/abs/1705.09655&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Mrkšić, N., Vulić, I., Séaghdha, D. Ó., Leviant, I., Reichart, R., Gašić, M., … Young, S. (2017). Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints. TACL. Retrieved from &lt;a href="http://arxiv.org/abs/1706.00374"&gt;http://arxiv.org/abs/1706.00374&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ribeiro, M. T., Singh, S., &amp;amp; Guestrin, C. (2016, August). Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). ACM. &lt;a href="http://ruder.io/requests-for-research/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Ravi, S., &amp;amp; Larochelle, H. (2017). Optimization as a Model for Few-Shot Learning. In ICLR 2017. &lt;a href="http://ruder.io/requests-for-research/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Snell, J., Swersky, K., &amp;amp; Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/requests-for-research/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Song, Y., &amp;amp; Roth, D. (2014). On dataless hierarchical text classification. Proceedings of AAAI, 1579–1585. Retrieved from &lt;a href="http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf"&gt;http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Song, Y., Upadhyay, S., Peng, H., &amp;amp; Roth, D. (2016). Cross-Lingual Dataless Classification for Many Languages. Ijcai, 2901–2907. &lt;a href="http://ruder.io/requests-for-research/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Augenstein, I., Ruder, S., &amp;amp; Søgaard, A. (2018). Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces. In Proceedings of NAACL 2018. &lt;a href="http://ruder.io/requests-for-research/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Alonso, H. M., &amp;amp; Plank, B. (2017). When is multitask learning effective? Multitask learning for semantic sequence prediction under varying data conditions. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1612.02251"&gt;http://arxiv.org/abs/1612.02251&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Misra, I., Shrivastava, A., Gupta, A., &amp;amp; Hebert, M. (2016). Cross-stitch Networks for Multi-task Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. &lt;a href="http://doi.org/10.1109/CVPR.2016.433"&gt;http://doi.org/10.1109/CVPR.2016.433&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv preprint arXiv:1705.08142. &lt;a href="http://ruder.io/requests-for-research/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/requests-for-research/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp;amp; Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings of NAACL. &lt;a href="http://ruder.io/requests-for-research/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Howard, J., &amp;amp; Ruder, S. (2018). Fine-tuned Language Models for Text Classification. arXiv preprint arXiv:1801.06146. &lt;a href="http://ruder.io/requests-for-research/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp;amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Subramanian, S., Trischler, A., Bengio, Y., &amp;amp; Pal, C. J. (2018). Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Proceedings of ICLR 2018. &lt;a href="http://ruder.io/requests-for-research/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Ruder, S., Vulić, I., &amp;amp; Søgaard, A. (2017). A Survey of Cross-lingual Word Embedding Models. arXiv Preprint arXiv:1706.04902. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04902"&gt;http://arxiv.org/abs/1706.04902&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/requests-for-research/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Mou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L., &amp;amp; Jin, Z. (2016). How Transferable are Neural Networks in NLP Applications? Proceedings of 2016 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/requests-for-research/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Xie, Z., Wang, S. I., Li, J., Levy, D., Nie, A., Jurafsky, D., &amp;amp; Ng, A. Y. (2017). Data Noising as Smoothing in Neural Network Language Models. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/requests-for-research/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Nie, A., Bennett, E. D., &amp;amp; Goodman, N. D. (2017). DisSent: Sentence Representation Learning from Explicit Discourse Relations. arXiv Preprint arXiv:1710.04334. Retrieved from &lt;a href="http://arxiv.org/abs/1710.04334"&gt;http://arxiv.org/abs/1710.04334&lt;/a&gt; &lt;a href="http://ruder.io/requests-for-research/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Optimization for Deep Learning Highlights in 2017</title><description>An overview of the most exciting highlights and research directions in optimization for Deep Learning in 2017.</description><link>http://ruder.io/deep-learning-optimization-2017/</link><guid isPermaLink="false">24033557-01b5-4d3a-aecf-9497f83f90a9</guid><category>deep learning</category><category>optimization</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 03 Dec 2017 15:36:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/12/snapshot_ensembles.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/12/snapshot_ensembles.png" alt="Optimization for Deep Learning Highlights in 2017"&gt;&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#improvingadam"&gt;Improving Adam&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#decouplingweightdecay"&gt;Decoupling weight decay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fixingtheexponentialmovingaverage"&gt;Fixing the exponential moving average&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#tuningthelearningrate"&gt;Tuning the learning rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#warmrestarts"&gt;Warm restarts&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#sgdwithrestarts"&gt;SGD with restarts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#snapshotensembles"&gt;Snapshot ensembles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#adamwithrestarts"&gt;Adam with restarts&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#learningtooptimize"&gt;Learning to optimize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#understandinggeneralization"&gt;Understanding generalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deep Learning ultimately is about finding a minimum that generalizes well -- with bonus points for finding one fast and reliably. Our workhorse, stochastic gradient descent (SGD), is a 60-year old algorithm (Robbins and Monro, 1951) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;], that is as essential to the current generation of Deep Learning algorithms as back-propagation.&lt;/p&gt;

&lt;p&gt;Different optimization algorithms have been proposed in recent years, which use different equations to update a model's parameters. Adam (Kingma and Ba, 2015) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] was introduced in 2015 and is arguably today still the most commonly used one of these algorithms. This indicates that from the Machine Learning practitioner's perspective, best practices for optimization for Deep Learning have largely remained the same.&lt;/p&gt;

&lt;p&gt;New ideas, however, have been developed over the course of this year, which may shape the way will optimize our models in the future. In this blog post, I will touch on the most exciting highlights and most promising directions in optimization for Deep Learning in my opinion. Note that this blog post assumes a familiarity with SGD and with adaptive learning rate methods such as Adam. To get up to speed, refer to &lt;a href="http://ruder.io/optimizing-gradient-descent/index.html"&gt;this blog post&lt;/a&gt; for an overview of existing gradient descent optimization algorithms.&lt;/p&gt;

&lt;h2 id="improvingadam"&gt;Improving Adam&lt;/h2&gt;

&lt;p&gt;Despite the apparent supremacy of adaptive learning rate methods such as Adam, state-of-the-art results for many tasks in computer vision and NLP such as object recognition (Huang et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] or machine translation (Wu et al., 2016) [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] have still been achieved by plain old SGD with momentum. Recent theory (Wilson et al., 2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] provides some justification for this, suggesting that adaptive learning rate methods converge to different (and less optimal) minima than SGD with momentum. It is empirically shown that the minima found by adaptive learning rate methods perform generally worse compared to those found by SGD with momentum on object recognition, character-level language modeling, and constituency parsing. This seems counter-intuitive given that Adam comes with nice convergence guarantees and that its adaptive learning rate should give it an edge over the regular SGD. However, Adam and other adaptive learning rate methods are not without their own flaws.&lt;/p&gt;

&lt;h3 id="decouplingweightdecay"&gt; Decoupling weight decay&lt;/h3&gt;

&lt;p&gt;One factor that partially accounts for Adam's poor generalization ability compared with SGD with momentum on some datasets is weight decay. Weight decay is most commonly used in image classification problems and decays the weights \(\theta_t\) after every parameter update by multiplying them by a decay rate \(w_t\) that is slightly less than \(1\):&lt;/p&gt;

&lt;p&gt;\(\theta_{t+1} = w_t \: \theta_t \)&lt;/p&gt;

&lt;p&gt;This prevents the weights from growing too large. As such, weight decay can also be understood as an \(\ell_2\) regularization term that depends on the weight decay rate \(w_t\) added to the loss:&lt;/p&gt;

&lt;p&gt;\(\mathcal{L}_\text{reg} = \dfrac{w_t}{2} \|\theta_t \|^2_2 \)&lt;/p&gt;

&lt;p&gt;Weight decay is commonly implemented in many neural network libraries either as the above regularization term or directly to modify the gradient. As the gradient is modified in both the momentum and Adam update equations (via multiplication with other decay terms), weight decay no longer equals \(\ell_2\) regularization. Loshchilov and Hutter (2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] thus propose to decouple weight decay from the gradient update by adding it after the parameter update as in the original definition. &lt;br&gt;
The SGD with momentum and weight decay (SGDW) update then looks like the following:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
v_t &amp;amp;= \gamma v_{t-1} + \eta g_t \\ &lt;br&gt;
\theta_{t+1} &amp;amp;= \theta_t - v_t - \eta w_t \theta_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;where \(\eta\) is the learning rate and the third term in the second equation is the decoupled weight decay. Similarly, for Adam with weight decay (AdamW) we obtain:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
m_t &amp;amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ &lt;br&gt;
v_t &amp;amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\ &lt;br&gt;
\hat{m}_t &amp;amp;= \dfrac{m_t}{1 - \beta^t_1} \\
\hat{v}_t &amp;amp;= \dfrac{v_t}{1 - \beta^t_2} \\
\theta_{t+1} &amp;amp;= \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t - \eta w_t \theta_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;where \(m_t\) and \(\hat{m}_t\) and \(v_t\) and \(\hat{v}_t\) are the biased and bias-corrected estimates of the first and second moments respectively and \(\beta_1\) and \(\beta_2\) are their decay rates, with the same weight decay term added to it. The authors show that this substantially improves Adam’s generalization performance and allows it to compete with SGD with momentum on image classification datasets.&lt;/p&gt;

&lt;p&gt;In addition, it decouples the choice of the learning rate from the choice of the weight decay, which enables better hyperparameter optimization as the hyperparameters no longer depend on each other. It also separates the implementation of the optimizer from the implementation of the weight decay, which contributes to cleaner and more reusable code (see e.g. the &lt;a href="https://github.com/fastai/fastai/pull/46/files"&gt;fast.ai AdamW/SGDW implementation&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id="fixingtheexponentialmovingaverage"&gt;Fixing the exponential moving average&lt;/h3&gt;

&lt;p&gt;Several recent papers (Dozat and Manning, 2017; Laine and Aila, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;,&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] empirically find that a lower \(\beta_2\) value, which controls the contribution of the exponential moving average of past squared gradients in Adam, e.g. \(0.99\) or \(0.9\) vs. the default \(0.999\) worked better in their respective applications, indicating that there might be an issue with the exponential moving average.&lt;/p&gt;

&lt;p&gt;An &lt;a href="https://openreview.net/forum?id=ryQu7f-RZ"&gt;ICLR 2018 submission&lt;/a&gt; formalizes this issue and pinpoints the exponential moving average of past squared gradients as another reason for the poor generalization behaviour of adaptive learning rate methods. Updating the parameters via an exponential moving average of past squared gradients is at the heart of adaptive learning rate methods such as Adadelta, RMSprop, and Adam. The contribution of the exponential average is well-motivated: It should prevent the learning rates to become infinitesimally small as training progresses, the key flaw of the Adagrad algorithm. However, this short-term memory of the gradients becomes an obstacle in other scenarios.&lt;/p&gt;

&lt;p&gt;In settings where Adam converges to a suboptimal solution, it has been observed that some minibatches provide large and informative gradients, but as these minibatches only occur rarely, exponential averaging diminishes their influence, which leads to poor convergence. The authors provide an example for a simple convex optimization problem where the same behaviour can be observed for Adam.&lt;/p&gt;

&lt;p&gt;To fix this behaviour, the authors propose a new algorithm, AMSGrad that uses the maximum of past squared gradients rather than the exponential average to update the parameters. The full AMSGrad update without bias-corrected estimates can be seen below:&lt;/p&gt;

&lt;p&gt;\(
\begin{align}
\begin{split}
m_t &amp;amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ &lt;br&gt;
v_t &amp;amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\ &lt;br&gt;
\hat{v}_t &amp;amp;= \text{max}(\hat{v}_{t-1}, v_t) \\
\theta_{t+1} &amp;amp;= \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} m_t
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p&gt;The authors observe improved performance compared to Adam on small datasets and on CIFAR-10.&lt;/p&gt;

&lt;h2 id="tuningthelearningrate"&gt;Tuning the learning rate&lt;/h2&gt;

&lt;p&gt;In many cases, it is not our models that require improvement and tuning, but our hyperparameters. Recent examples for language modelling demonstrate that tuning LSTM parameters (Melis et al., 2017) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;] and regularization parameters (Merity et al., 2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] can yield state-of-the-art results compared to more complex models.&lt;/p&gt;

&lt;p&gt;An important hyperparameter for optimization in Deep Learning is the learning rate \(\eta\). In fact, SGD has been shown to require a learning rate annealing schedule to converge to a good minimum in the first place. It is often thought that adaptive learning rate methods such as Adam are more robust to different learning rates, as they update the learning rate themselves. Even for these methods, however, there can be a large difference between a good and the optimal learning rate (psst... it's &lt;a href="https://twitter.com/karpathy/status/801621764144971776"&gt;\(3e-4\)&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;Zhang et al. (2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] show that SGD with a tuned learning rate annealing schedule and momentum parameter is not only competitive with Adam, but also converges faster. On the other hand, while we might think that the adaptivity of Adam's learning rates might mimic learning rate annealing, an explicit annealing schedule can still be beneficial: If we add SGD-style learning rate annealing to Adam, it converges faster and outperforms SGD on Machine Translation (Denkowski and Neubig, 2017) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;In fact, learning rate annealing schedule engineering seems to be the new feature engineering as we can often find highly-tuned learning rate annealing schedules that improve the final convergence behaviour of our model. An interesting example of this is Vaswani et al. (2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;]. While it is usual to see a model's hyperparameters being subjected to large-scale hyperparameter optimization, it is interesting to see a learning rate annealing schedule as the focus of the same attention to detail: The authors use Adam with \(\beta_1=0.9\), a non-default \(\beta_2=0.98\), \(\epsilon = 10^{-9}\), and arguably one of the most elaborate annealing schedules for the learning rate \(\eta\):&lt;/p&gt;

&lt;p&gt;\(\eta  = d_\text{model}^{-0.5} \cdot \min(step\text{_}num^{-0.5}, step\text{_}num \cdot warmup\text{_}steps^{-1.5}) \)&lt;/p&gt;

&lt;p&gt;where \(d_\text{model}\) is the number of parameters of the model and \(warmup\text{_}steps = 4000\). &lt;/p&gt;

&lt;p&gt;Another recent paper by Smith et al. (2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] demonstrates an interesting connection between the learning rate and the batch size, two hyperparameters that are typically thought to be independent of each other: They show that decaying the learning rate is equivalent to increasing the batch size, while the latter allows for increased parallelism. Conversely, we can reduce the number of model updates and thus speed up training by increasing the learning rate and scaling the batch size. This has ramifications for large-scale Deep Learning, which can now repurpose existing training schedules with no hyperparameter tuning.&lt;/p&gt;

&lt;h2 id="warmrestarts"&gt;Warm restarts&lt;/h2&gt;

&lt;h3 id="sgdwithrestarts"&gt; SGD with restarts&lt;/h3&gt;

&lt;p&gt;Another effective recent development is SGDR (Loshchilov and Hutter, 2017) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;], an SGD alternative that uses warm restarts instead of learning rate annealing. In each restart, the learning rate is initialized to some value and is scheduled to decrease. Importantly, the restart is warm as the optimization does not start from scratch but from the parameters to which the model converged during the last step. The key factor is that the learning rate is decreased with an aggressive cosine annealing schedule, which rapidly lowers the learning rate and looks like the following:&lt;/p&gt;

&lt;p&gt;\(\eta_t = \eta_{min}^i + \dfrac{1}{2}(\eta_{max}^i - \eta_{min}^i)(1 + \text{cos}(\dfrac{T_{cur}}{T_i}\pi)) \)&lt;/p&gt;

&lt;p&gt;where \(\eta_{min}^i\) and \(\eta_{max}^i\) are ranges for the learning rate during the \(i\)-th run, \(T_{cur}\) indicates how many epochs passed since the last restart, and \(T_i\) specifies the epoch of the next restart. The warm restart schedules for \(T_i=50\), \(T_i=100\), and \(T_i=200\) compared with regular learning rate annealing are shown in Figure 1.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/warm_restarts.png" style="width: 100%" title="Learning rate schedules with warm restarts" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 1: Learning rate schedules with warm restarts (Loshchilov and Hutter,  
 2017)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The high initial learning rate after a restart is used to essentially catapult the parameters out of the minimum to which they previously converged and to a different area of the loss surface. The aggressive annealing then enables the model to rapidly converge to a new and better solution. The authors empirically find that SGD with warm restarts requires 2 to 4 times fewer epochs than learning rate annealing and achieves comparable or better performance.&lt;/p&gt;

&lt;p&gt;Learning rate annealing with warm restarts is also known as cyclical learning rates and has been originally proposed by Smith (2017) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;]. Two more articles by students of &lt;a href="http://www.fast.ai/"&gt;fast.ai&lt;/a&gt; (which has recently started to teach this method) that discuss warm restarts and cyclical learning rates can be found &lt;a href="https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b"&gt;here&lt;/a&gt; and &lt;a href="http://teleported.in/posts/cyclic-learning-rate/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="snapshotensembles"&gt; Snapshot ensembles&lt;/h3&gt;

&lt;p&gt;Snapshot ensembles (Huang et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] are a clever, recent technique that uses warm restarts to assemble an ensemble essentially for free when training a single model. The method trains a single model until convergence with the cosine annealing schedule that we have seen above. It then saves the model parameters, performs a warm restart, and then repeats these steps \(M\) times. In the end, all saved model snapshots are ensembled. The common SGD optimization behaviour on an error surface compared to the behaviour of snapshot ensembling can be seen in Figure 2.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/snapshot_ensembles.png" style="width: 100%" title="Learning rate schedules with warm restarts" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 2: SGD vs. snapshot ensemble (Huang et al., 2017)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The success of ensembling in general relies on the diversity of the individual models in the ensemble. Snapshot ensembling thus relies on the cosine annealing schedule's ability to enable the model to converge to a different local optimum after every restart. The authors demonstrate that this holds in practice, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN.&lt;/p&gt;

&lt;h3 id="adamwithrestarts"&gt;Adam with restarts&lt;/h3&gt;

&lt;p&gt;Warm restarts did not work originally with Adam due to its dysfunctional weight decay, which we have seen before. After fixing weight decay, Loshchilov and Hutter (2017) similarly extend Adam to work with warm restarts. They set \(\eta_{min}^i=0\) and \(\eta_{max}^i=1\), which yields:&lt;/p&gt;

&lt;p&gt;\(\eta_t = 0.5 + 0.5 \: \text{cos}(\dfrac{T_{cur}}{T_i}\pi))\)&lt;/p&gt;

&lt;p&gt;They recommend to start with an initially small \(T_i\) (between \(1%\) and \(10%\) of the total number of epochs) and multiply it by a factor of \(T_{mult}\) (e.g. \(T_{mult}=2\)) at every restart.&lt;/p&gt;

&lt;h2 id="learningtooptimize"&gt;Learning to optimize&lt;/h2&gt;

&lt;p&gt;One of the most interesting papers of last year (and &lt;a href="https://www.reddit.com/r/MachineLearning/comments/5n53k7/d_results_from_the_best_paper_awards/"&gt;reddit's "Best paper name of 2016" winner&lt;/a&gt;) was a paper by Andrychowicz et al. (2016) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] where they train an LSTM optimizer to provide the updates to the main model during training. Unfortunately, learning a separate LSTM optimizer or even using a pre-trained LSTM optimizer for optimization greatly increases the complexity of model training.&lt;/p&gt;

&lt;p&gt;Another very influential learning-to-learn paper from this year uses an LSTM to generate model architectures in a domain-specific language (Zoph and Quoc, 2017) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;]. While the search process requires vast amounts of resources, the discovered architectures can be used as-is to replace their existing counterparts. This search process has proved effective and found architectures that achieve state-of-the-art results on language modeling and results competitive with the state-of-the-art on CIFAR-10.&lt;/p&gt;

&lt;p&gt;The same search principle can be applied to any other domain where key processes have been previously defined by hand. One such domain are optimization algorithms for Deep Learning. As we have seen before, optimization algorithms are more similar than they seem: All of them use a combination of an exponential moving average of past gradients (as in momentum) and of an exponential moving average of past squared gradients (as in Adadelta, RMSprop, and Adam) (Ruder, 2016) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Bello et al. (2017) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] define a domain-specific language that consists of primitives useful for optimization such as these exponential moving averages. They then sample an update rule from the space of possible update rules, use this update rule to train a model, and update the RNN controller based on the performance of the trained model on the test set. The full procedure can be seen in Figure 3. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/11/neural_optimizer_search.png" style="width: 100%" title="Neural Optimizer Search" alt="Optimization for Deep Learning Highlights in 2017"&gt;
&lt;figcaption&gt;Figure 3: Neural Optimizer Search (Bello et al., 2017)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In particular, they discover two update equations, PowerSign and AddSign. The update equation for PowerSign is the following:&lt;/p&gt;

&lt;p&gt;\( \theta_{t+1} = \theta_{t} - \alpha^{f(t)*
\text{sign}(g_t)*\text{sign}(m_t)}*g_t \)&lt;/p&gt;

&lt;p&gt;where \(\alpha\) is a hyperparameter that is often set to \(e\) or \(2\), \(f(t)\) is either \(1\) or a decay function that performs linear, cyclical or decay with restarts based on time step \(t\), and \(m_t\) is the moving average of past gradients. The common configuration uses \(\alpha=e\) and no decay.  We can observe that the update scales the gradient by \(\alpha^{f(t)}\) or \(1/\alpha^{f(t)}\) depending on whether the direction of the gradient and its moving average agree. This indicates that this momentum-like agreement between past gradients and the current one is a key piece of information for optimizing Deep Learning models.&lt;/p&gt;

&lt;p&gt;AddSign in turn is defined as follows:&lt;/p&gt;

&lt;p&gt;\( \theta_{t+1} = \theta_{t} - \alpha + f(t) * \text{sign}(g_t) * \text{sign}(m_t)) * g_t\)&lt;/p&gt;

&lt;p&gt;with \(\alpha\) often set to \(1\) or \(2\). Similar to the above, this time the update scales \(\alpha + f(t)\) or \(\alpha - f(t)\) again depending on the agreement of the direction of the gradients. The authors show that PowerSign and AddSign outperform Adam, RMSprop, and SGD with momentum on CIFAR-10 and transfer well to other tasks such as ImageNet classification and machine translation.&lt;/p&gt;

&lt;h2 id="understandinggeneralization"&gt;Understanding generalization&lt;/h2&gt;

&lt;p&gt;Optimization is closely tied to generalization as the minimum to which a model converges defines how well the model generalizes. Advances in optimization are thus closely correlated with theoretical advances in understanding the generalization behaviour of such minima and more generally of gaining a deeper understanding of generalization in Deep Learning.&lt;/p&gt;

&lt;p&gt;However, our understanding of the generalization behaviour of deep neural networks is still very shallow. Recent work showed that the number of possible local minima grows exponentially with the number of parameters (Kawaguchi, 2016) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;]. Given the huge number of parameters of current Deep Learning architectures, it still seems almost magical that such models converge to solutions that generalize well, in particular given that they can completely memorize random inputs (Zhang et al., 2017) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Keskar et al. (2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] identify the sharpness of a minimum as a source for poor generalization: In particular, they show that sharp minima found by batch gradient descent have high generalization error. This makes intuitive sense, as we generally would like our functions to be smooth and a sharp minima indicates a high irregularity in the corresponding error surface. However, more recent work suggests that sharpness may not be such a good indicator after all by showing that local minima that generalize well can be made arbitrarily sharp (Dinh et al., 2017) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/deep-learning-optimization-2017/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;]. A &lt;a href="https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important/answer/Eric-Jang?srid=dWc3"&gt;Quora answer by Eric Jang&lt;/a&gt; also discusses these articles.&lt;/p&gt;

&lt;p&gt;An &lt;a href="https://openreview.net/forum?id=r1iuQjxCZ"&gt;ICLR 2018 submission&lt;/a&gt; demonstrates through a series of ablation analyses that a model's reliance on single directions in activation space, i.e. the activation of single units or feature maps is a good predictor of its generalization performance. They show that this holds across models trained on different datasets and for different degrees of label corruption. They find that dropout does not help to resolve this, while batch normalization discourages single direction reliance.&lt;/p&gt;

&lt;p&gt;While these findings indicate that there is still much we do not know in terms of Optimization for Deep Learning, it is important to remember that convergence guarantees and a large body of work exists for convex optimization and that existing ideas and insights can also be applied to non-convex optimization to some extent. The large-scale optimization tutorial at NIPS 2016 provides an excellent overview of more theoretical work in this area (see the &lt;a href="https://www.di.ens.fr/~fbach/fbach_tutorial_vr_nips_2016.pdf"&gt;slides part 1&lt;/a&gt;, &lt;a href="http://suvrit.de/talks/vr_nips16_sra.pdf"&gt;part 2&lt;/a&gt;, and the &lt;a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Large-Scale-Optimization-Beyond-Stochastic-Gradient-Descent-and-Convexity"&gt;video&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id="conclusion"&gt; Conclusion&lt;/h2&gt;

&lt;p&gt;I hope that I was able to provide an impression of some of the compelling developments in optimization for Deep Learning over the past year. I've undoubtedly failed to mention many other approaches that are equally important and noteworthy. Please let me know in the comments below what I missed, where I made a mistake or misrepresented a method, or which aspect of optimization for Deep Learning you find particularly exciting or underexplored.&lt;/p&gt;

&lt;h2 id="hackernews"&gt; Hacker News&lt;/h2&gt;

&lt;p&gt;You can find the discussion of this post on HN &lt;a href="https://news.ycombinator.com/item?id=15839564"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="references"&gt;References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Robbins, H., &amp;amp; Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, 400-407. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Zhang, J., Mitliagkas, I., &amp;amp; Ré, C. (2017). YellowFin and the Art of Momentum Tuning. In arXiv preprint arXiv:1706.03471. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Denkowski, M., &amp;amp; Neubig, G. (2017). Stronger Baselines for Trustable Results in Neural Machine Translation. In Workshop on Neural Machine Translation (WNMT). Retrieved from &lt;a href="https://arxiv.org/abs/1706.09733"&gt;https://arxiv.org/abs/1706.09733&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Smith, S. L., Kindermans, P.-J., &amp;amp; Le, Q. V. (2017). Don’t Decay the Learning Rate, Increase the Batch Size. In arXiv preprint arXiv:1711.00489. Retrieved from &lt;a href="http://arxiv.org/abs/1711.00489"&gt;http://arxiv.org/abs/1711.00489&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Loshchilov, I., &amp;amp; Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts. In Proceedings of ICLR 2017. &lt;a href="https://doi.org/10.1002/fut"&gt;https://doi.org/10.1002/fut&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Bello, I., Zoph, B., Vasudevan, V., &amp;amp; Le, Q. V. (2017). Neural Optimizer Search with Reinforcement Learning. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Kawaguchi, K. (2016). Deep Learning without Poor Local Minima. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1605.07110"&gt;http://arxiv.org/abs/1605.07110&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp;amp; Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., &amp;amp; Tang, P. T. P. (2017). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In Proceedings of ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1609.04836"&gt;http://arxiv.org/abs/1609.04836&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Dinh, L., Pascanu, R., Bengio, S., &amp;amp; Bengio, Y. (2017). Sharp Minima Can Generalize For Deep Nets. In Proceedings of the 34th International Conference on Machine Learning. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Dozat, T., &amp;amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01734"&gt;http://arxiv.org/abs/1611.01734&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., &amp;amp; Recht, B. (2017). The Marginal Value of Adaptive Gradient Methods in Machine Learning. arXiv Preprint arXiv:1705.08292. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08292"&gt;http://arxiv.org/abs/1705.08292&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Laine, S., &amp;amp; Aila, T. (2017). Temporal Ensembling for Semi-Supervised Learning. In Proceedings of ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Huang, G., Liu, Z., Weinberger, K. Q., &amp;amp; van der Maaten, L. (2017). Densely Connected Convolutional Networks. In Proceedings of CVPR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Loshchilov, I., &amp;amp; Hutter, F. (2017). Fixing Weight Decay Regularization in Adam. arXiv Preprint arXi1711.05101. Retrieved from &lt;a href="http://arxiv.org/abs/1711.05101"&gt;http://arxiv.org/abs/1711.05101&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Melis, G., Dyer, C., &amp;amp; Blunsom, P. (2017). On the State of the Art of Evaluation in Neural Language Models. In arXiv preprint arXiv:1707.05589. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Merity, S., Shirish Keskar, N., &amp;amp; Socher, R. (2017). Regularizing and Optimizing LSTM Language Models. arXiv Preprint arXiv:1708.02182. Retrieved from &lt;a href="https://arxiv.org/pdf/1708.02182.pdf"&gt;https://arxiv.org/pdf/1708.02182.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Smith, Leslie N. "Cyclical learning rates for training neural networks." In Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on, pp. 464-472. IEEE, 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., &amp;amp; de Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04474"&gt;http://arxiv.org/abs/1606.04474&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Zoph, B., &amp;amp; Le, Q. V. (2017). Neural Architecture Search with Reinforcement Learning. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv Preprint arXiv:1609.04747. &lt;a href="http://ruder.io/deep-learning-optimization-2017/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Word embeddings in 2017: Trends and future directions</title><description>This post gives an overview of the deficiencies of pre-trained word embeddings in 2017 and how recent approaches have tried to resolve them.</description><link>http://ruder.io/word-embeddings-2017/</link><guid isPermaLink="false">f4b25277-c418-46b1-97cd-4773a7365d23</guid><category>word embeddings</category><category>natural language processing</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sat, 21 Oct 2017 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/10/semantic_change.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/10/semantic_change.png" alt="Word embeddings in 2017: Trends and future directions"&gt;&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#subwordlevelembeddings"&gt;Subword-level embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#oovhandling"&gt;OOV handling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#multisenseembeddings"&gt;Multi-sense embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#beyondwordsaspoints"&gt;Beyond words as points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#phrasesandmultiwordexpressions"&gt;Phrases and multi-word expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#bias"&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#temporaldimension"&gt;Temporal dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#lackoftheoreticalunderstanding"&gt;Lack of theoretical understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#taskanddomainspecificembeddings"&gt;Task and domain-specific embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#embeddingsformultiplelanguages"&gt;Embeddings for multiple languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#embeddingsbasedonothercontexts"&gt;Embeddings based on other contexts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The word2vec method based on skip-gram with negative sampling (Mikolov et al., 2013) [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;] was published in 2013 and had a large impact on the field, mainly through its accompanying software package, which enabled efficient training of dense word representations and a straightforward integration into downstream models. In some respects, we have come far since then: Word embeddings have established themselves as an integral part of Natural Language Processing (NLP) models. In other aspects, we might as well be in 2013 as we have not found ways to pre-train word embeddings that have managed to supersede the original word2vec.&lt;/p&gt;

&lt;p&gt;This post will focus on the deficiencies of word embeddings and how recent approaches have tried to resolve them. If not otherwise stated, this post discusses &lt;em&gt;pre-trained&lt;/em&gt; word embeddings, i.e. word representations that have been learned on a large corpus using word2vec and its variants. Pre-trained word embeddings are most effective if not millions of training examples are available (and thus transferring knowledge from a large unlabelled corpus is useful), which is true for most tasks in NLP. For an introduction to word embeddings, refer to &lt;a href="http://ruder.io/word-embeddings-1/"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="subwordlevelembeddings"&gt;Subword-level embeddings&lt;/h2&gt;

&lt;p&gt;Word embeddings have been augmented with subword-level information for many applications such as named entity recognition (Lample et al., 2016) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], part-of-speech tagging (Plank et al., 2016) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;], dependency parsing (Ballesteros et al., 2015; Yu &amp;amp; Vu, 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;], and language modelling (Kim et al., 2016) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;]. Most of these models employ a CNN or a BiLSTM that takes as input the characters of a word and outputs a &lt;em&gt;character-based&lt;/em&gt; word representation.&lt;/p&gt;

&lt;p&gt;For incorporating character information into pre-trained embeddings, however, character n-grams features have been shown to be more powerful than composition functions over individual characters (Wieting et al., 2016; Bojanowski et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;]. Character n-grams -- by far not a novel feature for text categorization (Cavnar et al., 1994) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] -- are particularly efficient and also form the basis of Facebook's fastText classifier (Joulin et al., 2016) [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;]. Embeddings learned using fastText are &lt;a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"&gt;available in 294 languages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Subword units based on byte-pair encoding have been found to be particularly useful for machine translation (Sennrich et al., 2016) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;] where they have replaced words as the standard input units. They are also useful for tasks with many unknown words such as entity typing (Heinzerling &amp;amp; Strube, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;], but have not been shown to be helpful yet for standard NLP tasks, where this is not a major concern. While they can be learned easily, it is difficult to see their advantage over character-based representations for most tasks (Vania &amp;amp; Lopez, 2017) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Another choice for using pre-trained embeddings that integrate character information is to leverage a state-of-the-art language model (Jozefowicz et al., 2016) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] trained on a large in-domain corpus, e.g. the 1 Billion Word Benchmark (a pre-trained Tensorflow model can be found &lt;a href="https://github.com/tensorflow/models/tree/master/research/lm_1b"&gt;here&lt;/a&gt;). While language modelling has been found to be useful for different tasks as auxiliary objective (Rei, 2017) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;], pre-trained language model embeddings have also been used to augment word embeddings (Peters et al., 2017) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;]. As we start to better understand how to pre-train and initialize our models, pre-trained language model embeddings are poised to become more effective. They might even supersede word2vec as the go-to choice for initializing word embeddings by virtue of having become more expressive and easier to train due to better frameworks and more computational resources over the last years.&lt;/p&gt;

&lt;h2 id="oovhandling"&gt;OOV handling&lt;/h2&gt;

&lt;p&gt;One of the main problems of using pre-trained word embeddings is that they are unable to deal with out-of-vocabulary (OOV) words, i.e. words that have not been seen during training. Typically, such words are set to the UNK token and are assigned the same vector, which is an ineffective choice if the number of OOV words is large. Subword-level embeddings as discussed in the last section are one way to mitigate this issue. Another way, which is effective for reading comprehension (Dhingra et al., 2017) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] is to assign OOV words their pre-trained word embedding, if one is available. &lt;/p&gt;

&lt;p&gt;Recently, different approaches have been proposed for generating embeddings for OOV words on-the-fly. Herbelot and Baroni (2017) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] initialize the embedding of OOV words as the sum of their context words and then rapidly refine only the OOV embedding with a high learning rate. Their approach is successful for a dataset that explicitly requires to model nonce words, but it is unclear if it can be scaled up to work reliably for more typical NLP tasks. Another interesting approach for generating OOV word embeddings is to train a character-based model to explicitly re-create pre-trained embeddings (Pinter et al., 2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;]. This is particularly useful in low-resource scenarios, where a large corpus is inaccessible and only pre-trained embeddings are available.&lt;/p&gt;

&lt;h2 id="evaluation"&gt; Evaluation&lt;/h2&gt;

&lt;p&gt;Evaluation of pre-trained embeddings has been a contentious issue since their inception as the commonly used evaluation via word similarity or analogy datasets has been shown to only correlate weakly with downstream performance (Tsvetkov et al., 2015) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;]. The &lt;a href="https://sites.google.com/site/repevalacl16/"&gt;RepEval Workshop at ACL 2016&lt;/a&gt; exclusively focused on better ways to evaluate pre-trained embeddings. As it stands, the consensus seems to be that -- while pre-trained embeddings can be evaluated on intrinsic tasks such as word similarity for comparison against previous approaches -- the best way to evaluate them is extrinsic evaluation on downstream tasks.&lt;/p&gt;

&lt;h2 id="multisenseembeddings"&gt;Multi-sense embeddings&lt;/h2&gt;

&lt;p&gt;A commonly cited criticism of word embeddings is that they are unable to capture polysemy. &lt;a href="http://wwwusers.di.uniroma1.it/~collados/Slides_ACL16Tutorial_SemanticRepresentation.pdf"&gt;A tutorial at ACL 2016&lt;/a&gt; outlined the work in recent years that focused on learning separate embeddings for multiple senses of a word (Neelakantan et al., 2014; Iacobacci et al., 2015; Pilehvar &amp;amp; Collier, 2016) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. However, most existing approaches for learning multi-sense embeddings solely evaluate on word similarity. Pilehvar et al. (2017) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] are one of the first to show results on topic categorization as a downstream task; while multi-sense embeddings outperform randomly initialized word embeddings in their experiments, they are outperformed by pre-trained word embeddings.&lt;/p&gt;

&lt;p&gt;Given the stellar results Neural Machine Translation systems using word embeddings have achieved in recent years (Johnson et al., 2016) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;], it seems that the current generation of models is expressive enough to contextualize and disambiguate words in context without having to rely on a dedicated disambiguation pipeline or multi-sense embeddings. However, we still need better ways to understand whether our models are actually able to sufficiently disambiguate words and how to improve this disambiguation behaviour if necessary.&lt;/p&gt;

&lt;h2 id="beyondwordsaspoints"&gt;Beyond words as points&lt;/h2&gt;

&lt;p&gt;While we might not need separate embeddings for every sense of each word for good downstream performance, reducing each word to a point in a vector space is unarguably overly simplistic and causes us to miss out on nuances that might be useful for downstream tasks. An interesting direction is thus to employ other representations that are better able to capture these facets. Vilnis &amp;amp; McCallum (2015) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] propose to model each word as a probability distribution rather than a point vector, which allows us to represent probability mass and uncertainty across certain dimensions. Athiwaratkun &amp;amp; Wilson (2017) [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] extend this approach to a multimodal distribution that allows to deal with polysemy, entailment, uncertainty, and enhances interpretability.&lt;/p&gt;

&lt;p&gt;Rather than altering the representation, the embedding space can also be changed to better represent certain features. Nickel and Kiela (2017) [&lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;], for instance, embed words in a hyperbolic space, to learn hierarchical representations. Finding other ways to represent words that incorporate linguistic assumptions or better deal with the characteristics of downstream tasks is a compelling research direction.&lt;/p&gt;

&lt;h2 id="phrasesandmultiwordexpressions"&gt;Phrases and multi-word expressions&lt;/h2&gt;

&lt;p&gt;In addition to not being able to capture multiple senses of words, word embeddings also fail to capture the meanings of phrases and multi-word expressions, which can be a function of the meaning of their constituent words, or have an entirely new meaning. Phrase embeddings have been proposed already in the original word2vec paper (Mikolov et al., 2013) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;] and there has been consistent work on learning better compositional and non-compositional phrase embeddings (Yu &amp;amp; Dredze, 2015; Hashimoto &amp;amp; Tsuruoka, 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;]. However, similar to multi-sense embeddings, explicitly modelling phrases has so far not shown significant improvements on downstream tasks that would justify the additional complexity. Analogously, a better understanding of how phrases are modelled in neural networks would pave the way to methods that augment the capabilities of our models to capture compositionality and non-compositionality of expressions.&lt;/p&gt;

&lt;h2 id="bias"&gt;Bias&lt;/h2&gt;

&lt;p&gt;Bias in our models is becoming a larger issue and we are only starting to understand its implications for training and evaluating our models. Even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent (Bolukbasi et al., 2016) [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;]. Understanding what other biases word embeddings capture and finding better ways to remove theses biases will be key to developing fair algorithms for natural language processing.&lt;/p&gt;

&lt;h2 id="temporaldimension"&gt;Temporal dimension&lt;/h2&gt;

&lt;p&gt;Words are a mirror of the zeitgeist and their meanings are subject to continuous change; current representations of words might differ substantially from the way these words where used in the past and will be used in the future. An interesting direction is thus to take into account the temporal dimension and the diachronic nature of words. This can allows us to reveal laws of semantic change (Hamilton et al., 2016; Bamler &amp;amp; Mandt, 2017; Dubossarsky et al., 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;], to model temporal word analogy or relatedness (Szymanski, 2017; Rosin et al., 2017) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;], or to capture the dynamics of semantic relations (Kutuzov et al., 2017) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="lackoftheoreticalunderstanding"&gt; Lack of theoretical understanding&lt;/h2&gt;

&lt;p&gt;Besides the insight that word2vec with skip-gram negative sampling implicitly factorizes a PMI matrix (Levy &amp;amp; Goldberg, 2014) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;], there has been comparatively little work on gaining a better theoretical understanding of the word embedding space and its properties, e.g. that summation captures analogy relations. Arora et al. (2016) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;] propose a new generative model for word embeddings, which treats corpus generation as a random walk of a discourse vector and establishes some theoretical motivations regarding the analogy behaviour. Gittens et al. (2017) [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;] provide a more thorough theoretical justification of additive compositionality and show that skip-gram word vectors are optimal in an information-theoretic sense. Mimno &amp;amp; Thompson (2017) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] furthermore reveal an interesting relation between word embeddings and the embeddings of context words, i.e. that they are not evenly dispersed across the vector space, but occupy a narrow cone that is diametrically opposite to the context word embeddings. Despite these additional insights, our understanding regarding the location and properties of word embeddings is still lacking and more theoretical work is necessary. &lt;/p&gt;

&lt;h2 id="taskanddomainspecificembeddings"&gt; Task and domain-specific embeddings&lt;/h2&gt;

&lt;p&gt;One of the major downsides of using pre-trained embeddings is that the news data used for training them is often very different from the data on which we would like to use them. In most cases, however, we do not have access to millions of unlabelled documents in our target domain that would allow for pre-training good embeddings from scratch. We would thus like to be able to adapt embeddings pre-trained on large news corpora, so that they capture the characteristics of our target domain, but still retain all relevant existing knowledge. Lu &amp;amp; Zheng (2017) [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;] proposed a regularized skip-gram model for learning such cross-domain embeddings. In the future, we will need even better ways to adapt pre-trained embeddings to new domains or to incorporate the knowledge from multiple relevant domains.&lt;/p&gt;

&lt;p&gt;Rather than adapting to a new domain, we can also use existing knowledge encoded in semantic lexicons to augment pre-trained embeddings with information that is relevant for our task. An effective way to inject such relations into the embedding space is retro-fitting (Faruqui et al., 2015) [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;], which has been expanded to other resources such as ConceptNet (Speer et al., 2017) [&lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;] and extended with an intelligent selection of positive and negative examples (Mrkšić et al., 2017) [&lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. Injecting additional prior knowledge into word embeddings such as monotonicity (You et al., 2017) [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;], word similarity (Niebler et al., 2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;], task-related grading or intensity, or logical relations is an important research direction that will allow to make our models more robust.&lt;/p&gt;

&lt;p&gt;Word embeddings are useful for a wide variety of applications beyond NLP such as information retrieval, recommendation, and link prediction in knowledge bases, which all have their own task-specific approaches. Wu et al. (2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;] propose a general-purpose model that is compatible with many of these applications and can serve as a strong baseline.&lt;/p&gt;

&lt;h2 id="embeddingsformultiplelanguages"&gt;Embeddings for multiple languages&lt;/h2&gt;

&lt;p&gt;As NLP models are being increasingly employed and evaluated on multiple languages, creating multilingual word embeddings is becoming a more important issue and has received increased interest over recent years. A promising direction is to develop methods that learn cross-lingual representations with as few parallel data as possible, so that they can be easily applied to learn representations even for low-resource languages. For a recent survey in this area, refer to Ruder et al. (2017) [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="embeddingsbasedonothercontexts"&gt;Embeddings based on other contexts&lt;/h2&gt;

&lt;p&gt;Word embeddings are typically learned only based on the window of surrounding context words. Levy &amp;amp; Goldberg (2014) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] have shown that dependency structures can be used as context to capture more syntactic word relations; Köhn (2015) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;] finds that such dependency-based embeddings perform best for a particular multilingual evaluation method that clusters embeddings along different syntactic features. &lt;/p&gt;

&lt;p&gt;Melamud et al. (2016) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;] observe that different context types work well for different downstream tasks and that simple concatenation of word embeddings learned with different context types can yield further performance gains. Given the recent success of incorporating graph structures into neural models for different tasks as -- for instance -- exhibited by graph-convolutional neural networks (Bastings et al., 2017; Marcheggiani &amp;amp; Titov, 2017) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;], we can conjecture that incorporating such structures for learning embeddings for downstream tasks may also be beneficial.&lt;/p&gt;

&lt;p&gt;Besides selecting context words differently, additional context may also be used in other ways: Tissier et al. (2017) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/word-embeddings-2017/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] incorporate co-occurrence information from dictionary definitions into the negative sampling process to move related works closer together and prevent them from being used as negative samples. We can think of topical or relatedness information derived from other contexts such as article headlines or Wikipedia intro paragraphs that could similarly be used to make the representations more applicable to a particular downstream task.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It is nice to see that as a community we are progressing from applying word embeddings to every possible problem to gaining a more principled, nuanced, and practical understanding of them. This post was meant to highlight some of the current trends and future directions for learning word embeddings that I found most compelling. I've undoubtedly failed to mention many other areas that are equally important and noteworthy. Please let me know in the comments below what I missed, where I made a mistake or misrepresented a method, or just which aspect of word embeddings you find particularly exciting or unexplored.&lt;/p&gt;

&lt;h1 id="hackernews"&gt;Hacker News&lt;/h1&gt;

&lt;p&gt;Refer to the &lt;a href="https://news.ycombinator.com/item?id=15521957"&gt;discussion on Hacker News&lt;/a&gt; for some more insights on word embeddings.&lt;/p&gt;

&lt;h2 id="otherblogpostsonwordembeddings"&gt;Other blog posts on word embeddings&lt;/h2&gt;

&lt;p&gt;If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;On word embeddings - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;On word embeddings - Part 2: Approximating the softmax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/secret-word2vec/index.html"&gt;On word embeddings - Part 3: The secret ingredients of word2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/cross-lingual-embeddings/index.html"&gt;Unofficial Part 4: A survey of cross-lingual embedding models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="references"&gt;References&lt;/h2&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Cavnar, W. B., Trenkle, J. M., &amp;amp; Mi, A. A. (1994). N-Gram-Based Text Categorization. Ann Arbor MI 48113.2, 161–175. &lt;a href="https://doi.org/10.1.1.53.9367"&gt;https://doi.org/10.1.1.53.9367&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Wieting, J., Bansal, M., Gimpel, K., &amp;amp; Livescu, K. (2016). Charagram: Embedding Words and Sentences via Character n-grams. Retrieved from &lt;a href="http://arxiv.org/abs/1607.02789"&gt;http://arxiv.org/abs/1607.02789&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Bojanowski, P., Grave, E., Joulin, A., &amp;amp; Mikolov, T. (2017). Enriching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1607.04606"&gt;http://arxiv.org/abs/1607.04606&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Joulin, A., Grave, E., Bojanowski, P., &amp;amp; Mikolov, T. (2016). Bag of Tricks for Efficient Text Classification. arXiv Preprint arXiv:1607.01759. Retrieved from &lt;a href="http://arxiv.org/abs/1607.01759"&gt;http://arxiv.org/abs/1607.01759&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 1756–1765). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv Preprint arXiv:1602.02410. Retrieved from &lt;a href="http://arxiv.org/abs/1602.02410"&gt;http://arxiv.org/abs/1602.02410&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp;amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In NAACL-HLT 2016. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Yu, X., &amp;amp; Vu, N. T. (2017). Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 672–678). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from &lt;a href="http://arxiv.org/abs/1508.06615"&gt;http://arxiv.org/abs/1508.06615&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1508.07909"&gt;http://arxiv.org/abs/1508.07909&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Heinzerling, B., &amp;amp; Strube, M. (2017). BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages. Retrieved from &lt;a href="http://arxiv.org/abs/1710.02187"&gt;http://arxiv.org/abs/1710.02187&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Dhingra, B., Liu, H., Salakhutdinov, R., &amp;amp; Cohen, W. W. (2017). A Comparative Study of Word Embeddings for Reading Comprehension. arXiv preprint arXiv:1703.00993. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Herbelot, A., &amp;amp; Baroni, M. (2017). High-risk learning: acquiring new word vectors from tiny data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Pinter, Y., Guthrie, R., &amp;amp; Eisenstein, J. (2017). Mimicking Word Embeddings using Subword RNNs. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.06961"&gt;http://arxiv.org/abs/1707.06961&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Ballesteros, M., Dyer, C., &amp;amp; Smith, N. A. (2015). Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs. In Proceedings of EMNLP 2015. &lt;a href="https://doi.org/10.18653/v1/D15-1041"&gt;https://doi.org/10.18653/v1/D15-1041&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Neelakantan, A., Shankar, J., Passos, A., &amp;amp; Mccallum, A. (2014). Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space. In Proceedings fo (pp. 1059–1069). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Iacobacci, I., Pilehvar, M. T., &amp;amp; Navigli, R. (2015). SensEmbed: Learning Sense Embeddings for Word and Relational Similarity. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (pp. 95–105). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Pilehvar, M. T., &amp;amp; Collier, N. (2016). De-Conflated Semantic Representations. In Proceedings of EMNLP. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Tsvetkov, Y., Faruqui, M., Ling, W., Lample, G., &amp;amp; Dyer, C. (2015). Evaluation of Word Vector Representations by Subspace Alignment. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015, 2049–2054. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Pilehvar, M. T., Camacho-Collados, J., Navigli, R., &amp;amp; Collier, N. (2017). Towards a Seamless Integration of Word Senses into Downstream NLP Applications. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1857–1869). &lt;a href="https://doi.org/10.18653/v1/P17-1170"&gt;https://doi.org/10.18653/v1/P17-1170&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Vilnis, L., &amp;amp; McCallum, A. (2015). Word Representations via Gaussian Embedding. ICLR. Retrieved from &lt;a href="http://arxiv.org/abs/1412.6623"&gt;http://arxiv.org/abs/1412.6623&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Athiwaratkun, B., &amp;amp; Wilson, A. G. (2017). Multimodal Word Distributions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., &amp;amp; Kalai, A. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In 30th Conference on Neural Information Processing Systems (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1607.06520"&gt;http://arxiv.org/abs/1607.06520&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Hamilton, W. L., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (pp. 1489–1501). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Bamler, R., &amp;amp; Mandt, S. (2017). Dynamic Word Embeddings via Skip-Gram Filtering. In Proceedings of ICML 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08359"&gt;http://arxiv.org/abs/1702.08359&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Dubossarsky, H., Grossman, E., &amp;amp; Weinshall, D. (2017). Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models. In Conference on Empirical Methods in Natural Language Processing (pp. 1147–1156). Retrieved from &lt;a href="http://aclweb.org/anthology/D17-1119"&gt;http://aclweb.org/anthology/D17-1119&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Szymanski, T. (2017). Temporal Word Analogies : Identifying Lexical Replacement with Diachronic Word Embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 448–453). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Rosin, G., Radinsky, K., &amp;amp; Adar, E. (2017). Learning Word Relatedness over Time. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="https://arxiv.org/pdf/1707.08081.pdf"&gt;https://arxiv.org/pdf/1707.08081.pdf&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Kutuzov, A., Velldal, E., &amp;amp; Øvrelid, L. (2017). Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.08660"&gt;http://arxiv.org/abs/1707.08660&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from &lt;a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization"&gt;http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Arora, S., Li, Y., Liang, Y., Ma, T., &amp;amp; Risteski, A. (2016). A Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4, 385–399. Retrieved from &lt;a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204"&gt;https://transacl.org/ojs/index.php/tacl/article/viewFile/742/204&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Gittens, A., Achlioptas, D., &amp;amp; Mahoney, M. W. (2017). Skip-Gram – Zipf + Uniform = Vector Additivity. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 69–76). &lt;a href="https://doi.org/10.18653/v1/P17-1007"&gt;https://doi.org/10.18653/v1/P17-1007&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Mimno, D., &amp;amp; Thompson, L. (2017). The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2863–2868). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Yu, M., &amp;amp; Dredze, M. (2015). Learning Composition Models for Phrase Embeddings. Transactions of the ACL, 3, 227–242. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Hashimoto, K., &amp;amp; Tsuruoka, Y. (2016). Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings. ACL, 205–215. Retrieved from &lt;a href="http://arxiv.org/abs/1603.06067"&gt;http://arxiv.org/abs/1603.06067&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Lu, W., &amp;amp; Zheng, V. W. (2017). A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2888–2894). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Faruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., &amp;amp; Smith, N. A. (2015). Retrofitting Word Vectors to Semantic Lexicons. In NAACL 2015. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Mrkšić, N., Vulić, I., Séaghdha, D. Ó., Leviant, I., Reichart, R., Gašić, M., … Young, S. (2017). Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints. TACL. Retrieved from &lt;a href="http://arxiv.org/abs/1706.00374"&gt;http://arxiv.org/abs/1706.00374&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Ruder, S., Vulić, I., &amp;amp; Søgaard, A. (2017). A Survey of Cross-lingual Word Embedding Models Sebastian. arXiv preprint arXiv:1706.04902. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04902"&gt;http://arxiv.org/abs/1706.04902&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Dependency-Based Word Embeddings. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), 302–308. &lt;a href="https://doi.org/10.3115/v1/P14-2050"&gt;https://doi.org/10.3115/v1/P14-2050&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Köhn, A. (2015). What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 17-21 September 2015, (2014), 2067–2073. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Melamud, O., McClosky, D., Patwardhan, S., &amp;amp; Bansal, M. (2016). The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of NAACL-HLT 2016 (pp. 1030–1040). Retrieved from &lt;a href="http://arxiv.org/abs/1601.00893"&gt;http://arxiv.org/abs/1601.00893&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., &amp;amp; Sima’an, K. (2017). Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Marcheggiani, D., &amp;amp; Titov, I. (2017). Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Mikolov, T., Corrado, G., Chen, K., &amp;amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Vania, C., &amp;amp; Lopez, A. (2017). From Characters to Words to in Between: Do We Capture Morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 2016–2027). &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;You, S., Ding, D., Canini, K., Pfeifer, J., &amp;amp; Gupta, M. (2017). Deep Lattice Networks and Partial Monotonic Functions. In 31st Conference on Neural Information Processing Systems (NIPS 2017). Retrieved from &lt;a href="http://arxiv.org/abs/1709.06680"&gt;http://arxiv.org/abs/1709.06680&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Nickel, M., &amp;amp; Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. arXiv Preprint arXiv:1705.08039. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08039"&gt;http://arxiv.org/abs/1705.08039&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Niebler, T., Becker, M., Pölitz, C., &amp;amp; Hotho, A. (2017). Learning Semantic Relatedness From Human Feedback Using Metric Learning. In Proceedings of ISWC 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1705.07425"&gt;http://arxiv.org/abs/1705.07425&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Wu, L., Fisch, A., Chopra, S., Adams, K., Bordes, A., &amp;amp; Weston, J. (2017). StarSpace: Embed All The Things! arXiv preprint arXiv:1709.03856. &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Speer, R., Chin, J., &amp;amp; Havasi, C. (2017). ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. In AAAI 31 (pp. 4444–4451). Retrieved from &lt;a href="http://arxiv.org/abs/1612.03975"&gt;http://arxiv.org/abs/1612.03975&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Tissier, J., Gravier, C., &amp;amp; Habrard, A. (2017). Dict2Vec : Learning Word Embeddings using Lexical Dictionaries. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://aclweb.org/anthology/D17-1024"&gt;http://aclweb.org/anthology/D17-1024&lt;/a&gt; &lt;a href="http://ruder.io/word-embeddings-2017/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Cover image credit: Hamilton et al. (2016)&lt;/p&gt;</content:encoded></item><item><title>Multi-Task Learning Objectives for Natural Language Processing</title><description>An overview of auxiliary tasks and objectives that have been used for multi-task learning for natural language processing.</description><link>http://ruder.io/multi-task-learning-nlp/</link><guid isPermaLink="false">2e84d768-9785-4ce7-bf05-7f4773fc34b2</guid><category>natural language processing</category><category>nlp</category><category>multi-task learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sun, 24 Sep 2017 13:42:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/09/soft_parameter_sharing.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/09/soft_parameter_sharing.png" alt="Multi-Task Learning Objectives for Natural Language Processing"&gt;&lt;p&gt;In a &lt;a href="http://ruder.io/multi-task/index.html"&gt;previous blog post&lt;/a&gt;, I discussed how multi-task learning (MTL) can be used to improve the performance of a model by leveraging a related task. Multi-task learning consists of two main components: a) The architecture used for learning and b) the auxiliary task(s) that are trained jointly. Both facets still have a lot of room for improvement. In addition, multi-task learning has the potential to be a key technique on the path to more robust models that learn from limited data: Training a model to acquire proficiency in performing a wide range of NLP tasks would allow us to induce representations, which should be useful for transferring knowledge to many other tasks, as outlined in &lt;a href="http://ruder.io/transfer-learning/index.html"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the way to this goal, we first need to learn more about the relationships between our tasks, what we can learn from each, and how to combine them most effectively. Most of the existing theory in MTL has focused on homogeneous tasks, i.e. tasks that are variations of the same classification or regression problem, such as classifying individual MNIST digits. These guarantees, however, do not hold for the heterogeneous tasks to which MTL is most often applied in Natural Language Processing (NLP) and Computer Vision.&lt;/p&gt;

&lt;p&gt;There have been some recent studies looking into when multi-task learning between different NLP tasks works but we still do not understand very well which tasks are useful. To this end, as inspiration, I will give an overview in the following of different approaches for multi-task learning for NLP. I will focus on the second component of multi-task learning; instead of discussing &lt;em&gt;how&lt;/em&gt; a model is trained, as most architectures only differ in which layers they share, I will concentrate on the auxiliary tasks and objectives that are used for learning.&lt;/p&gt;

&lt;p&gt;This post has two main parts: In the first part, I will talk about artificial tasks that can be used as auxiliary objectives for MTL. In the second part, I will focus on common NLP tasks and discuss which other NLP tasks have benefited them.&lt;/p&gt;

&lt;h1 id="artificialauxiliaryobjectives"&gt;Artificial auxiliary objectives&lt;/h1&gt;

&lt;p&gt;Multi-task learning is all about coming up with ways to add a suitable bias to your model. Incorporating artificial auxiliary tasks that cleverly complement your target task is arguably one of the most ingenious and fun ways to do MTL. It is a feature-engineering of sorts: instead of engineering the features, you are engineering the auxiliary task you optimize. Similarly to feature engineering, domain expertise is therefore required as we will see in the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language modelling&lt;/strong&gt; Language modelling has been shown to be beneficial for many NLP tasks and can be incorporated in various ways. Word embeddings pre-trained by word2vec have been shown to beneficial -- as is known, word2vec approximates the language modelling objective; languages models have been used to pre-train MT and sequence-to-sequence models [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;]; contextual language model embeddings have also been found useful for many tasks [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;]. In this context, we can also treat language modelling as an auxiliary task that is learned together with the main task. Rei (2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] shows that this improves performance on several sequence labelling tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conditioning the initial state&lt;/strong&gt; &amp;nbsp; The initial state of a recurrent neural network is typically initialized to a \(0\) vector. According to a &lt;a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf"&gt;lecture by Hinton in 2013&lt;/a&gt;, it is beneficial to learn the initial state just like any other sets of weights. While a learned state will be more helpful than a \(0\) vector it will be independent of the sequence and thus unable to adapt. Weng et al. (2017) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] propose to add a suitable bias to the initial encoder and decoder states for NMT by training it to predict the words in the sentence. In this sense, this objective can essentially be seen as a &lt;em&gt;language modelling objective for the initial state&lt;/em&gt; and might thus be helpful for other tasks. Similarly, we can think of other task-specific biases that could be encoded in the initial state to aid learning: A sentiment model might benefit from knowing about the general audience response to a movie or whether a user is more likely to be sarcastic while a parser might be able to leverage prior knowledge of the domain's tree depth or complexity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adversarial loss&lt;/strong&gt; &amp;nbsp; An auxiliary adversarial loss was first found to be useful for domain adaptation [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;], where it is used to learn domain-invariant representations by rendering the model unable to distinguish between different domains. This is typically done by adding a gradient reversal layer that reverses the sign of the gradient during back-propagation, which in turn leads to a maximization rather than a minimization of the adversarial loss. It is not to be confused with adversarial examples [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;], which significantly increase the model's loss typically via small perturbations to its input; adversarial training [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;], which trains a model to correctly classify such examples; or Generative Adversarial Networks, which are trained to generate some output representation. An adversarial loss can be added to many tasks in order to learn task-independent representations [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;]. It can also be used to ignore certain features of the input that have been found to be detrimental to generalization, such as data-specific properties that are unlikely to generalize. Finally, an adversarial auxiliary task might also help to combat bias and ensure more privacy by encouraging the model to learn representations, which do not contain information that would allow the reconstruction of sensitive user attributes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Predicting data statistics&lt;/strong&gt; &amp;nbsp; An auxiliary loss can also be to predict certain underlying statistics of the training data. In contrast to the adversarial loss, which tries to make the model oblivious to certain features, this auxiliary task explicitly encourages the model to predict certain data statistics. Plank et al. (2016) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;] predict the log frequency of a word as an auxiliary task for language modelling. Intuitively, this makes the representation predictive of frequency, which encourages the model to not share representations between common and rare words, which benefits the handling of rare tokens. Another facet of this auxiliary task is to predict attributes of the user, such as their gender, which has been shown to be beneficial for predicting mental health conditions [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;] or other demographic information [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;]. We can think of other statistics that might be beneficial for a model to encode, such as the frequency of POS tags, parsing structures, or entities, the preferences of users, a sentence's coverage for summarization, or even a user's website usage patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning the inverse&lt;/strong&gt; &amp;nbsp; Another auxiliary task that might be useful in many circumstances is to learn the inverse of the task together with the main task. A popular example of this framework is CycleGAN [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;], which can &lt;a href="https://github.com/junyanz/CycleGAN"&gt;generate photos from paintings&lt;/a&gt;. An inverse auxiliary loss, however, is applicable to many other tasks: MT might be the most intuitive, as every translation direction such as English-&gt;French directly provides data for the inverse direction, as Xia et al. (2016) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] demonstrate. Xia et al. (2017) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;] show that this has applications not only to MT, but also to image classification (with image generation as its inverse) and sentiment classification (paired with sentence generation). For multimodal translation, Elliott and Kádár (2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] jointly learn an inverse task by predicting image representations. It is not difficult to think of inverse complements for many other tasks: Entailment has hypothesis generation; video captioning has video generation; speech recognition has speech synthesis, etc. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Predicting what should be there&lt;/strong&gt; &amp;nbsp; For many tasks, where a model has to pick up on certain features of the training data, we can focus the model's attention on these characteristics by encouraging it explicitly to predict them. For sentiment analysis, for instance, Yu and Jiang (2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;] predict whether the sentence contains a positive or negative domain-independent sentiment word, which sensitizes the model towards the sentiment of the words in the sentence. For name error detection, Cheng et al. (2015) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;] predict if a sentence contains a name. We can envision similar auxiliary tasks that might be useful for other tasks: Predicting whether certain entities occur in a sentence might be useful for relation extraction; predicting whether a headline contains certain lurid terms might help for clickbait detection, while predicting whether an emotion word occurs in the sentence might benefit emotion detection. In summary, this auxiliary task should be helpful whenever a task includes certain highly predictive terms or features.&lt;/p&gt;

&lt;h1 id="jointtrainingofexistingnlptasks"&gt;Joint training of existing NLP tasks&lt;/h1&gt;

&lt;p&gt;In this second section, we will now look at existing NLP tasks, which have been used to improve the performance of a main task. While certain tasks such as chunking and semantic tagging have been found to be useful for many tasks [&lt;sup id="fnref:60"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:60" rel="footnote"&gt;60&lt;/a&gt;&lt;/sup&gt;], the choice whether to use a particular auxiliary task largely depends on characteristics of the main task. In the following, I will thus highlight different strategies and rationals that were used to select auxiliary tasks for many common tasks in NLP:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Speech recognition&lt;/strong&gt; &amp;nbsp; Recent multi-task learning approaches for automatic speech recognition (ASR) typically use additional supervision signals that are available in the speech recognition pipeline as auxiliary tasks to train an ASR model end-to-end. Phonetic recognition and frame-level state classification can be used as auxiliary tasks to induce helpful intermediate representations. Toshniwal et al. (2017) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] find that positioning the auxiliary loss at an intermediate layer improves performance. Similarly, Arık et al. (2017) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;] predict the phoneme duration and frequency profile as auxiliary tasks for speech synthesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine translation&lt;/strong&gt; &amp;nbsp; The main benefit MTL has brought to machine translation (MT) is by jointly training translation models from and to different languages: Dong et al. (2015) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] jointly train the decoders; Zoph and Knight (2016) [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] jointly train the encoders, while Johnson et al. (2016) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] jointly train both encoders and decoders; Malaviya et al. (2017) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] train one model to translate from 1017 languages into English.&lt;/p&gt;

&lt;p&gt;Other tasks have also shown to be useful for MT: Luong et al. (2015) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] show gains using parsing and image captioning as auxiliary tasks; Niehues and Cho (2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] combine NMT with POS tagging and NER; Wu et al. (2017) [&lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;] jointly model the target word sequence and its dependency tree structure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multilingual tasks&lt;/strong&gt; &amp;nbsp; Similarly to MT, it can often be beneficial to jointly train models for different languages: Gains have been shown for dependency parsing [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;], named entity recognition [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;], part-of-speech tagging [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;], document classification [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;], discourse segmentation [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;], and sequence tagging [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language grounding&lt;/strong&gt; &amp;nbsp; For grounding language in images or videos, it is often useful to enable the model to learn causal relationships in the data. For video captioning, Pasunuru and Bansal (2017) [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] jointly learn to predict the next frame in the video and to predict entailment, while Hermann et al. (2017) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;] also predict the next frame in a video and the words that represent the visual state for language learning in a simulated environment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Semantic parsing&lt;/strong&gt; &amp;nbsp; For a task where multiple label sets or formalisms are available such as for semantic parsing, an interesting MTL strategy is to learn these formalisms together: To this end, Guo et al. (2016) [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] jointly train on multi-typed treebanks; Peng et al. (2017) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] learn three semantic dependency graph formalisms simultaneously; Fan et al. (2017) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] jointly learn different Alexa-based semantic parsing formalisms; and Zhao and Huang (2017) [&lt;sup id="fnref:57"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:57" rel="footnote"&gt;57&lt;/a&gt;&lt;/sup&gt;] jointly train a syntactic and a discourse parser. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation learning&lt;/strong&gt; &amp;nbsp; For learning general-purpose representations, the challenge often is in defining the objective. Most existing representation learning models have been based on a single loss function, such as predicting the next word [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;] or sentence [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;] or training on a certain task such as entailment [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] or MT [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. Rather than learning representations based on a single loss, intuitively, representations should become more general as more tasks are used to learn them. As an example of this strategy, Hashimoto et al. (2017) [&lt;sup id="fnref:59"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:59" rel="footnote"&gt;59&lt;/a&gt;&lt;/sup&gt;] jointly train a model on multiple NLP tasks, while Jernite et al. (2017) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;] propose several discourse-based artificial auxiliary tasks for sentence representation learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question answering&lt;/strong&gt; &amp;nbsp; For question answering (QA) and reading comprehension, it is beneficial to learn the different parts of a more complex end-to-end model together: Choi et al. (2017) [&lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;] jointly learn a sentence selection and answer generation model, while Wang et al. (2017) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] jointly train a ranking and reader model for open-domain QA. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Information retrieval&lt;/strong&gt; &amp;nbsp; For relation extraction, information related to different relations or roles can often be shared. To this end, Jiang (2009) [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] jointly learn linear models between different relation types; Yang and Mitchell (2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;] jointly predict semantic role labels and relations; Katiyar and Cardie (2017) [&lt;sup id="fnref:58"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:58" rel="footnote"&gt;58&lt;/a&gt;&lt;/sup&gt;] jointly extract entities and relations; and Liu et al. (2015) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] jointly train domain classification and web search ranking. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Chunking&lt;/strong&gt; &amp;nbsp; Chunking has been shown to benefit from being jointly trained with low-level tasks such as POS tagging [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt; &amp;nbsp; Besides the tasks mentioned above, various other tasks have been shown to benefit from MTL: Balikas and Moura (2017) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] jointly train coarse-grained and fine-grained sentiment analysis; Luo et al. (2017) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;] jointly predict charges and extract articles; Augenstein and Søgaard (2017) [&lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;] use several auxiliary tasks for keyphrase boundary detection; and Isonuma et al. (2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/multi-task-learning-nlp/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;] pair sentence extraction with document classification.&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I hope this blog post was able to provide you with some insight with regard to which strategies are employed to select auxiliary tasks and objectives for multi-task learning in NLP. As I mentioned &lt;a href="http://ruder.io/multi-task/index.html"&gt;before&lt;/a&gt;, multi-task learning can be very broadly defined. I have tried to provide as broad of an overview as possible but I still likely have omitted many relevant approaches. If you are aware of an approach that provides a valuable perspective that is not represented here, please let me know in the comments below.&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Weng, R., Huang, S., Zheng, Z., Dai, X., &amp;amp; Chen, J. (2017). Neural Machine Translation with Word Predictions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Ramachandran, P., Liu, P. J., &amp;amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Peters, M. E., Ammar, W., Bhagavatula, C., &amp;amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 1756–1765). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17, 1–35. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf"&gt;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp;amp; Fergus, R. (2014). Intriguing properties of neural networks. In ICLR 2014. Retrieved from &lt;a href="http://arxiv.org/abs/1312.6199"&gt;http://arxiv.org/abs/1312.6199&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Miyato, T., Dai, A. M., &amp;amp; Goodfellow, I. (2016). Virtual Adversarial Training for Semi-Supervised Text Classification. Retrieved from &lt;a href="http://arxiv.org/abs/1605.07725"&gt;http://arxiv.org/abs/1605.07725&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Liu, P., Qiu, X., &amp;amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1704.05742"&gt;http://arxiv.org/abs/1704.05742&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Toshniwal, S., Tang, H., Lu, L., &amp;amp; Livescu, K. (2017). Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition. Retrieved from &lt;a href="http://arxiv.org/abs/1704.01631"&gt;http://arxiv.org/abs/1704.01631&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Phoneme duration and frequency profile &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., … Shoeybi, M. (2017). Deep Voice: Real-time Neural Text-to-Speech. In ICML 2017.

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Dong, D., Wu, H., He, W., Yu, D., &amp;amp; Wang, H. (2015). Multi-Task Learning for Multiple Language Translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (pp. 1723–1732). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Zoph, B., &amp;amp; Knight, K. (2016). Multi-Source Neural Translation. NAACL, 30–34. Retrieved from &lt;a href="http://arxiv.org/abs/1601.00710"&gt;http://arxiv.org/abs/1601.00710&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Malaviya, C., Neubig, G., &amp;amp; Littell, P. (2017). Learning Language Representations for Typology Prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.09569"&gt;http://arxiv.org/abs/1707.09569&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., &amp;amp; Kaiser, L. (2015). Multi-task Sequence to Sequence Learning. In arXiv preprint arXiv:1511.06114. Retrieved from &lt;a href="http://arxiv.org/abs/1511.06114"&gt;http://arxiv.org/abs/1511.06114&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Niehues, J., &amp;amp; Cho, E. (2017). Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning. In WMT 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1708.00993"&gt;http://arxiv.org/abs/1708.00993&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Elliott, D., &amp;amp; Kádár, Á. (2017). Imagination improves Multimodal Translation. Retrieved from &lt;a href="http://arxiv.org/abs/1705.04350"&gt;http://arxiv.org/abs/1705.04350&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Yu, J., &amp;amp; Jiang, J. (2016). Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 236–246. Retrieved from &lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1023.pdf"&gt;http://www.aclweb.org/anthology/D/D16/D16-1023.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Balikas, G., &amp;amp; Moura, S. (2017). Multitask Learning for Fine-Grained Twitter Sentiment Analysis. In International ACM SIGIR Conference on Research and Development in Information Retrieval 2017. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Duong, L., Cohn, T., Bird, S., &amp;amp; Cook, P. (2015). Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), 845–850. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Gillick, D., Brunk, C., Vinyals, O., &amp;amp; Subramanya, A. (2016). Multilingual Language Processing From Bytes. NAACL, 1296–1306. Retrieved from &lt;a href="http://arxiv.org/abs/1512.00103"&gt;http://arxiv.org/abs/1512.00103&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Fang, M., &amp;amp; Cohn, T. (2017). Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Popescu-belis, A. (2017). Multilingual Hierarchical Attention Networks for Document Classification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Braud, C., Lacroix, O., &amp;amp; Søgaard, A. (2017). Cross-lingual and cross-domain discourse segmentation of entire documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Yang, Z., Salakhutdinov, R., &amp;amp; Cohen, W. (2016). Multi-Task Cross-Lingual Sequence Tagging from Scratch. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Ammar, W., Mulcaire, G., Ballesteros, M., Dyer, C., &amp;amp; Smith, N. A. (2016). One Parser, Many Languages. Transactions of the Association for Computational Linguistics, Vol. 4, Pp. 431–444, 2016, 4, 431–444. Retrieved from &lt;a href="http://arxiv.org/abs/1602.01595"&gt;http://arxiv.org/abs/1602.01595&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Jiang, J. (2009). Multi-task transfer learning for weakly-supervised relation extraction. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, (August), 1012–1020. &lt;a href="https://doi.org/10.3115/1690219.1690288"&gt;https://doi.org/10.3115/1690219.1690288&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Pasunuru, R., &amp;amp; Bansal, M. (2017). Multi-Task Video Captioning with Video and Entailment Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Guo, J., Che, W., Wang, H., &amp;amp; Liu, T. (2016). Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01161"&gt;http://arxiv.org/abs/1606.01161&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Peng, H., Thomson, S., Smith, N. A., &amp;amp; Allen, P. G. (2017). Deep Multitask Learning for Semantic Dependency Parsing. In ACL 2017. Retrieved from &lt;a href="https://arxiv.org/pdf/1704.06855.pdf"&gt;https://arxiv.org/pdf/1704.06855.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Fan, X., Monti, E., Mathias, L., &amp;amp; Dreyer, M. (2017). Transfer Learning for Neural Semantic Parsing. ACL Repl4NLP 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1706.04326"&gt;http://arxiv.org/abs/1706.04326&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp;amp; Fidler, S. (2015). Skip-Thought Vectors, (786). Retrieved from &lt;a href="http://arxiv.org/abs/1506.06726"&gt;http://arxiv.org/abs/1506.06726&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp;amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Mccann, B., Bradbury, J., Xiong, C., &amp;amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Jernite, Y., Bowman, S. R., &amp;amp; Sontag, D. (2017). Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Retrieved from &lt;a href="http://arxiv.org/abs/1705.00557"&gt;http://arxiv.org/abs/1705.00557&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Liu, X., Gao, J., He, X., Deng, L., Duh, K., &amp;amp; Wang, Y.-Y. (2015). Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval. NAACL-2015, 912–921. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. &lt;a href="https://doi.org/10.1145/1390156.1390177"&gt;https://doi.org/10.1145/1390156.1390177&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Søgaard, A., &amp;amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Zhu, J., Park, T., Efros, A. A., Ai, B., &amp;amp; Berkeley, U. C. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Xia, Y., He, D., Qin, T., Wang, L., Yu, N., Liu, T.-Y., &amp;amp; Ma, W.-Y. (2016). Dual Learning for Machine Translation. In Advances in Neural Information Processing Systems 29 (NIPS 2016) (pp. 1–9). Retrieved from &lt;a href="http://arxiv.org/abs/1611.00179"&gt;http://arxiv.org/abs/1611.00179&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Xia, Y., Qin, T., Chen, W., Bian, J., Yu, N., &amp;amp; Liu, T. (2017). Dual Supervised Learning. In ICML. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., … Phil Blunsom. (2017). Grounded Language Learning in a Simulated 3D World. Retrieved from &lt;a href="https://arxiv.org/pdf/1706.06551.pdf"&gt;https://arxiv.org/pdf/1706.06551.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Luo, B., Feng, Y., Xu, J., Zhang, X., &amp;amp; Zhao, D. (2017). Learning to Predict Charges for Criminal Cases with Legal Basis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1707.09168"&gt;http://arxiv.org/abs/1707.09168&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Augenstein, I., &amp;amp; Søgaard, A. (2017). Multi-Task Learning of Keyphrase Boundary Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1704.00514"&gt;http://arxiv.org/abs/1704.00514&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Benton, A., Mitchell, M., &amp;amp; Hovy, D. (2017). Multi-Task Learning for Mental Health using Social Media Text. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Retrieved from &lt;a href="http://m-mitchell.com/publications/multitask-clinical.pdf"&gt;http://m-mitchell.com/publications/multitask-clinical.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Cheng, H., Fang, H., &amp;amp; Ostendorf, M. (2015). Open-Domain Name Error Detection using a Multi-Task RNN. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 737–746). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;Roy, D. (2017). Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 478–483). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Choi, E., Hewlett, D., Uszkoreit, J., Lacoste, A., &amp;amp; Berant, J. (2017). Coarse-to-Fine Question Answering for Long Documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 209–220). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Yang, B., &amp;amp; Mitchell, T. (2017). A Joint Sequential and Relational Model for Frame-Semantic Parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Isonuma, M., Fujino, T., Mori, J., Matsuo, Y., &amp;amp; Sakata, I. (2017). Extractive Summarization Using Multi-Task Learning with Document Classification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2091–2100). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Wu, S., Zhang, D., Yang, N., Li, M., &amp;amp; Zhou, M. (2017). Sequence-to-Dependency Neural Machine Translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 698–707). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., &amp;amp; Zhang, W. (2017). R^3: Reinforced Reader-Ranker for Open-Domain Question Answering. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:57"&gt;&lt;p&gt;Zhao, K., &amp;amp; Huang, L. (2017). Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:57" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:58"&gt;&lt;p&gt;Katiyar, A., &amp;amp; Cardie, C. (2017). Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 917–928). &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:58" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:59"&gt;&lt;p&gt;Hashimoto, K., Xiong, C., Tsuruoka, Y., &amp;amp; Socher, R. (2017). A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01587"&gt;http://arxiv.org/abs/1611.01587&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:59" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:60"&gt;&lt;p&gt;Bingel, J., &amp;amp; Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08303"&gt;http://arxiv.org/abs/1702.08303&lt;/a&gt; &lt;a href="http://ruder.io/multi-task-learning-nlp/#fnref:60" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more</title><description>This post gives an overview of highlights of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen.</description><link>http://ruder.io/highlights-emnlp-2017/</link><guid isPermaLink="false">9205b2f6-88bb-4db7-ac6d-9449953f1348</guid><category>nlp</category><category>natural language processing</category><category>word embeddings</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Fri, 22 Sep 2017 16:06:14 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/09/emnlp_landscape.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/09/emnlp_landscape.jpg" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I spent the past week at the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen, Denmark. The conference handbook can be found &lt;a href="http://emnlp2017.net/downloads/handbook.pdf"&gt;here&lt;/a&gt; and the proceedings can be found &lt;a href="http://aclweb.org/anthology/D/D17/"&gt;here&lt;/a&gt;. Videos of the conference talks and presentations can be found &lt;a href="https://ku.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%229042b495-7b6b-4169-a5a1-d250cc0ee4ec%22"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The program consisted of two days of workshops and tutorials and three days of main conference. The conference was superbly organized, had a great venue, and a social event with fireworks.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/emnlp_fireworks-1.jpg" style="width: 60%; height: 60%" title="EMNLP 2017 fireworks" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 1: Fireworks at the social event&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;225 long papers, 107 papers, and 9 TACL papers had been accepted, with a clear uptick of submissions compared to last year. The number of long and short paper submissions to EMNLP this year was even higher than those at ACL for the first time within the last 13 years, as can be seen in Figure 2.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/long_short_paper_submissions_emnlp_2017.jpg" style="width: 60%; height: 60%" title="Long and short paper submissions EMNLP 2017" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 2: Long and short paper submissions at ACL and EMNLP from 2004-2017&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In the following, I will outline my highlights and list some research papers that caught my eye.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exciting datasets&lt;/strong&gt; &amp;nbsp; Evaluating your approach on CoNLL-2003 or PTB is appropriate for comparing against previous state-of-the-art, but kind of boring. The two following papers introduce datasets that allow you to test your model in more exciting settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1274.pdf"&gt;Durrett et al.&lt;/a&gt; release a new domain adaptation dataset. The dataset evaluates models on their ability to identify products being bought and sold in online cybercrime forums.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf"&gt;Kutuzov et al.&lt;/a&gt; evaluate their word embedding model on a new dataset that focuses on predicting insurgent armed groups based on geographical locations.&lt;/li&gt;
&lt;li&gt;While he did not introduce a new dataset, Nando de Freitas made the point during &lt;a href="http://emnlp2017.net/invited-speakers.html"&gt;his keynote&lt;/a&gt; that the best environment for learning and evaluating language is simulation.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/nando_de_freitas_vision.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 3: Nando de Freitas’ vision for AI research&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Return of the clusters&lt;/strong&gt; &amp;nbsp; Brown clusters, an agglomerative, hierarchical clustering of word types based on contexts that was introduced in 1992 seem to come in vogue again. They were found to be particularly helpful for cross-lingual applications, while clusters were key features in several approaches:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1268.pdf"&gt;Mayhew et al.&lt;/a&gt; found that Brown cluster features were an important signal for cross-lingual NER.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1308.pdf"&gt;Botha et al.&lt;/a&gt; use word clusters as a key feature in their small, efficient feed-forward neural networks.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1070.pdf"&gt;Mekala et al.&lt;/a&gt;’s new document representations cluster word embeddings, which give it an edge for text classification.&lt;/li&gt;
&lt;li&gt;In his talk at the &lt;a href="https://sites.google.com/view/sclem2017/home"&gt;SCLeM workshop&lt;/a&gt;, Noah Smith cites the benefits of using Brown clusters as features for tasks such as POS tagging and sentiment analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/noah_smith_benefits_clustering.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 4: Noah Smith on the benefits of clustering in his invited talk at the SCLeM workshop&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Distant supervision&lt;/strong&gt; &amp;nbsp; Distant supervision can be leveraged to collect large amounts of noisy training data, which can be useful in many applications. Some papers used novel forms of distant supervision to create new corpora or to train a model more effectively:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1127.pdf"&gt;Lan et al.&lt;/a&gt; use urls in tweets to collect a large corpus of paraphrase data. Paraphrase data is usually hard to create, so this approach facilitates the process significantly and enables a continuously expanding collection of paraphrases.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1169.pdf"&gt;Felbo et al.&lt;/a&gt; show that training on fine-grained emoji detection is more effective for pre-training sentiment and emotion models. Previous approaches primarily pre-trained on positive and negative emoticons or emotion hashtags.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Data selection&lt;/strong&gt; &amp;nbsp; The current generation of deep learning models is excellent at learning from data. However, we often do not pay much attention to the actual data our model is using. In many settings, we can improve upon the model by selecting the most relevant data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1064.pdf"&gt;Fang et al.&lt;/a&gt; reframe active learning as reinforcement learning and explicitly learn a data selection policy. Active learning is one of the best ways to create a model with as few annotations as possible; any improvement to this process is beneficial.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1148.pdf"&gt;Van der Wees et al.&lt;/a&gt; introduce dynamic data selection for NMT, which varies the selected subset of the training data between different training epochs. This approach has the potential to reduce the training time of NMT models at comparable or better performance.&lt;/li&gt;
&lt;li&gt;In &lt;a href="http://aclweb.org/anthology/D/D17/D17-1038.pdf"&gt;my paper with Barbara Plank&lt;/a&gt;, we use Bayesian Optimization to learn data selection policies for transfer learning and investigate how well these transfer across models, domains, and tasks. This approach brings us a step closer towards gaining a better understanding of what constitutes similarity between different tasks and domains.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Character-level models&lt;/strong&gt; &amp;nbsp; Characters are nowadays used as standard features in most sequence models. The &lt;a href="https://sites.google.com/view/sclem2017/home"&gt;Subword and Character-level Models in NLP workshop&lt;/a&gt; discussed approaches in more detail, with invited talks on subword language models and character-level NMT. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1297.pdf"&gt;Schmaltz et al.&lt;/a&gt; find that character-based sequence-to-sequence models outperform word-based models and models with character convolutions for sentence correction.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ryancotterell.github.io/"&gt;Ryan Cotterell&lt;/a&gt; gave a great, movie-inspired tutorial on combining the best of FSTs (cowboys) and sequence-to-sequence models (aliens) for string-to-string transduction. While evaluated on morphological segmentation, the tutorial raised awareness in an entertaining way that often the best of both worlds, i.e. a combination of traditional and neural approaches performs best.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/ryan_cotterell_fsts_seq2seq.jpg" style="width: 60%; height: 60%" title="Combining FSTs and seq2seq models" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 5: Ryan Cotterell on combining FSTs and seq2seq models for string-to-string transduction  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Word embeddings&lt;/strong&gt; &amp;nbsp; Research in word embeddings has matured and now mainly tries to 1) address deficits of word2vec, such as its ability of dealing with OOV words; 2) extend it to new settings, e.g. modelling the relations of words over time; and 3) understand the induced representations better:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1010.pdf"&gt;Pinter et al.&lt;/a&gt; propose an approach for generating OOV word embeddings by training a character-based BiLSTM to generate embeddings that are close to pre-trained ones. This approach is promising as it provides us with a more sophisticated way to deal with out-of-vocabulary words than replacing them with an &lt;unk&gt; token. &lt;/unk&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1030.pdf"&gt;Herbelot and Baroni&lt;/a&gt; slightly modify word2vec to allow it to learn embeddings for OOV words from few data.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1122.pdf"&gt;Rosin et al.&lt;/a&gt; propose a model for analyzing &lt;em&gt;when&lt;/em&gt; two words relate to each other.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf"&gt;Kutuzov et al.&lt;/a&gt; propose another model that analyzes how two words relate to each other over time.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1033.pdf"&gt;Hasan and Curry&lt;/a&gt; improve the performance of word embeddings on word similarity tasks by re-embedding them in a manifold.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1311.pdf"&gt;Yang et al.&lt;/a&gt; introduce a simple approach to learning cross-domain word embeddings. Creating embeddings tuned on a small, in-domain corpus is still a challenge, so it is nice to see more approaches addressing this pain point.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1307.pdf"&gt;Mimno and Thompson&lt;/a&gt; try to understand the geometry of word2vec better. They show that the learned word embeddings are positioned diametrically opposite of their context vectors in the embedding space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cross-lingual&lt;/strong&gt; &amp;nbsp; An increasing number of papers evaluate their methods on multiple languages. In addition, there was an excellent &lt;a href="http://emnlp2017.net/tutorials/day2/xling_word_rep.html"&gt;tutorial on cross-lingual word representations&lt;/a&gt;, which summarized and tried to unify much of the existing literature. Slides of the tutorial are available &lt;a href="http://people.ds.cam.ac.uk/iv250/tutorial/xlingrep-tutorial.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1267.pdf"&gt;Malaviya et al.&lt;/a&gt; train a many-to-one NMT to translate 1017 languages into English and use this model to predict information missing from typological databases. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1268.pdf"&gt;Mayhew et al.&lt;/a&gt; introduce a cheap translation method for cross-lingual NER that only requires a bilingual dictionary. They even perform a case study on Uyghur, a truly low-resource language.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/D/D17/D17-1301.pdf"&gt;Kim et al.&lt;/a&gt; present a cross-lingual transfer learning model for POS tagging without parallel data. Parallel data is expensive to create and rarely available for low-resource languages, so this approach fills an important need.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1269.pdf"&gt;Vulic et al.&lt;/a&gt; propose a new cross-lingual transfer method for inducing VerbNets for different languages. The method leverages vector space specialisation, an effective word embedding post-processing technique similar to retro-fitting.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1257.pdf"&gt;Braud et al.&lt;/a&gt; propose a robust, cross-lingual discourse segmentation model that only relies on POS tags. They show that dependency information is less useful than expected; it is important to evaluate our models on multiple languages, so we do not overfit to features that are specific to analytic languages, such as English.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/anders_sogaard_crosslingual.jpg" style="width: 60%; height: 60%" title="Anders Søgaard on cross-lingual embeddings" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 6: Anders Søgaard demonstrating the similarities between different cross-lingual embedding models at the cross-lingual representations tutorial  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Summarization&lt;/strong&gt; &amp;nbsp; &lt;a href="https://summarization2017.github.io/"&gt;The Workshop on New Frontiers of Summarization&lt;/a&gt; brought researchers together to discuss key issues related to automatic summarization. Much of the research on summarization sought to develop new datasets and tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://research.google.com/pubs/author39008.html"&gt;Katja Filippova&lt;/a&gt; gave an interesting talk on sentence compression and passage summarization for Q&amp;amp;A. She described how they went from syntax-based methods to Deep Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4508.pdf"&gt;Volkse et al.&lt;/a&gt; created a new summarization corpus by looking for ‘TL;DR’ on Reddit. This is another example of a creative use of distant supervision, leveraging information that is already contained in the data in order to create a new corpus.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1322.pdf"&gt;Falke and Gurevych&lt;/a&gt; won the best resource paper award for creating a new summary corpus that is based on concept maps rather than textual summaries. The concept map can be explored using a &lt;a href="https://github.com/UKPLab/emnlp2017-graphdocexplore"&gt;graph-based document exploration system&lt;/a&gt;, which is available as a &lt;a href="http://cmaps.ukp.informatik.tu-darmstadt.de/graph-doc-explorer/#!/"&gt;demo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W17-4504"&gt;Pasunuru et al.&lt;/a&gt; use multi-task learning to improve abstractive summarization by leveraging entailment generation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1222.pdf"&gt;Isonuma et al.&lt;/a&gt; also use multi-task learning with document classification in conjunction with curriculum learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4512.pdf"&gt;Li et al.&lt;/a&gt; propose a new task, reader-aware multi-document summarization, which uses comments of articles, along with a dataset for this task.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1065.pdf"&gt;Naranyan et al.&lt;/a&gt; propose another new task, split and rephrase, which aims to split a complex sentence into a sequence of shorter sentences with the same meaning, and also release a new dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-4511.pdf"&gt;Ghalandari&lt;/a&gt; revisits the traditional centroid-based method and proposes a new strong baseline for multi-document summarization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; &amp;nbsp; Data and model-inherent bias is an issue that is receiving more attention in the community. Some papers investigate and propose methods to address the bias in certain datasets and evaluations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1110.pdf"&gt;Chaganty et al.&lt;/a&gt; investigate bias in the evaluation of knowledge base population models and propose an importance sampling-based evaluation to mitigate the bias.&lt;/li&gt;
&lt;li&gt;Dan Jurafsky gave a truly insightful &lt;a href="http://emnlp2017.net/invited-speakers.html"&gt;keynote&lt;/a&gt; about his three year-long study analyzing the body camera recordings his team obtained from the Oakland police department for racial bias. Besides describing the first contemporary linguistic study of officer-community member interaction, he also provided entertaining insights on the language of food (cheaper restaurants use terms related to addiction, more expensive venues use language related to indulgence) and the challenges of interdisciplinary publishing. The entire keynote can be viewed &lt;a href="https://ku.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4f45fc4c-e7d6-4743-b843-0aae2ae8b17e&amp;amp;utm_campaign=Revue%20newsletter&amp;amp;utm_medium=Newsletter&amp;amp;utm_source=NLP%20News"&gt;here&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1119.pdf"&gt;Dubossarsky et al.&lt;/a&gt; analyze the bias in word representation models and propose that recently proposed laws of semantic change must be revised. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1319.pdf"&gt;Zhao et al.&lt;/a&gt; won the best paper award for an approach using Lagrangian relaxation to inject constraints based on corpus-level label statistics. An important finding of their work is bias amplification: While some bias is inherent in all datasets, they observed that models trained on the data amplified its bias. While a gendered dataset might only contain women in 30% of examples, the situation at prediction time might thus be even more dire.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/bias_amplification.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 7: Zhao et al.’s proposed method for reducing bias amplification  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Argument mining &amp;amp; debate analysis&lt;/strong&gt; &amp;nbsp; Argument mining is closely related to summarization. In order to summarize argumentative texts, we have to understand claims and their justifications. This research area had the &lt;a href="https://argmining2017.wordpress.com/"&gt;4th Workshop on Argument Mining&lt;/a&gt; dedicated to it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-5102.pdf"&gt;Hidey et al.&lt;/a&gt; analyse the semantic types of claims (e.g. agreement, interpretation) and premises (ethos, logos, pathos) in the Subreddit &lt;a href="https://www.reddit.com/r/changemyview/"&gt;Change My View&lt;/a&gt;. This is another creative use of reddit to create a dataset and analyze linguistic patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/W/W17/W17-5106.pdf"&gt;Wachsmut et al.&lt;/a&gt; presented an argument web search engine, which can be queried &lt;a href="http://www.args.me/"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D17-1260"&gt;Potash and Rumshinsky&lt;/a&gt; predict the winner of debates, based on audience favorability.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1166.pdf"&gt;Swamy et al.&lt;/a&gt; also forecast winners for the Oscars, the US presidential primaries, and many other contests based on user predictions on Twitter. They create a dataset to test their approach.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1164.pdf"&gt;Zhang et al.&lt;/a&gt; analyze the rhetorical role of questions in discourse.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1143.pdf"&gt;Liu et al.&lt;/a&gt; show that argument-based features are also helpful for predicting review helpfulness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Multi-agent communication&lt;/strong&gt; &amp;nbsp; Multi-agent communication is a niche topic, which has nevertheless received some recent interest, notably in the representation learning community. Most papers deal with a scenario where two agents play a communicative referential game. The task is interesting, as the agents are required to cooperate and have been observed to develop a common pseudo-language in the process.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1310.pdf"&gt;Andreas and Klein&lt;/a&gt; investigate the structure encoded by RNN representations for messages in a communication game. They find that the mistakes are similar to the ones made by humans. In addition, they find that negation is encoded as a linear relationship in the vector space.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1320.pdf"&gt;Kottur et al.&lt;/a&gt; show in their best short paper that language does not emerge naturally when two agents are cooperating, but that they can be coerced to develop compositional expressions.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/09/multi-agent_setup.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"&gt;
&lt;figcaption&gt;Figure 8: The multi-agent setup in the paper of Kottur et al.  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Relation extraction&lt;/strong&gt; &amp;nbsp; Extracting relations from documents is more compelling than simply extracting entities or concepts. Some papers improve upon existing approaches using better distant supervision or adversarial training:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D17-1189"&gt;Liu et al.&lt;/a&gt; reduce the noise in distantly supervised relation extraction with a soft-label method.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1004.pdf"&gt;Zhang et al.&lt;/a&gt; publish TACRED, a large supervised dataset knowledge base population, as well as a new model.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1187.pdf"&gt;Wu et al.&lt;/a&gt; improve the precision of relation extraction with adversarial training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Document and sentence representations&lt;/strong&gt; &amp;nbsp; Learning better sentence representations is closely related to learning more general word representations. While word embeddings still have to be contextualized, sentence representations are promising as they can be directly applied to many different tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1070.pdf"&gt;Mekala et al.&lt;/a&gt; propose a novel technique for building document vectors from word embeddings, with good results for text classification. They use a combination of adding and concatenating word embeddings to represent multiple topics of a document, based on word clusters. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf"&gt;Conneau et al.&lt;/a&gt; learn sentence representations from the SNLI dataset and evaluate them on 12 different tasks. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These were my highlights. Naturally, I was not able to attend every session and see every paper. What were your highlights from the conference or which papers from the &lt;a href="http://aclweb.org/anthology/D/D17/"&gt;proceedings&lt;/a&gt; did you like most? Let me know in the comments below.&lt;/p&gt;</content:encoded></item><item><title>Learning to select data for transfer learning</title><description>This blog post discusses the motivation and findings of our EMNLP 2017 paper Learning to select data for transfer learning.</description><link>http://ruder.io/learning-select-data/</link><guid isPermaLink="false">2d2ccdff-15b1-493d-888a-bec8acb5a346</guid><category>natural language processing</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Fri, 18 Aug 2017 10:59:23 GMT</pubDate><content:encoded>&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href="http://blog.aylien.com/learning-select-data-transfer-learning/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In Machine Learning, the common assumption is that the data our model is applied to is the same, i.e. comes from the same distribution as the data we used for training. This assumption is revealed to be false as soon as we apply our models to the real world: many of the data sources we encounter will be very different from our original training data. In practice, this causes the performance of our model to deteriorate significantly.&lt;/p&gt;

&lt;p&gt;Domain adaptation is a prominent approach to transfer learning that can help to bridge this discrepancy between the training and test data. Domain adaptation is a type of transfer learning, which I have written about &lt;a href="http://ruder.io/transfer-learning/"&gt;here&lt;/a&gt;. Domain adaptation methods typically seek to identify features that are shared between the domains or learn representations that are general enough to be useful for both domains. In this blog post, I will discuss the motivation for, and the findings of the &lt;a href="https://arxiv.org/abs/1707.05246"&gt;recent paper&lt;/a&gt; that I published with &lt;a href="http://www.let.rug.nl/bplank/"&gt;Barbara Planck&lt;/a&gt;. In it, we outline a complementary approach to domain adaptation – rather than learning a model that can &lt;em&gt;adapt&lt;/em&gt; between the domains, we will learn to &lt;em&gt;select data&lt;/em&gt; that is useful for training our model.&lt;/p&gt;

&lt;h1 id="preventingnegativetransfer"&gt;Preventing Negative Transfer&lt;/h1&gt;

&lt;p&gt;The main motivation behind selecting data for transfer learning is to prevent negative transfer. Negative transfer occurs if the information from our source training data is not only unhelpful but actually counter-productive for doing well on our target domain. The classic example for negative transfer comes from sentiment analysis: if we train a model to predict the sentiment of book reviews, we can expect the model to do well on domains that are similar to book reviews. Transferring a model trained on book reviews to reviews of electronics, however, results in negative transfer, as many of the terms our model learned to associate with a certain sentiment for books, e.g. “page-turner”, “gripping”, or — worse — “dangerous” and “electrifying”, will be meaningless or have different connotations for electronics reviews.&lt;/p&gt;

&lt;p&gt;In the classic scenario of adapting from one source to one target domain, the only thing we can do about this is to create a model that is capable of disentangling these shifts in meaning. However, adapting between two very dissimilar domains still fails frequently or leads to painfully poor performance.&lt;/p&gt;

&lt;p&gt;In the real world, we typically have access to multiple data sources. In this case, one thing that we can do is to train our model on the data that is most helpful for our target domain. It is unclear, however, what the best way to determine the helpfulness of source data with respect to a target domain is. Existing work generally relies on measures of similarity between the source and the target domain.&lt;/p&gt;

&lt;h1 id="bayesianoptimizationfordataselection"&gt;Bayesian Optimization for Data Selection&lt;/h1&gt;

&lt;p&gt;Our hypothesis is that the best way to select training data for transfer learning depends on the task and the target domain. In addition, while existing measures only consider data in relation to the target domain, we also argue that some training examples are inherently more helpful than others.&lt;/p&gt;

&lt;p&gt;For these reasons, we propose to learn a data selection measure for transfer learning. We do this using Bayesian Optimization, a framework that has been used successfully to optimize hyperparameters in neural networks and which can be used to optimize any black-box function. We learn this function by defining several features relating to the similarity of the training data to the target domain as well as to its diversity. Over the course of several iterations, the data selection model then learns the importance of each of those features for the relevant task.&lt;/p&gt;

&lt;h1 id="evaluationconclusion"&gt;Evaluation &amp;amp; Conclusion&lt;/h1&gt;

&lt;p&gt;We evaluate our approach on three tasks, sentiment analysis, part-of-speech tagging, and dependency parsing and compare our approach to random selection as well as existing methods that select either the most similar source domain or the most similar training examples.&lt;/p&gt;

&lt;p&gt;For sentiment analysis on reviews, training on the most similar domain is a strong baseline as review categories are clearly delimited. We significantly improve upon this baseline and demonstrate that diversity complements similarity. We even achieve performance competitive with a state-of-the-art domain adaptation approach, despite not performing any adaptation.&lt;/p&gt;

&lt;p&gt;We observe smaller, but consistent improvements for part-of-speech tagging and dependency parsing. Lastly, we evaluate how well learned measures transfer across models, tasks, and domains. We find that learning a data selection measure can be learned with a simpler model, which is used as a proxy for a state-of-the-art model. Transfer across domains is robust, while transfer across tasks holds — as one would expect — for related tasks such as POS tagging and parsing, but fails for dissimilar tasks, e.g. parsing and sentiment analysis.&lt;/p&gt;

&lt;p&gt;In the paper, we demonstrate the importance of selecting relevant data for transfer learning. We show that taking into account task and domain-specific characteristics and learning an appropriate data selection measure outperforms off-the-shelf metrics. We find that diversity complements similarity in selecting appropriate training data and that learned measures can be transferred robustly across models, domains, and tasks.&lt;/p&gt;

&lt;p&gt;This work will be presented at the &lt;a href="http://emnlp2017.net/"&gt;2017 Conference on Empirical Methods in Natural Language Processing&lt;/a&gt;. More details can be found in the paper &lt;a href="https://arxiv.org/abs/1707.05246"&gt;here&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title>Deep Learning for NLP Best Practices</title><description>A collection of best practices for Deep Learning for a wide array of Natural Language Processing tasks. </description><link>http://ruder.io/deep-learning-nlp-best-practices/</link><guid isPermaLink="false">c28facc9-6e69-42e1-b66f-d7449f387e9c</guid><category>nlp</category><category>natural language processing</category><category>deep learning</category><category>machine learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Tue, 25 Jul 2017 19:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/07/attention_bahdanau-1.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/07/attention_bahdanau-1.png" alt="Deep Learning for NLP Best Practices"&gt;&lt;p&gt;Update July 26, 2017: For additional context, the &lt;a href="https://news.ycombinator.com/item?id=14852704"&gt;HackerNews discussion&lt;/a&gt; about this post.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#bestpractices"&gt;Best practices&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#wordembeddings"&gt;Word embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#depth"&gt;Depth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#layerconnections"&gt;Layer connections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#dropout"&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#attention"&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#optimization"&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#ensembling"&gt;Ensembling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#hyperparameteroptimization"&gt;Hyperparameter optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#lstmtricks"&gt;LSTM tricks&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#taskspecificbestpractices"&gt;Task-specific best practices&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#classification"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#sequencelabelling"&gt;Sequence labelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#naturallanguagegeneration"&gt;Natural language generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#neuralmachinetranslation"&gt;Neural machine translation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This post is a collection of best practices for using neural networks in Natural Language Processing. It will be updated periodically as new insights become available and in order to keep track of our evolving understanding of Deep Learning for NLP.&lt;/p&gt;

&lt;p&gt;There has been a &lt;a href="https://twitter.com/IAugenstein/status/710837374473920512"&gt;running joke&lt;/a&gt; in the NLP community that an LSTM with attention will yield state-of-the-art performance on any task. While this has been true over the course of the last two years, the NLP community is slowly moving away from this now standard baseline and towards more interesting models.&lt;/p&gt;

&lt;p&gt;However, we as a community do not want to spend the next two years independently (re-)discovering the &lt;em&gt;next&lt;/em&gt; LSTM with attention. We do not want to reinvent tricks or methods that have already been shown to work. While many existing Deep Learning libraries already encode best practices for working with neural networks in general, such as initialization schemes, many other details, particularly task or domain-specific considerations, are left to the practitioner.&lt;/p&gt;

&lt;p&gt;This post is not meant to keep track of the state-of-the-art, but rather to collect best practices that are relevant for a wide range of tasks. In other words, rather than describing one particular architecture, this post aims to collect the features that underly successful architectures. While many of these features will be most useful for pushing the state-of-the-art, I hope that wider knowledge of them will lead to stronger evaluations, more meaningful comparison to baselines, and inspiration by shaping our intuition of what works.&lt;/p&gt;

&lt;p&gt;I assume you are familiar with neural networks as applied to NLP (if not, I recommend Yoav Goldberg's &lt;a href="https://www.jair.org/media/4992/live-4992-9623-jair.pdf"&gt;excellent primer&lt;/a&gt; [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;]) and are interested in NLP in general or in a particular task. The main goal of this article is to get you up to speed with the relevant best practices so you can make meaningful contributions as soon as possible.&lt;/p&gt;

&lt;p&gt;I will first give an overview of best practices that are relevant for most tasks. I will then outline practices that are relevant for the most common tasks, in particular classification, sequence labelling, natural language generation, and neural machine translation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; Treating something as &lt;em&gt;best practice&lt;/em&gt; is notoriously difficult: Best according to what? What if there are better alternatives? This post is based on my (necessarily incomplete) understanding and experience. In the following, I will only discuss practices that have been reported to be beneficial independently by &lt;em&gt;at least&lt;/em&gt; two different groups. I will try to give at least two references for each best practice.&lt;/p&gt;

&lt;h1 id="bestpractices"&gt;Best practices&lt;/h1&gt;

&lt;h2 id="wordembeddings"&gt;Word embeddings&lt;/h2&gt;

&lt;p&gt;Word embeddings are arguably the most widely known best practice in the recent history of NLP. It is well-known that using pre-trained embeddings helps (Kim, 2014) [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;]. The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] or part-of-speech (POS) tagging (Plank et al., 2016) [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="depth"&gt; Depth&lt;/h2&gt;

&lt;p&gt;While we will not reach the depths of computer vision for a while, neural networks in NLP have become progressively deeper. State-of-the-art approaches now regularly use deep Bi-LSTMs, typically consisting of 3-4 layers, e.g. for POS tagging (Plank et al., 2016) and semantic role labelling (He et al., 2017) [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;]. Models for some tasks can be even deeper, cf. Google's NMT model with 8 encoder and 8 decoder layers (Wu et al., 2016) [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. In most cases, however, performance improvements of making the model deeper than 2 layers are minimal (Reimers &amp;amp; Gurevych, 2017) [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;These observations hold for most sequence tagging and structured prediction problems. For classification, deep or very deep models perform well only with character-level input and shallow word-level models are still the state-of-the-art (Zhang et al., 2015; Conneau et al., 2016; Le et al., 2017) [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="layerconnections"&gt;Layer connections&lt;/h2&gt;

&lt;p&gt;For training deep neural networks, some tricks are essential to avoid the vanishing gradient problem. Different layers and connections have been proposed. Here, we will discuss three: i) highway layers, ii) residual connections, and iii) dense connections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Highway layers&lt;/strong&gt; &amp;nbsp; Highway layers (Srivastava et al., 2015) [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] are inspired by the gates of an LSTM. First let us assume a one-layer MLP, which applies an affine transformation followed by a non-linearity \(g\) to its input \(\mathbf{x}\): &lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{b})\)&lt;/p&gt;

&lt;p&gt;A highway layer then computes the following function instead:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = \mathbf{t} \odot g(\mathbf{W} \mathbf{x} + \mathbf{b}) + (1-\mathbf{t}) \odot \mathbf{x} \)&lt;/p&gt;

&lt;p&gt;where \(\odot\) is elementwise multiplication, \(\mathbf{t} = \sigma(\mathbf{W}_T \mathbf{x} + \mathbf{b}_T)\) is called the &lt;em&gt;transform&lt;/em&gt; gate, and \((1-\mathbf{t})\) is called the &lt;em&gt;carry&lt;/em&gt; gate. As we can see, highway layers are similar to the gates of an LSTM in that they adaptively &lt;em&gt;carry&lt;/em&gt; some dimensions of the input directly to the output. &lt;/p&gt;

&lt;p&gt;Highway layers have been used pre-dominantly to achieve state-of-the-art results for language modelling (Kim et al., 2016; Jozefowicz et al., 2016; Zilly et al., 2017) [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;], but have also been used for other tasks such as speech recognition (Zhang et al., 2016) [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;]. &lt;a href="http://people.idsia.ch/~rupesh/very_deep_learning/"&gt;Sristava's page&lt;/a&gt; contains more information and code regarding highway layers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Residual connections&lt;/strong&gt; &amp;nbsp; Residual connections (He et al., 2016) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] have been first proposed for computer vision and were the main factor for winning ImageNet 2016. Residual connections are even more straightforward than highway layers and learn the following function:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{b}) + \mathbf{x}\) &lt;/p&gt;

&lt;p&gt;which simply adds the input of the current layer to its output via a short-cut connection. This simple modification mitigates the vanishing gradient problem, as the model can default to using the identity function if the layer is not beneficial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense connections&lt;/strong&gt; &amp;nbsp; Rather than just adding layers from each layer to the next, dense connections (Huang et al., 2017) [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] (best paper award at CVPR 2017) add direct connections from each layer to all subsequent layers. Let us augment the layer output \(h\) and layer input \(x\) with indices \(l\) indicating the current layer. Dense connections then feed the concatenated output from all previous layers as input to the current layer:&lt;/p&gt;

&lt;p&gt;\(\mathbf{h}^l = g(\mathbf{W}[\mathbf{x}^1; \ldots; \mathbf{x}^l] + \mathbf{b})\)&lt;/p&gt;

&lt;p&gt;where \([\cdot; \cdot]\) represents concatenation. Dense connections have been successfully used in computer vision. They have also found to be useful for Multi-Task Learning of different NLP tasks (Ruder et al., 2017) [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;], while a residual variant that uses summation has been shown to consistently outperform residual connections for neural machine translation (Britz et al., 2017) [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="dropout"&gt;Dropout&lt;/h2&gt;

&lt;p&gt;While batch normalisation in computer vision has made other regularizers obsolete in most applications, dropout (Srivasta et al., 2014) [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] is still the go-to regularizer for deep neural networks in NLP. A dropout rate of 0.5 has been shown to be effective in most scenarios (Kim, 2014). In recent years, variations of dropout such as adaptive (Ba &amp;amp; Frey, 2013) [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;] and evolutional dropout (Li et al., 2016) [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;] have been proposed, but none of these have found wide adoption in the community. The main problem hindering dropout in NLP has been that it could not be applied to recurrent connections, as the aggregating dropout masks would effectively zero out embeddings over time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recurrent dropout&lt;/strong&gt; &amp;nbsp; Recurrent dropout (Gal &amp;amp; Ghahramani, 2016) [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] addresses this issue by applying the same dropout mask across timesteps at layer \(l\). This avoids amplifying the dropout noise along the sequence and leads to effective regularization for sequence models. Recurrent dropout has been used for instance to achieve state-of-the-art results in semantic role labelling (He et al., 2017) and language modelling (Melis et al., 2017) [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="multitasklearning"&gt;Multi-task learning&lt;/h2&gt;

&lt;p&gt;If additional data is available, multi-task learning (MTL) can often be used to improve performance on the target task. Have a look &lt;a href="http://ruder.io/multi-task/index.html"&gt;this blog post&lt;/a&gt; for more information on MTL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Auxiliary objectives&lt;/strong&gt; &amp;nbsp; We can often find auxiliary objectives that are useful for the task we care about (Ruder, 2017) [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;]. While we can already predict surrounding words in order to pre-train word embeddings (Mikolov et al., 2013), we can also use this as an auxiliary objective during training (Rei, 2017) [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]. A similar objective has also been used by  (Ramachandran et al., 2016) [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] for sequence-to-sequence models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task-specific layers&lt;/strong&gt; &amp;nbsp; While the standard approach to MTL for NLP is hard parameter sharing, it is beneficial to allow the model to learn task-specific layers. This can be done by placing the output layer of one task at a lower level (Søgaard &amp;amp; Goldberg, 2016) [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;]. Another way is to induce private and shared subspaces (Liu et al., 2017; Ruder et al., 2017) [&lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="attention"&gt; Attention&lt;/h2&gt;

&lt;p&gt;Attention is most commonly used in sequence-to-sequence models to attend to encoder states, but can also be used in any sequence model to look back at past states. Using attention, we obtain a context vector \(\mathbf{c}_i\) based on hidden states \(\mathbf{s}_1, \ldots, \mathbf{s}_m\) that can be used together with the current hidden state \(\mathbf{h}_i\) for prediction. The context vector \(\mathbf{c}_i\) at position is calculated as an average of the previous states weighted with the attention scores \(\mathbf{a}_i\):&lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{c}_i &amp;amp;= \sum\limits_j a_{ij}\mathbf{s}_j\\
\mathbf{a}_i &amp;amp;= \text{softmax}(f_{att}(\mathbf{h}_i, \mathbf{s}_j))
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;The attention function \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\) calculates an unnormalized alignment score between the current hidden state \(\mathbf{h}_i\) and the previous hidden state \(\mathbf{s}_j\). In the following, we will discuss four attention variants: i) additive attention, ii) multiplicative attention, iii) self-attention, and iv) key-value attention.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Additive attention&lt;/strong&gt; &amp;nbsp; The original attention mechanism (Bahdanau et al., 2015) [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] uses a one-hidden layer feed-forward network to calculate the attention alignment:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_a[\mathbf{h}_i; \mathbf{s}_j]) \)&lt;/p&gt;

&lt;p&gt;where \(\mathbf{v}_a\) and \(\mathbf{W}_a\) are learned attention parameters. Analogously, we can also use matrices \(\mathbf{W}_1\) and \(\mathbf{W}_2\) to learn separate transformations for \(\mathbf{h}_i\) and \(\mathbf{s}_j\) respectively, which are then summed:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}_j) \)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multiplicative attention&lt;/strong&gt; &amp;nbsp; Multiplicative attention (Luong et al., 2015) [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] simplifies the attention operation by calculating the following function:&lt;/p&gt;

&lt;p&gt;\(f_{att}(h_i, s_j) = h_i^\top \mathbf{W}_a s_j \)&lt;/p&gt;

&lt;p&gt;Additive and multiplicative attention are similar in complexity, although multiplicative attention is faster and more space-efficient in practice as it can be implemented more efficiently using matrix multiplication. Both variants perform similar for small dimensionality \(d_h\) of the decoder states, but additive attention performs better for larger dimensions. One way to mitigate this is to scale \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\) by \(1 / \sqrt{d_h}\) (Vaswani et al., 2017) [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;Attention cannot only be used to attend to encoder or previous hidden states, but also to obtain a distribution over other features, such as the word embeddings of a text as used for reading comprehension (Kadlec et al., 2017) [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. However, attention is not directly applicable to classification tasks that do not require additional information, such as sentiment analysis. In such models, the final hidden state of an LSTM or an aggregation function such as max pooling or averaging is often used to obtain a sentence representation. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Self-attention&lt;/strong&gt; &amp;nbsp; Without any additional information, however, we can still extract relevant aspects from the sentence by allowing it to attend to itself using self-attention (Lin et al., 2017) [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;]. Self-attention, also called intra-attention has been used successfully in a variety of tasks including reading comprehension (Cheng et al., 2016) [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;], textual entailment (Parikh et al., 2016) [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;], and abstractive summarization (Paulus et al., 2017) [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;We can simplify additive attention to compute the unnormalized alignment score for each hidden state \(\mathbf{h}_i\):&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_a \mathbf{h}_i) \)&lt;/p&gt;

&lt;p&gt;In matrix form, for hidden states \(\mathbf{H} = \mathbf{h}_1, \ldots, \mathbf{h}_n\) we can calculate the attention vector \(\mathbf{a}\) and the final sentence representation \(\mathbf{c}\) as follows:&lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{a} &amp;amp;= \text{softmax}(\mathbf{v}_a \text{tanh}(\mathbf{W}_a \mathbf{H}^\top))\\
\mathbf{c} &amp;amp; = \mathbf{H} \mathbf{a}^\top
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;Rather than only extracting one vector, we can perform several hops of attention by using a matrix \(\mathbf{V}_a\) instead of \(\mathbf{v}_a\), which allows us to extract an attention matrix \(\mathbf{A}\): &lt;/p&gt;

&lt;p&gt;\(\begin{align}\begin{split}
\mathbf{A} &amp;amp;= \text{softmax}(\mathbf{V}_a \text{tanh}(\mathbf{W}_a \mathbf{H}^\top))\\
\mathbf{C} &amp;amp; = \mathbf{A} \mathbf{H}
\end{split}\end{align}\)&lt;/p&gt;

&lt;p&gt;In practice, we enforce the following orthogonality constraint to penalize redundancy and encourage diversity in the attention vectors in the form of the squared Frobenius norm:&lt;/p&gt;

&lt;p&gt;\(\Omega = \|(\mathbf{A}\mathbf{A}^\top - \mathbf{I} \|^2_F \)&lt;/p&gt;

&lt;p&gt;A similar multi-head attention is also used by Vaswani et al. (2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key-value attention&lt;/strong&gt; &amp;nbsp; Finally, key-value attention (Daniluk et al., 2017) [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] is a recent attention variant that separates form from function by keeping separate vectors for the attention calculation. It has also been found useful for different document modelling tasks (Liu &amp;amp; Lapata, 2017) [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;]. Specifically, key-value attention splits each hidden vector \(\mathbf{h}_i\) into a key \(\mathbf{k}_i\) and a value \(\mathbf{v}_i\): \([\mathbf{k}_i; \mathbf{v}_i] = \mathbf{h}_i\). The keys are used for calculating the attention distribution \(\mathbf{a}_i\) using additive attention:&lt;/p&gt;

&lt;p&gt;\(\mathbf{a}_i = \text{softmax}(\mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 [\mathbf{k}_{i-L}; \ldots; \mathbf{k}_{i-1}] + (\mathbf{W}_2 \mathbf{k}_i)\mathbf{1}^\top)) \)&lt;/p&gt;

&lt;p&gt;where \(L\) is the length of the attention window and \(\mathbf{1}\) is a vector of ones. The  values are then used to obtain the context representation \(\mathbf{c}_i\):&lt;/p&gt;

&lt;p&gt;\(\mathbf{c}_i = [\mathbf{v}_{i-L}; \ldots; \mathbf{v}_{i-1}] \mathbf{a}^\top\) &lt;/p&gt;

&lt;p&gt;The context \(\mathbf{c}_i\) is used together with the current value \(\mathbf{v}_i\) for prediction.&lt;/p&gt;

&lt;h2 id="optimization"&gt; Optimization&lt;/h2&gt;

&lt;p&gt;The optimization algorithm and scheme is often one of the parts of the model that is used as-is and treated as a black-box. Sometimes, even slight changes to the algorithm, e.g. reducing the \(\beta_2\) value in Adam (Dozat &amp;amp; Manning, 2017) [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;] can make a large difference to the optimization behaviour.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization algorithm&lt;/strong&gt; &amp;nbsp; Adam (Kingma &amp;amp; Ba, 2015) [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] is one of the most popular and widely used optimization algorithms and often the go-to optimizer for NLP researchers. It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam (Wu et al., 2016). Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam (Zhang et al., 2017) [&lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimization scheme&lt;/strong&gt; &amp;nbsp; While Adam internally tunes the learning rate for every parameter (Ruder, 2016) [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;], we can explicitly use SGD-style annealing with Adam. In particular, we can perform learning rate annealing with restarts: We set a learning rate and train the model until convergence. We then halve the learning rate and restart by loading the previous best model. In Adam's case, this causes the optimizer to forget its per-parameter learning rates and start fresh. Denkowski &amp;amp; Neubig (2017) [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing.&lt;/p&gt;

&lt;h2 id="ensembling"&gt;Ensembling&lt;/h2&gt;

&lt;p&gt;Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance. While predicting with an ensemble is expensive at test time, recent advances in distillation allow us to compress an expensive ensemble into a much smaller model (Hinton et al., 2015; Kuncoro et al., 2016; Kim &amp;amp; Rush, 2016) [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp;amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect (Huang et al., 2017) [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;]. However, if resources are available, we prefer to ensemble multiple independently trained models to maximize model diversity.&lt;/p&gt;

&lt;h2 id="hyperparameteroptimization"&gt;Hyperparameter optimization&lt;/h2&gt;

&lt;p&gt;Rather than pre-defining or using off-the-shelf hyperparameters, simply tuning the hyperparameters of our model can yield significant improvements over baselines. Recent advances in Bayesian Optimization have made it an ideal tool for the black-box optimization of hyperparameters in neural networks (Snoek et al., 2012) [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;] and far more efficient than the widely used grid search. Automatic tuning of hyperparameters of an LSTM has led to state-of-the-art results in language modeling, outperforming models that are far more complex (Melis et al., 2017).&lt;/p&gt;

&lt;h2 id="lstmtricks"&gt;LSTM tricks&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Learning the initial state&lt;/strong&gt; &amp;nbsp; We generally initialize the initial LSTM states with a \(0\) vector. Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance and is also &lt;a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf"&gt;recommended by Hinton&lt;/a&gt;. Refer to &lt;a href="https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html"&gt;this blog post&lt;/a&gt; for a Tensorflow implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tying input and output embeddings&lt;/strong&gt; &amp;nbsp; Input and output embeddings account for the largest number of parameters in the LSTM model. If the LSTM predicts words as in language modelling, input and output parameters can be shared (Inan et al., 2016; Press &amp;amp; Wolf, 2017) [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;]. This is particularly useful on small datasets that do not allow to learn a large number of parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gradient norm clipping&lt;/strong&gt; &amp;nbsp; One way to decrease the risk of exploding gradients is to clip their maximum value (Mikolov, 2012) [&lt;sup id="fnref:57"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:57" rel="footnote"&gt;57&lt;/a&gt;&lt;/sup&gt;]. This, however, does not improve performance consistently (Reimers &amp;amp; Gurevych, 2017). Rather than clipping each gradient independently, clipping the global norm of the gradient (Pascanu et al., 2013) [&lt;sup id="fnref:58"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:58" rel="footnote"&gt;58&lt;/a&gt;&lt;/sup&gt;] yields more significant improvements (a Tensorflow implementation can be found &lt;a href="https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow"&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Down-projection&lt;/strong&gt; &amp;nbsp; To reduce the number of output parameters further, the hidden state of the LSTM can be projected to a smaller size. This is useful particularly for tasks with a large number of outputs, such as language modelling (Melis et al., 2017).&lt;/p&gt;

&lt;h1 id="taskspecificbestpractices"&gt;Task-specific best practices&lt;/h1&gt;

&lt;p&gt;In the following, we will discuss task-specific best practices. Most of these perform best for a particular type of task. Some of them might still be applied to other tasks, but should be validated before. We will discuss the following tasks: classification, sequence labelling, natural language generation (NLG), and -- as a special case of NLG -- neural machine translation.&lt;/p&gt;

&lt;h2 id="classification"&gt;Classification&lt;/h2&gt;

&lt;p&gt;More so than for sequence tasks, where CNNs have only recently found application due to more efficient convolutional operations, CNNs have been popular for classification tasks in NLP. The following best practices relate to CNNs and capture some of their optimal hyperparameter choices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CNN filters&lt;/strong&gt; &amp;nbsp; Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp;amp; Wallace, 2015) [&lt;sup id="fnref:59"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:59" rel="footnote"&gt;59&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aggregation function&lt;/strong&gt; &amp;nbsp; 1-max-pooling outperforms average-pooling and \(k\)-max pooling (Zhang &amp;amp; Wallace, 2015). &lt;/p&gt;

&lt;h2 id="sequencelabelling"&gt;Sequence labelling&lt;/h2&gt;

&lt;p&gt;Sequence labelling is ubiquitous in NLP. While many of the existing best practices are with regard to a particular part of the model architecture, the following guidelines discuss choices for the model's output and prediction stage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tagging scheme&lt;/strong&gt; &amp;nbsp; For some tasks, which can assign labels to segments of texts, different tagging schemes are possible. These are: &lt;em&gt;BIO&lt;/em&gt;, which marks the first token in a segment with a &lt;em&gt;B-&lt;/em&gt; tag, all remaining tokens in the span with an &lt;em&gt;I-&lt;/em&gt;tag, and tokens outside of segments with an &lt;em&gt;O-&lt;/em&gt; tag; &lt;em&gt;IOB&lt;/em&gt;, which is similar to BIO, but only uses &lt;em&gt;B-&lt;/em&gt; if the previous token is of the same class but not part of the segment; and &lt;em&gt;IOBES&lt;/em&gt;, which in addition distinguishes between single-token entities (&lt;em&gt;S-&lt;/em&gt;) and the last token in a segment (&lt;em&gt;E-&lt;/em&gt;). Using IOBES and BIO yield similar performance (Lample et al., 2017) &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CRF output layer&lt;/strong&gt; &amp;nbsp; If there are any dependencies between outputs, such as in named entity recognition the final softmax layer can be replaced with a linear-chain conditional random field (CRF). This has been shown to yield consistent improvements for tasks that require the modelling of constraints (Huang et al., 2015; Max &amp;amp; Hovy, 2016; Lample et al., 2016) [&lt;sup id="fnref:60"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:60" rel="footnote"&gt;60&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:61"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:61" rel="footnote"&gt;61&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:62"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:62" rel="footnote"&gt;62&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Constrained decoding&lt;/strong&gt; &amp;nbsp; Rather than using a CRF output layer, constrained decoding can be used as an alternative approach to reject erroneous sequences, i.e. such that do not produce valid BIO transitions (He et al., 2017). Constrained decoding has the advantage that arbitrary constraints can be enforced this way, e.g. task-specific or syntactic constraints.&lt;/p&gt;

&lt;h2 id="naturallanguagegeneration"&gt;Natural language generation&lt;/h2&gt;

&lt;p&gt;Most of the existing best practices can be applied to natural language generation (NLG). In fact, many of the tips presented so far stem from advances in language modelling, the most prototypical NLP task. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modelling coverage&lt;/strong&gt; &amp;nbsp; Repetition is a big problem in many NLG tasks as current models do not have a good way of remembering what outputs they already produced. Modelling coverage explicitly in the model is a good way of addressing this issue. A checklist can be used if it is known in advances, which entities should be mentioned in the output, e.g. ingredients in recipes (Kiddon et al., 2016) [&lt;sup id="fnref:63"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:63" rel="footnote"&gt;63&lt;/a&gt;&lt;/sup&gt;]. If attention is used, we can keep track of a coverage vector \(\mathbf{c}_i\), which is the sum of attention distributions \(\mathbf{a}_t\) over previous time steps (Tu et al., 2016; See et al., 2017) [&lt;sup id="fnref:64"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:64" rel="footnote"&gt;64&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:65"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:65" rel="footnote"&gt;65&lt;/a&gt;&lt;/sup&gt;]:&lt;/p&gt;

&lt;p&gt;\(\mathbf{c}_i = \sum\limits^{i-1}_{t=1} \mathbf{a}_t \)&lt;/p&gt;

&lt;p&gt;This vector captures how much attention we have paid to all words in the source. We can now condition additive attention additionally on this coverage vector in order to encourage our model not to attend to the same words repeatedly:&lt;/p&gt;

&lt;p&gt;\(f_{att}(\mathbf{h}_i,\mathbf{s}_j,\mathbf{c}_i) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}_j + \mathbf{W}_3 \mathbf{c}_i )\)&lt;/p&gt;

&lt;p&gt;In addition, we can add an auxiliary loss that captures the task-specific attention behaviour that we would like to elicit: For NMT, we would like to have a roughly one-to-one alignment; we thus penalize the model if the final coverage vector is more or less than one at every index (Tu et al., 2016). For summarization, we only want to penalize the model if it repeatedly attends to the same location (See et al., 2017). &lt;/p&gt;

&lt;h2 id="neuralmachinetranslation"&gt;Neural machine translation&lt;/h2&gt;

&lt;p&gt;While neural machine translation (NMT) is an instance of NLG, NMT receives so much attention that many methods have been developed specifically for the task. Similarly, many best practices or hyperparameter choices apply exclusively to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Embedding dimensionality&lt;/strong&gt; &amp;nbsp; 2048-dimensional embeddings yield the best performance, but only do so by a small margin. Even 128-dimensional embeddings perform surprisingly well and converge almost twice as quickly (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encoder and decoder depth&lt;/strong&gt; &amp;nbsp; The encoder does not need to be deeper than \(2-4\) layers. Deeper models outperform shallower ones, but more than \(4\) layers is not necessary for the decoder (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Directionality&lt;/strong&gt; &amp;nbsp; Bidirectional encoders outperform unidirectional ones by a small margin. 
Sutskever et al., (2014) [&lt;sup id="fnref:67"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:67" rel="footnote"&gt;67&lt;/a&gt;&lt;/sup&gt;] proposed to reverse the source sequence to reduce the number of long-term dependencies. Reversing the source sequence in unidirectional encoders outperforms its non-reversed counter-part (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beam search strategy&lt;/strong&gt; &amp;nbsp; Medium beam sizes around \(10\) with length normalization penalty of \(1.0\) (Wu et al., 2016) yield the best performance (Britz et al., 2017).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sub-word translation&lt;/strong&gt; &amp;nbsp; Senrich et al. (2016) [&lt;sup id="fnref:66"&gt;&lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fn:66" rel="footnote"&gt;66&lt;/a&gt;&lt;/sup&gt;] proposed to split words into sub-words based on a byte-pair encoding (BPE). BPE iteratively merges frequent symbol pairs, which eventually results in frequent character n-grams being merged into a single symbol, thereby effectively eliminating out-of-vocabulary-words. While it was originally meant to handle rare words, a model with sub-word units outperforms full-word systems across the board, with 32,000 being an effective vocabulary size for sub-word units (Denkowski &amp;amp; Neubig, 2017).&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I hope this post was helpful in kick-starting your learning of a new NLP task. Even if you have already been familiar with most of these, I hope that you still learnt something new or refreshed your knowledge of useful tips.&lt;/p&gt;

&lt;p&gt;I am sure that I have forgotten many best practices that deserve to be on this list. Similarly, there are many tasks such as parsing, information extraction, etc., which I do not know enough about to give recommendations. If you have a best practice that should be on this list, do let me know in the comments below. Please provide at least one reference and your handle for attribution. If this gets very collaborative, I might open a GitHub repository rather than collecting feedback here (I won't be able to accept PRs submitted directly to the generated HTML source of this article).&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Srivastava, R. K., Greff, K., &amp;amp; Schmidhuber, J. (2015). Training Very Deep Networks. In Advances in Neural Information Processing Systems. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Kim, Y., Jernite, Y., Sontag, D., &amp;amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from &lt;a href="http://arxiv.org/abs/1508.06615"&gt;http://arxiv.org/abs/1508.06615&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv Preprint arXiv:1602.02410. Retrieved from &lt;a href="http://arxiv.org/abs/1602.02410"&gt;http://arxiv.org/abs/1602.02410&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Zilly, J. G., Srivastava, R. K., Koutnik, J., &amp;amp; Schmidhuber, J. (2017). Recurrent Highway Networks. In International Conference on Machine Learning (ICML 2017). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Zhang, Y., Chen, G., Yu, D., Yao, K., Kudanpur, S., &amp;amp; Glass, J. (2016). Highway Long Short-Term Memory RNNS for Distant Speech Recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In CVPR. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Huang, G., Weinberger, K. Q., &amp;amp; Maaten, L. Van Der. (2016). Densely Connected Convolutional Networks. CVPR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp;amp; Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15, 1929–1958. &lt;a href="http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf"&gt;http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Ba, J., &amp;amp; Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems. Retrieved from file:///Files/A5/A51D0755-5CEF-4772-942D-C5B8157FBE5E.pdf &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Li, Z., Gong, B., &amp;amp; Yang, T. (2016). Improved Dropout for Shallow and Deep Learning. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1602.02220"&gt;http://arxiv.org/abs/1602.02220&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Gal, Y., &amp;amp; Ghahramani, Z. (2016). A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. In Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1512.05287"&gt;http://arxiv.org/abs/1512.05287&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from &lt;a href="http://arxiv.org/abs/1408.5882"&gt;http://arxiv.org/abs/1408.5882&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Ruder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. In arXiv preprint arXiv:1706.05098. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Bahdanau, D., Cho, K., &amp;amp; Bengio, Y.. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015. &lt;a href="https://doi.org/10.1146/annurev.neuro.26.041002.131047"&gt;https://doi.org/10.1146/annurev.neuro.26.041002.131047&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Luong, M.-T., Pham, H., &amp;amp; Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015. Retrieved from &lt;a href="http://arxiv.org/abs/1508.04025"&gt;http://arxiv.org/abs/1508.04025&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. arXiv Preprint arXiv:1706.03762. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Lin, Z., Feng, M., Santos, C. N. dos, Yu, M., Xiang, B., Zhou, B., &amp;amp; Bengio, Y. (2017). A Structured Self-Attentive Sentence Embedding. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Daniluk, M., Rockt, T., Welbl, J., &amp;amp; Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. In ICLR 2017.  &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Ruder, S. (2016). An overview of gradient descent optimization. arXiv Preprint arXiv:1609.04747. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Denkowski, M., &amp;amp; Neubig, G. (2017). Stronger Baselines for Trustable Results in Neural Machine Translation. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Hinton, G., Vinyals, O., &amp;amp; Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv Preprint arXiv:1503.02531. &lt;a href="https://doi.org/10.1063/1.4931082"&gt;https://doi.org/10.1063/1.4931082&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C., &amp;amp; Smith, N. A. (2016). Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser. Empirical Methods in Natural Language Processing. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Kim, Y., &amp;amp; Rush, A. M. (2016). Sequence-Level Knowledge Distillation. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Britz, D., Goldie, A., Luong, T., &amp;amp; Le, Q. (2017). Massive Exploration of Neural Machine Translation Architectures. In arXiv preprint arXiv:1703.03906. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Zhang, X., Zhao, J., &amp;amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems, 649–657. Retrieved from &lt;a href="http://arxiv.org/abs/1509.01626"&gt;http://arxiv.org/abs/1509.01626&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Conneau, A., Schwenk, H., Barrault, L., &amp;amp; Lecun, Y. (2016). Very Deep Convolutional Networks for Natural Language Processing. arXiv Preprint arXiv:1606.01781. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01781"&gt;http://arxiv.org/abs/1606.01781&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Le, H. T., Cerisara, C., &amp;amp; Denis, A. (2017). Do Convolutional Networks need to be Deep for Text Classification ? In arXiv preprint arXiv:1707.04108. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;He, L., Lee, K., Lewis, M., &amp;amp; Zettlemoyer, L. (2017). Deep Semantic Role Labeling: What Works and What’s Next. ACL. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Melis, G., Dyer, C., &amp;amp; Blunsom, P. (2017). On the State of the Art of Evaluation in Neural Language Models. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Ramachandran, P., Liu, P. J., &amp;amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Kadlec, R., Schmid, M., Bajgar, O., &amp;amp; Kleindienst, J. (2016). Text Understanding with the Attention Sum Reader Network. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Cheng, J., Dong, L., &amp;amp; Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. arXiv Preprint arXiv:1601.06733. Retrieved from &lt;a href="http://arxiv.org/abs/1601.06733"&gt;http://arxiv.org/abs/1601.06733&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Parikh, A. P., Täckström, O., Das, D., &amp;amp; Uszkoreit, J. (2016). A Decomposable Attention Model for Natural Language Inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Retrieved from &lt;a href="http://arxiv.org/abs/1606.01933"&gt;http://arxiv.org/abs/1606.01933&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Paulus, R., Xiong, C., &amp;amp; Socher, R. (2017). A Deep Reinforced Model for Abstractive Summarization. In arXiv preprint arXiv:1705.04304,. Retrieved from &lt;a href="http://arxiv.org/abs/1705.04304"&gt;http://arxiv.org/abs/1705.04304&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Liu, Y., &amp;amp; Lapata, M. (2017). Learning Structured Text Representations. In arXiv preprint arXiv:1705.09207. Retrieved from &lt;a href="http://arxiv.org/abs/1705.09207"&gt;http://arxiv.org/abs/1705.09207&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Zhang, J., Mitliagkas, I., &amp;amp; Ré, C. (2017). YellowFin and the Art of Momentum Tuning. arXiv preprint arXiv:1706.03471. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Goldberg, Y. (2016). A Primer on Neural Network Models for Natural Language Processing. Journal of Artificial Intelligence Research, 57, 345–420. &lt;a href="https://doi.org/10.1613/jair.4992"&gt;https://doi.org/10.1613/jair.4992&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Melamud, O., McClosky, D., Patwardhan, S., &amp;amp; Bansal, M. (2016). The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of NAACL-HLT 2016 (pp. 1030–1040). Retrieved from &lt;a href="http://arxiv.org/abs/1601.00893"&gt;http://arxiv.org/abs/1601.00893&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Ruder, S., Ghaffari, P., &amp;amp; Breslin, J. G. (2016). A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 999–1005. Retrieved from &lt;a href="http://arxiv.org/abs/1609.02745"&gt;http://arxiv.org/abs/1609.02745&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Reimers, N., &amp;amp; Gurevych, I. (2017). Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks. In arXiv preprint arXiv:1707.06799: Retrieved from &lt;a href="https://arxiv.org/pdf/1707.06799.pdf"&gt;https://arxiv.org/pdf/1707.06799.pdf&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Søgaard, A., &amp;amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Liu, P., Qiu, X., &amp;amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1704.05742"&gt;http://arxiv.org/abs/1704.05742&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Dozat, T., &amp;amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01734"&gt;http://arxiv.org/abs/1611.01734&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;Jean, S., Cho, K., Memisevic, R., &amp;amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from &lt;a href="http://www.aclweb.org/anthology/P15-1001"&gt;http://www.aclweb.org/anthology/P15-1001&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Edinburgh Neural Machine Translation Systems for WMT 16. In Proceedings of the First Conference on Machine Translation (WMT 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1606.02891"&gt;http://arxiv.org/abs/1606.02891&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., &amp;amp; Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. In ICLR 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Inan, H., Khosravi, K., &amp;amp; Socher, R. (2016). Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling. arXiv Preprint arXiv:1611.01462. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Press, O., &amp;amp; Wolf, L. (2017). Using the Output Embedding to Improve Language Models. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2, 157--163. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Snoek, J., Larochelle, H., &amp;amp; Adams, R. P. (2012). Practical Bayesian Optimization of Machine Learning Algorithms. Neural Information Processing Systems Conference (NIPS 2012). &lt;a href="https://doi.org/2012arXiv1206.2944S"&gt;https://doi.org/2012arXiv1206.2944S&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:57"&gt;&lt;p&gt;Mikolov, T. (2012). Statistical language models based on neural networks (Doctoral dissertation, PhD thesis, Brno University of Technology). &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:57" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:58"&gt;&lt;p&gt;Pascanu, R., Mikolov, T., &amp;amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International Conference on Machine Learning, (2), 1310–1318. &lt;a href="https://doi.org/10.1109/72.279181"&gt;https://doi.org/10.1109/72.279181&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:58" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:59"&gt;&lt;p&gt;Zhang, Y., &amp;amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification. arXiv Preprint arXiv:1510.03820, (1). Retrieved from &lt;a href="http://arxiv.org/abs/1510.03820"&gt;http://arxiv.org/abs/1510.03820&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:59" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:60"&gt;&lt;p&gt;Huang, Z., Xu, W., &amp;amp; Yu, K. (2015). Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv preprint arXiv:1508.01991. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:60" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:61"&gt;&lt;p&gt;Ma, X., &amp;amp; Hovy, E. (2016). End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. arXiv Preprint arXiv:1603.01354. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:61" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:62"&gt;&lt;p&gt;Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp;amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. NAACL-HLT 2016. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:62" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:63"&gt;&lt;p&gt;Kiddon, C., Zettlemoyer, L., &amp;amp; Choi, Y. (2016). Globally Coherent Text Generation with Neural Checklist Models. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 329–339. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:63" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:64"&gt;&lt;p&gt;Tu, Z., Lu, Z., Liu, Y., Liu, X., &amp;amp; Li, H. (2016). Modeling Coverage for Neural Machine Translation. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="https://doi.org/10.1145/2856767.2856776"&gt;https://doi.org/10.1145/2856767.2856776&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:64" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:65"&gt;&lt;p&gt;See, A., Liu, P. J., &amp;amp; Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. In ACL 2017. &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:65" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:66"&gt;&lt;p&gt;Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Retrieved from &lt;a href="http://arxiv.org/abs/1508.07909"&gt;http://arxiv.org/abs/1508.07909&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:66" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:67"&gt;&lt;p&gt;Sutskever, I., Vinyals, O., &amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 9. Retrieved from &lt;a href="http://arxiv.org/abs/1409.3215%5Cnhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"&gt;http://arxiv.org/abs/1409.3215%5Cnhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks&lt;/a&gt; &lt;a href="http://ruder.io/deep-learning-nlp-best-practices/#fnref:67" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Credit for the cover image goes to Bahdanau et al. (2015).&lt;/p&gt;</content:encoded></item><item><title>An Overview of Multi-Task Learning in Deep Neural Networks</title><description>This blog post gives an overview of multi-task learning in deep neural networks. It discusses existing approaches as well as recent advances.</description><link>http://ruder.io/multi-task/</link><guid isPermaLink="false">c52283dd-3a6e-4430-a7e9-49626ce9c43e</guid><category>deep learning</category><category>transfer learning</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 29 May 2017 13:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/05/mtl_images-002-2.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/05/mtl_images-002-2.png" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;&lt;p&gt;Note: If you are looking for a review paper, this blog post is also available as an &lt;a href="https://arxiv.org/abs/1706.05098"&gt;article on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#introduction"&gt;Introduction&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#motivation"&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#twomtlmethodsfordeeplearning"&gt;Two MTL methods for Deep Learning&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#hardparametersharing"&gt;Hard parameter sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#softparametersharing"&gt;Soft parameter sharing&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#whydoesmtlwork"&gt;Why does MTL work?&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#implicitdataaugmentation"&gt;Implicit data augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#attentionfocusing"&gt;Attention focusing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#eavesdropping"&gt;Eavesdropping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#representationbias"&gt;Representation bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#regularization"&gt;Regularization&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#mtlinnonneuralmodels"&gt;MTL in non-neural models&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#blocksparseregularization"&gt;Block-sparse regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#learningtaskrelationships"&gt;Learning task relationships&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#recentworkonmtlfordeeplearning"&gt;Recent work on MTL for Deep Learning&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#deeprelationshipnetworks"&gt;Deep Relationship Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#fullyadaptivefeaturesharing"&gt;Fully-Adaptive Feature Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#crossstitchnetworks"&gt;Cross-stitch Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#lowsupervision"&gt;Low supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#ajointmanytaskmodel"&gt;A Joint Many-Task model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#weightinglosseswithuncertainty"&gt;Weighting losses with uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#tensorfactorisationformtl"&gt;Tensor factorisation for MTL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#sluicenetworks"&gt;Sluice Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#whatshouldishareinmymodel"&gt;What should I share in my model?&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#auxiliarytasks"&gt;Auxiliary tasks&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#relatedtask"&gt;Related task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#adversarial"&gt;Adversarial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#hints"&gt;Hints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#focusingattention"&gt;Focusing attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#quantizationsmoothing"&gt;Quantization smoothing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#predictinginputs"&gt;Predicting inputs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#usingthefuturetopredictthepresent"&gt;Using the future to predict the present&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#representationlearning"&gt;Representation learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#whatauxiliarytasksarehelpful"&gt;What auxiliary tasks are helpful?&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/multi-task/#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In Machine Learning (ML), we typically care about optimizing for a particular metric, whether this is a score on a certain benchmark or a business KPI. In order to do this, we generally train a single model or an ensemble of models to perform our desired task. We then fine-tune and tweak these models until their performance no longer increases. While we can generally achieve acceptable performance this way, by being laser-focused on our single task, we ignore information that might help us do even better on the metric we care about. Specifically, this information comes from the training signals of related tasks. By sharing representations between related tasks, we can enable our model to generalize better on our original task. This approach is called Multi-Task Learning (MTL) and will be the topic of this blog post.&lt;/p&gt;

&lt;p&gt;Multi-task learning has been used successfully across all applications of machine learning, from natural language processing [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/multi-task/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] and speech recognition [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/multi-task/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] to computer vision [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/multi-task/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] and drug discovery [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/multi-task/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;]. MTL comes in many guises: joint learning, learning to learn, and learning with auxiliary tasks are only some names that have been used to refer to it. Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it.&lt;/p&gt;

&lt;p&gt;Even if you're only optimizing one loss as is the typical case, chances are there is an auxiliary task that will help you improve upon your main task. Rich Caruana [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/multi-task/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] summarizes the goal of MTL succinctly: "MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks".&lt;/p&gt;

&lt;p&gt;Over the course of this blog post, I will try to give a general overview of the current state of multi-task learning, in particular when it comes to MTL with deep neural networks. I will first motivate MTL from different perspectives. I will then introduce the two most frequently employed methods for MTL in Deep Learning. Subsequently, I will describe mechanisms that together illustrate why MTL works in practice. Before looking at more advanced neural network-based MTL methods, I will provide some context by discussing the literature in MTL. I will then introduce some more powerful recently proposed methods for MTL in deep neural networks. Finally, I will talk about commonly used types of auxiliary tasks and discuss what makes a good auxiliary task for MTL.&lt;/p&gt;

&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;

&lt;p&gt;We can motivate multi-task learning in different ways: Biologically, we can see multi-task learning as being inspired by human learning. For learning new tasks, we often apply the knowledge we have acquired by learning related tasks. For instance, a baby first learns to recognize faces and can then apply this knowledge to recognize other objects.&lt;/p&gt;

&lt;p&gt;From a pedagogical perspective, we often learn tasks first that provide us with the necessary skills to master more complex techniques. This is true for learning the proper way of &lt;a href="https://www.youtube.com/watch?v=NFPPrhxPFR4"&gt;falling in martial arts, e.g. Judo&lt;/a&gt; as much as learning to program.&lt;/p&gt;

&lt;p&gt;Taking an example out of pop culture, we can also consider &lt;em&gt;The Karate Kid&lt;/em&gt; (1984) (thanks to &lt;a href="http://m-mitchell.com/publications/multitask-blurb.html"&gt;Margaret Mitchell&lt;/a&gt; and &lt;a href="https://twitter.com/mmitchell_ai/status/849596878694096896"&gt;Adrian Benton&lt;/a&gt; for the inspiration). In the movie, &lt;em&gt;sensei&lt;/em&gt; Mr Miyagi teaches the karate kid seemingly unrelated tasks such as sanding the floor and waxing a car. In hindsight, these, however, turn out to equip him with invaluable skills that are &lt;a href="https://www.youtube.com/embed/DsLk6hVBE6Y"&gt;relevant for learning karate&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we can motivate multi-task learning from a machine learning point of view: We can view multi-task learning as a form of inductive transfer. Inductive transfer can help improve a model by introducing an inductive bias, which causes a model to prefer some hypotheses over others. For instance, a common form of inductive bias is \(\ell_1\) regularization, which leads to a preference for sparse solutions. In the case of MTL, the inductive bias is provided by the auxiliary tasks, which cause the model to prefer hypotheses that explain more than one task. As we will see shortly, this generally leads to solutions that generalize better.&lt;/p&gt;

&lt;h1 id="twomtlmethodsfordeeplearning"&gt; Two MTL methods for Deep Learning&lt;/h1&gt;

&lt;p&gt;So far, we have focused on theoretical motivations for MTL. To make the ideas of MTL more concrete, we will now look at the two most commonly used ways to perform multi-task learning in deep neural networks. In the context of Deep Learning, multi-task learning is typically done with either &lt;em&gt;hard&lt;/em&gt; or &lt;em&gt;soft parameter sharing&lt;/em&gt; of hidden layers.&lt;/p&gt;

&lt;h2 id="hardparametersharing"&gt;Hard parameter sharing&lt;/h2&gt;

&lt;p&gt;Hard parameter sharing is the most commonly used approach to MTL in neural networks and goes back to [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/multi-task/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;]. It is generally applied by sharing the hidden layers between all tasks, while keeping several task-specific output layers.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/mtl_images-001-2.png" style="width: 50%" title="Hard parameter sharing" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 1: Hard parameter sharing for multi-task learning in deep neural networks&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Hard parameter sharing greatly reduces the risk of overfitting. In fact, [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/multi-task/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] showed that the risk of overfitting the shared parameters is an order N -- where N is the number of tasks -- smaller than overfitting the task-specific parameters, i.e. the output layers. This makes sense intuitively: The more tasks we are learning simultaneously, the more our model has to find a representation that captures all of the tasks and the less is our chance of overfitting on our original task.&lt;/p&gt;

&lt;h2 id="softparametersharing"&gt; Soft parameter sharing&lt;/h2&gt;

&lt;p&gt;In soft parameter sharing on the other hand, each task has its own model with its own parameters. The distance between the parameters of the model is then regularized in order to encourage the parameters to be similar. [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/multi-task/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] for instance use the \(\ell_2\) norm for regularization, while [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/multi-task/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;] use the trace norm.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/mtl_images-002-1.png" style="width: 80%" title="Soft parameter sharing" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 2: Soft parameter sharing for multi-task learning in deep neural networks&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The constraints used for soft parameter sharing in deep neural networks have been greatly inspired by regularization techniques for MTL that have been developed for other models, which we will soon discuss.&lt;/p&gt;

&lt;h1 id="whydoesmtlwork"&gt;Why does MTL work?&lt;/h1&gt;

&lt;p&gt;Even though an inductive bias obtained through multi-task learning seems intuitively plausible, in order to understand MTL better, we need to look at the mechanisms that underlie it. Most of these have first been proposed by Caruana (1998). For all examples, we will assume that we have two related tasks \(A\) and \(B\), which rely on a common hidden layer representation \(F\). &lt;/p&gt;

&lt;h2 id="implicitdataaugmentation"&gt; Implicit data augmentation&lt;/h2&gt;

&lt;p&gt;MTL effectively increases the sample size that we are using for training our model. As all tasks are at least somewhat noisy, when training a model on some task \(A\), our aim is to learn a good representation for task \(A\) that ideally ignores the data-dependent noise and generalizes well. As different tasks have different noise patterns, a model that learns two tasks simultaneously is able to learn a more general representation. Learning just task \(A\) bears the risk of overfitting to task \(A\), while learning \(A\) and \(B\) jointly enables the model to obtain a better representation \(F\) through averaging the noise patterns.&lt;/p&gt;

&lt;h2 id="attentionfocusing"&gt; Attention focusing&lt;/h2&gt;

&lt;p&gt;If a task is very noisy or data is limited and high-dimensional, it can be difficult for a model to differentiate between relevant and irrelevant features. MTL can help the model focus its attention on those features that actually matter as other tasks will provide additional evidence for the relevance or irrelevance of those features.&lt;/p&gt;

&lt;h2 id="eavesdropping"&gt; Eavesdropping&lt;/h2&gt;

&lt;p&gt;Some features \(G\) are easy to learn for some task \(B\), while being difficult to learn for another task \(A\). This might either be because \(A\) interacts with the features in a more complex way or because other features are impeding the model's ability to learn \(G\). Through MTL, we can allow the model to &lt;em&gt;eavesdrop&lt;/em&gt;, i.e. learn \(G\) through task \(B\). The easiest way to do this is through &lt;em&gt;hints&lt;/em&gt; [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/multi-task/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;], i.e. directly training the model to predict the most important features.&lt;/p&gt;

&lt;h2 id="representationbias"&gt;Representation bias&lt;/h2&gt;

&lt;p&gt;MTL biases the model to prefer representations that other tasks also prefer. This will also help the model to generalize to new tasks in the future as a hypothesis space that performs well for a sufficiently large number of training tasks will also perform well for learning novel tasks as long as they are from the same environment [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/multi-task/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="regularization"&gt; Regularization&lt;/h2&gt;

&lt;p&gt;Finally, MTL acts as a regularizer by introducing an inductive bias. As such, it reduces the risk of overfitting as well as the Rademacher complexity of the model, i.e. its ability to fit random noise.&lt;/p&gt;

&lt;h1 id="mtlinnonneuralmodels"&gt; MTL in non-neural models&lt;/h1&gt;

&lt;p&gt;In order to better understand MTL in deep neural networks, we will now look to the existing literature on MTL for linear models, kernel methods, and Bayesian algorithms. In particular, we will discuss two main ideas that have been pervasive throughout the history of multi-task learning: enforcing sparsity across tasks through norm regularization; and modelling the relationships between tasks.&lt;/p&gt;

&lt;p&gt;Note that many approaches to MTL in the literature deal with a homogenous setting: They assume that all tasks are associated with a single output, e.g. the multi-class MNIST dataset is typically cast as 10 binary classification tasks. More recent approaches deal with a more realistic, heterogeneous setting where each task corresponds to a unique set of outputs. &lt;/p&gt;

&lt;h2 id="blocksparseregularization"&gt;Block-sparse regularization&lt;/h2&gt;

&lt;p&gt;In order to better connect the following approaches, let us first introduce some notation. We have \(T\) tasks. For each task \(t\), we have a model \(m_t\) with parameters \(a_t\) of dimensionality \(d\). We can write the parameters as a column vector \(a_t = \begin{bmatrix}a_{1, t} \ \ldots \ a_{d, t} \end{bmatrix}^\top \). We now stack these column vectors \(a_1, \ldots, a_T\) column by column to form a matrix \(A \in &lt;br&gt;
\mathbb{R}^{d \times T}\). The \(i\)-th row of \(A\) then contains the parameter \(a_{i, \cdot}\) corresponding to the \(i\)-th feature of the model for every task, while the \(j\)-th column of \(A\) contains the parameters \(a_{\cdot,j}\) corresponding to the \(j\)-th model.&lt;/p&gt;

&lt;p&gt;Many existing methods make some sparsity assumption with regard to the parameters of our models. [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/multi-task/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;] assume that all models share a small set of features. In terms of our task parameter matrix \(A\), this means that all but a few rows are \(0\), which corresponds to only a few features being used across &lt;em&gt;all&lt;/em&gt; tasks. In order to enforce this, they generalize the \(\ell_1\) norm to the MTL setting. Recall that the \(\ell_1\) norm is a constraint on the sum of the parameters, which forces all but a few parameters to be exactly \(0\). It is also known as lasso (&lt;strong&gt;l&lt;/strong&gt;east &lt;strong&gt;a&lt;/strong&gt;bsolute &lt;strong&gt;s&lt;/strong&gt;hrinkage and &lt;strong&gt;s&lt;/strong&gt;election &lt;strong&gt;o&lt;/strong&gt;perator).&lt;/p&gt;

&lt;p&gt;While in the single-task setting, the \(\ell_1\) norm is computed based on the parameter vector \(a_t\) of the respective task \(t\), for MTL we compute it over our task parameter matrix \(A\). In order to do this, we first compute an \(\ell_q\) norm across each row \(a_i\) containing the parameter corresponding to the \(i\)-th feature across all tasks, which yields a vector \(b = \begin{bmatrix}\|a_1\|_q \ldots \|a_d\|_q \end{bmatrix} \in \mathbb{R}^d\). We then compute the \(\ell_1\) norm of this vector, which forces all but a few entries of \(b\), i.e. rows in \(A\) to be \(0\).&lt;/p&gt;

&lt;p&gt;As we can see, depending on what constraint we would like to place on each row, we can use a different \(\ell_q\). In general, we refer to these mixed-norm constraints as \(\ell_1/\ell_q\) norms. They are also known as block-sparse regularization, as they lead to entire rows of \(A\) being set to \(0\). [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/multi-task/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] use \(\ell_1/\ell_\infty\) regularization, while Argyriou et al. (2007) use a mixed \(\ell_1/\ell_2\) norm. The latter is also known as group lasso and was first proposed by [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/multi-task/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Argyriou et al. (2007) also show that the problem of optimizing the non-convex group lasso can be made convex by penalizing the trace norm of \(A\), which forces \(A\) to be low-rank and thereby constrains the column parameter vectors \(a_{\cdot, 1}, \ldots, a_{\cdot, t}\) to live in a low-dimensional subspace. [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/multi-task/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] furthermore establish upper bounds for using the group lasso in multi-task learning.&lt;/p&gt;

&lt;p&gt;As much as this block-sparse regularization is intuitively plausible, it is very dependent on the extent to which the features are shared across tasks. [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/multi-task/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] show that if features do not overlap by much, \(\ell_1/\ell_q\) regularization might actually be worse than element-wise \(\ell_1\) regularization.&lt;/p&gt;

&lt;p&gt;For this reason, [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/multi-task/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] improve upon block-sparse models by proposing a method that combines block-sparse and element-wise sparse regularization. They decompose the task parameter matrix \(A\) into two matrices \(B\) and \(S\) where \(A = B + S\). \(B\) is then enforced to be block-sparse using \(\ell_1/\ell_\infty\) regularization, while \(S\) is made element-wise sparse using lasso. Recently, [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/multi-task/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] propose a distributed version of group-sparse regularization.&lt;/p&gt;

&lt;h2 id="learningtaskrelationships"&gt;Learning task relationships&lt;/h2&gt;

&lt;p&gt;While the group-sparsity constraint forces our model to only consider a few features, these features are largely used across all tasks. All of the previous approaches thus assume that the tasks used in multi-task learning are closely related. However, each task might not be closely related to all of the available tasks. In those cases, sharing information with an unrelated task might actually hurt performance, a phenomenon known as negative transfer. &lt;/p&gt;

&lt;p&gt;Rather than sparsity, we would thus like to leverage prior knowledge indicating that some tasks are related while others are not. In this scenario, a constraint that enforces a clustering of tasks might be more appropriate. [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/multi-task/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] suggest to impose a clustering constraint by penalizing both the norms of our task column vectors \(a_{\cdot, 1}, \ldots, a_{\cdot, t}\) as well as their variance with the following constraint:&lt;/p&gt;

&lt;p&gt;\(\Omega = \|\bar{a}\|^2 + \dfrac{\lambda}{T} \sum^T_{t=1} \| a_{\cdot, t} - \bar{a} \|^2 \)&lt;/p&gt;

&lt;p&gt;where \(\bar{a} = (\sum^T_{t=1} a_{\cdot, t})/T \) is the mean parameter vector. This penalty enforces a clustering of the task parameter vectors \(a_{\cdot, 1}, \ldots, a_{\cdot, t}\) towards their mean that is controlled by \(\lambda\). They apply this constraint to kernel methods, but it is equally applicable to linear models.&lt;/p&gt;

&lt;p&gt;A similar constraint for SVMs was also proposed by [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/multi-task/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;]. Their constraint is inspired by Bayesian methods and seeks to make all models close to some mean model. In SVMs, the loss thus trades off having a large margin for each SVM with being close to the mean model.&lt;/p&gt;

&lt;p&gt;[&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/multi-task/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] make the assumptions underlying cluster regularization more explicit by formalizing a cluster constraint on \(A\) under the assumption that the number of clusters \(C\) is known in advance. They then decompose the penalty into three separate norms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A global penalty which measures how large our column parameter vectors are on average: \(\Omega_{mean}(A) = \|\bar{a}\|^2 \).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A measure of between-cluster variance that measures how close to each other the clusters are:  \(\Omega_{between}(A) = \sum^C_{c=1} T_c \| \bar{a}_c - \bar{a} \|^2 \) where \(T_c\) is the number of tasks in the \(c\)-th cluster and \(\bar{a}_c\) is the mean vector of the task parameter vectors in the \(c\)-th cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A measure of within-cluster variance that gauges how compact each cluster is: \(\Omega_{within} = \sum^C_{c=1} \sum_{t \in J(c)} \| a_{\cdot, t} - \bar{a}_c \|^2 \) where \(J(c)\) is the set of tasks in the \(c\)-th cluster.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final constraint then is the weighted sum of the three norms:&lt;/p&gt;

&lt;p&gt;\(\Omega(A) = \lambda_1 \Omega_{mean}(A) + \lambda_2 \Omega_{between}(A) + \lambda_3 \Omega_{within}(A)\).&lt;/p&gt;

&lt;p&gt;As this constraint assumes clusters are known in advance, they introduce a convex relaxation of the above penalty that allows to learn the clusters at the same time.&lt;/p&gt;

&lt;p&gt;In another scenario, tasks might not occur in clusters but have an inherent structure. [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/multi-task/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] extend the group lasso to deal with tasks that occur in a tree structure, while [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/multi-task/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] apply it to tasks with graph structures.&lt;/p&gt;

&lt;p&gt;While the previous approaches to modelling the relationship between tasks employ norm regularization, other approaches do so without regularization: [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/multi-task/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] were the first ones who presented a task clustering algorithm using k-nearest neighbour, while [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/multi-task/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] learn a common structure from multiple related tasks with an application to semi-supervised learning.&lt;/p&gt;

&lt;p&gt;Much other work on learning task relationships for multi-task learning uses Bayesian methods: &lt;br&gt;
[&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/multi-task/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;] propose a Bayesian neural network for multi-task learning by placing a prior on the model parameters to encourage similar parameters across tasks. [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/multi-task/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;] extend Gaussian processes (GP) to MTL by inferring parameters for a shared covariance matrix. As this is computationally very expensive, they adopt a sparse approximation scheme that greedily selects the most informative examples. [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/multi-task/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;] also use GP for MTL by assuming that all models are sampled from a common prior. &lt;/p&gt;

&lt;p&gt;[&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/multi-task/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] place a Gaussian as a prior distribution on each task-specific layer. In order to encourage similarity between different tasks, they propose to make the mean task-dependent and introduce a clustering of the tasks using a mixture distribution. Importantly, they require task characteristics that define the clusters and the number of mixtures to be specified in advance.&lt;/p&gt;

&lt;p&gt;Building on this, [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/multi-task/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] draw the distribution from a Dirichlet process and enable the model to learn the similarity between tasks as well as the number of clusters. They then share the same model among all tasks in the same cluster. [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/multi-task/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] propose a hierarchical Bayesian model, which learns a latent task hierarchy, while [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/multi-task/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] use a GP-based regularization for MTL and extend a previous GP-based approach to be more computationally feasible in larger settings. &lt;/p&gt;

&lt;p&gt;Other approaches focus on the online multi-task learning setting: [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/multi-task/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] adapt some existing methods such as the approach by Evgeniou et al. (2005) to the online setting. They also propose a MTL extension of the regularized Perceptron, which encodes task relatedness in a matrix. They use different forms of regularization to bias this task relatedness matrix, e.g. the closeness of the task characteristic vectors or the dimension of the spanned subspace. Importantly, similar to some earlier approaches, they require the task characteristics that make up this matrix to be provided in advance. [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/multi-task/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;] then extend the previous approach by learning the task relationship matrix.&lt;/p&gt;

&lt;p&gt;[&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/multi-task/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;] assume that tasks form disjoint groups and that the tasks within each group lie in a low-dimensional subspace. Within each group, tasks share the same feature representation whose parameters are learned jointly together with the group assignment matrix using an alternating minimization scheme. However, a total disjointness between groups might not be the ideal way, as the tasks might still share some features that are helpful for prediction. &lt;/p&gt;

&lt;p&gt;[&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/multi-task/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;] in turn allow two tasks from different groups to overlap by assuming that there exist a small number of latent basis tasks. They then model the parameter vector \(a_t\) of every actual task \(t\) as a linear combination of these: \(a_t = Ls_t\) where \(L\ \in \mathbb{R}^{k \times d}\) is a matrix containing the parameter vectors of \(k\) latent tasks, while \(s_t\ \in \mathbb{R}^k\) is a vector containing the coefficients of the linear combination. In addition, they constrain the linear combination to be sparse in the latent tasks; the overlap in the sparsity patterns between two tasks then controls the amount of sharing between these. Finally, [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/multi-task/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;] learn a small pool of shared hypotheses and then map each task to a single hypothesis.&lt;/p&gt;

&lt;h1 id="recentworkonmtlfordeeplearning"&gt;Recent work on MTL for Deep Learning&lt;/h1&gt;

&lt;p&gt;While many recent Deep Learning approaches have used multi-task learning -- either explicitly or implicitly -- as part of their model (prominent examples will be featured in the next section), they all employ the two approaches we introduced earlier, hard and soft parameter sharing. In contrast, only a few papers have looked at developing better mechanisms for MTL in deep neural networks.&lt;/p&gt;

&lt;h2 id="deeprelationshipnetworks"&gt; Deep Relationship Networks&lt;/h2&gt;

&lt;p&gt;In MTL for computer vision, approaches often share the convolutional layers, while learning task-specific fully-connected layers. [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/multi-task/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;] improve upon these models by proposing Deep Relationship Networks. In addition to the structure of shared and task-specific layers, which can be seen in Figure 3, they place matrix priors on the fully connected layers, which allow the model to learn the relationship between tasks, similar to some of the Bayesian models we have looked at before. This approach, however, still relies on a pre-defined structure for sharing, which may be adequate for well-studied computer vision problems, but prove error-prone for novel tasks.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/relationship_networks.png" style="width: 100%" title="Deep Relationship Networks" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 3: A Deep Relationship Network with shared convolutional and task-specific fully connected layers with matrix priors (Long and Wang, 2015).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="fullyadaptivefeaturesharing"&gt;Fully-Adaptive Feature Sharing&lt;/h2&gt;

&lt;p&gt;Starting at the other extreme, [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/multi-task/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] propose a bottom-up approach that starts with a thin network and dynamically widens it greedily during training using a criterion that promotes grouping of similar tasks. The widening procedure, which dynamically creates branches can be seen in Figure 4. However, the greedy method might not be able to discover a model that is globally optimal, while assigning each branch to exactly one task does not allow the model to learn more complex interactions between tasks.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/fully_adaptive_feature_sharing.png" style="width: 100%" title="Fully-Adaptive Feature Sharing" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 4: The widening procedure for fully-adaptive feature sharing (Lu et al., 2016).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="crossstitchnetworks"&gt;Cross-stitch Networks&lt;/h2&gt;

&lt;p&gt;[&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/multi-task/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;] start out with two separate model architectures just as in soft parameter sharing. They then use what they refer to as cross-stitch units to allow the model to determine in what way the task-specific networks leverage the knowledge of the other task by learning a linear combination of the output of the previous layers. Their architecture can be seen in Figure 5, in which they only place cross-stitch units after pooling and fully-connected layers.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/cross-stitch_networks.png" style="width: 70%" title="Cross-stitch networks" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 5: Cross-stitch networks for two tasks (Misra et al., 2016).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="lowsupervision"&gt;Low supervision&lt;/h2&gt;

&lt;p&gt;In contrast, in natural language processing (NLP), recent work focused on finding better task hierarchies for multi-task learning: [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/multi-task/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;] show that low-level tasks, i.e. NLP tasks typically used for preprocessing such as part-of-speech tagging and named entity recognition, should be supervised at lower layers when used as auxiliary task.&lt;/p&gt;

&lt;h2 id="ajointmanytaskmodel"&gt; A Joint Many-Task Model&lt;/h2&gt;

&lt;p&gt;Building on this finding, [&lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/multi-task/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;] pre-define a hierarchical architecture consisting of several NLP tasks, which can be seen in Figure 6, as a joint model for multi-task learning.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/joint_many_task_model.png" style="width: 60%" title="Joint Many-Task Model" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 6: A Joint Many-Task Model (Hashimoto et al., 2016).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="weightinglosseswithuncertainty"&gt; Weighting losses with uncertainty&lt;/h2&gt;

&lt;p&gt;Instead of learning the structure of sharing, [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/multi-task/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;] take a orthogonal approach by considering the uncertainty of each task. They then adjust each task's relative weight in the cost function by deriving a multi-task loss function based on maximizing the Gaussian likelihood with task-dependant uncertainty. Their architecture for per-pixel depth regression, semantic and instance segmentation can be seen in Figure 7.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/weighting_using_uncertainty.png" style="width: 70%" title="Uncertainty-based loss function weighting" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 7: Uncertainty-based loss function weighting for multi-task learning (Kendall et al., 2017).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="tensorfactorisationformtl"&gt;Tensor factorisation for MTL&lt;/h2&gt;

&lt;p&gt;More recent work seeks to generalize existing approaches to MTL to Deep Learning: [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/multi-task/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;] generalize some of the previously discussed matrix factorisation approaches using tensor factorisation to split the model parameters into shared and task-specific parameters for every layer.&lt;/p&gt;

&lt;h2 id="sluicenetworks"&gt;Sluice Networks&lt;/h2&gt;

&lt;p&gt;Finally, we propose Sluice Networks [&lt;sup id="fnref:45"&gt;&lt;a href="http://ruder.io/multi-task/#fn:45" rel="footnote"&gt;45&lt;/a&gt;&lt;/sup&gt;], a model that generalizes Deep Learning-based MTL approaches such as hard parameter sharing and cross-stitch networks, block-sparse regularization approaches, as well as recent NLP approaches that create a task hierarchy. The model, which can be seen in Figure 8, allows to learn what layers and subspaces should be shared, as well as at what layers the network has learned the best representations of the input sequences.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/05/sluice_network-003.png" style="width: 70%" title="Sluice networks" alt="An Overview of Multi-Task Learning in Deep Neural Networks"&gt;
&lt;figcaption&gt;Figure 8: A sluice network for two tasks (Ruder et al., 2017).&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="whatshouldishareinmymodel"&gt; What should I share in my model?&lt;/h2&gt;

&lt;p&gt;Having surveyed these recent approaches, let us now briefly summarize and draw a conclusion on what to share in our deep MTL models. Most approaches in the history of MTL have focused on the scenario where tasks are drawn from the same distribution (Baxter, 1997). While this scenario is beneficial for sharing, it does not always hold. In order to develop robust models for MTL, we thus have to be able to deal with unrelated or only loosely related tasks.&lt;/p&gt;

&lt;p&gt;While early work in MTL for Deep Learning has pre-specified which layers to share for each task pairing, this strategy does not scale and heavily biases MTL architectures. Hard parameter sharing, a technique that was originally proposed by Caruana (1996), is still the norm 20 years later. While useful in many scenarios, hard parameter sharing quickly breaks down if tasks are not closely related or require reasoning on different levels. Recent approaches have thus looked towards &lt;em&gt;learning&lt;/em&gt; what to share and generally outperform hard parameter sharing. In addition, giving our models the capacity to learn a task hierarchy is helpful, particularly in cases that require different granularities.&lt;/p&gt;

&lt;p&gt;As mentioned initially, we are doing MTL as soon as we are optimizing more than one loss function. Rather than constraining our model to compress the knowledge of all tasks into the same parameter space, it is thus helpful to draw on the advances in MTL that we have discussed and enable our model to learn how the tasks should interact with each other.&lt;/p&gt;

&lt;h1 id="auxiliarytasks"&gt;Auxiliary tasks&lt;/h1&gt;

&lt;p&gt;MTL is a natural fit in situations where we are interested in obtaining predictions for multiple tasks at once. Such scenarios are common for instance in finance or economics forecasting, where we might want to predict the value of many possibly related indicators, or in bioinformatics where we might want to predict symptoms for multiple diseases simultaneously. In scenarios such as drug discovery, where tens or hundreds of active compounds should be predicted, MTL accuracy increases continuously with the number of tasks (Ramsundar et al., 2015).&lt;/p&gt;

&lt;p&gt;In most situations, however, we only care about performance on one task. In this section, we will thus look at how we can find a suitable auxiliary task in order to still reap the benefits of multi-task learning. &lt;/p&gt;

&lt;h2 id="relatedtask"&gt; Related task&lt;/h2&gt;

&lt;p&gt;Using a related task as an auxiliary task for MTL is the classical choice. To get an idea what a related task can be, we will present some prominent examples. Caruana (1998) uses tasks that predict different characteristics of the road as auxiliary tasks for predicting the steering direction in a self-driving car; [&lt;sup id="fnref:46"&gt;&lt;a href="http://ruder.io/multi-task/#fn:46" rel="footnote"&gt;46&lt;/a&gt;&lt;/sup&gt;] use head pose estimation and facial attribute inference as auxiliary tasks for facial landmark detection; [&lt;sup id="fnref:47"&gt;&lt;a href="http://ruder.io/multi-task/#fn:47" rel="footnote"&gt;47&lt;/a&gt;&lt;/sup&gt;] jointly learn query classification and web search; Girshick (2015) jointly predicts the class and the coordinates of an object in an image; finally, [&lt;sup id="fnref:48"&gt;&lt;a href="http://ruder.io/multi-task/#fn:48" rel="footnote"&gt;48&lt;/a&gt;&lt;/sup&gt;] jointly predict the phoneme duration and frequency profile for text-to-speech.&lt;/p&gt;

&lt;h2 id="adversarial"&gt; Adversarial&lt;/h2&gt;

&lt;p&gt;Often, labeled data for a related task is unavailable. In some circumstances, however, we have access to a task that is &lt;em&gt;opposite&lt;/em&gt; of what we want to achieve. This data can be leveraged using an adversarial loss, which does not seek to minimize but maximize the training error using a gradient reversal layer. This setup has found recent success in domain adaptation [&lt;sup id="fnref:49"&gt;&lt;a href="http://ruder.io/multi-task/#fn:49" rel="footnote"&gt;49&lt;/a&gt;&lt;/sup&gt;]. The adversarial task in this case is predicting the domain of the input; by reversing the gradient of the adversarial task, the adversarial task loss is maximized, which is beneficial for the main task as it forces the model to learn representations that cannot distinguish between domains.&lt;/p&gt;

&lt;h2 id="hints"&gt; Hints&lt;/h2&gt;

&lt;p&gt;As mentioned before, MTL can be used to learn features that might not be easy to learn just using the original task. An effective way to achieve this is to use hints, i.e. predicting the features as an auxiliary task. Recent examples of this strategy in the context of natural language processing are [&lt;sup id="fnref:50"&gt;&lt;a href="http://ruder.io/multi-task/#fn:50" rel="footnote"&gt;50&lt;/a&gt;&lt;/sup&gt;] who predict whether an input sentence contains a positive or negative sentiment word as auxiliary tasks for sentiment analysis and [&lt;sup id="fnref:51"&gt;&lt;a href="http://ruder.io/multi-task/#fn:51" rel="footnote"&gt;51&lt;/a&gt;&lt;/sup&gt;] who predict whether a name is present in a sentence as auxiliary task for name error detection.&lt;/p&gt;

&lt;h2 id="focusingattention"&gt;Focusing attention&lt;/h2&gt;

&lt;p&gt;Similarly, the auxiliary task can be used to focus attention on parts of the image that a network might normally ignore. For instance, for learning to steer (Caruana, 1998) a single-task model might typically ignore lane markings as these make up only a small part of the image and are not always present. Predicting lane markings as auxiliary task, however, forces the model to learn to represent them; this knowledge can then also be used for the main task. Analogously, for facial recognition, one might learn to predict the location of facial landmarks as auxiliary tasks, since these are often distinctive.&lt;/p&gt;

&lt;h2 id="quantizationsmoothing"&gt;Quantization smoothing&lt;/h2&gt;

&lt;p&gt;For many tasks, the training objective is quantized, i.e. while a continuous scale might be more plausible, labels are available as a discrete set. This is the case in many scenarios that require human assessment for data gathering, such as predicting the risk of a disease (e.g. low/medium/high) or sentiment analysis (positive/neutral/negative). Using less quantized auxiliary tasks might help in these cases, as they might be learned more easily due to their objective being smoother. &lt;/p&gt;

&lt;h2 id="predictinginputs"&gt; Predicting inputs&lt;/h2&gt;

&lt;p&gt;In some scenarios, it is impractical to use some features as inputs as they are unhelpful for predicting the desired objective. However, they might still be able to guide the learning of the task. In those cases, the features can be used as outputs rather than inputs. [&lt;sup id="fnref:52"&gt;&lt;a href="http://ruder.io/multi-task/#fn:52" rel="footnote"&gt;52&lt;/a&gt;&lt;/sup&gt;] present several problems where this is applicable.&lt;/p&gt;

&lt;h2 id="usingthefuturetopredictthepresent"&gt;Using the future to predict the present&lt;/h2&gt;

&lt;p&gt;In many situations, some features only become available &lt;em&gt;after&lt;/em&gt; the predictions are supposed to be made. For instance, for self-driving cars, more accurate measurements of obstacles and lane markings can be made once the car is passing them. Caruana (1998) also gives the example of pneumonia prediction, after which the results of additional medical trials will be available. For these examples, the additional data cannot be used as features as it will not be available as input at runtime. However, it can be used as an auxiliary task to impart additional knowledge to the model during training.&lt;/p&gt;

&lt;h2 id="representationlearning"&gt;Representation learning&lt;/h2&gt;

&lt;p&gt;The goal of an auxiliary task in MTL is to enable the model to learn representations that are shared or helpful for the main task. All auxiliary tasks discussed so far do this implicitly: They are closely related to the main task, so that learning them likely allows the model to learn beneficial representations. A more explicit modelling is possible, for instance by employing a task that is known to enable a model to learn transferable representations. The language modelling objective as employed by Cheng et al. (2015) and [&lt;sup id="fnref:53"&gt;&lt;a href="http://ruder.io/multi-task/#fn:53" rel="footnote"&gt;53&lt;/a&gt;&lt;/sup&gt;] fulfils this role. In a similar vein, an autoencoder objective can also be used as an auxiliary task.&lt;/p&gt;

&lt;h2 id="whatauxiliarytasksarehelpful"&gt;What auxiliary tasks are helpful?&lt;/h2&gt;

&lt;p&gt;In this section, we have discussed different auxiliary tasks that can be used to leverage MTL even if we only care about one task. We still do not know, though, what auxiliary task will be useful in practice. Finding an auxiliary task is largely based on the assumption that the auxiliary task should be related to the main task in some way and that it should be helpful for predicting the main task.&lt;/p&gt;

&lt;p&gt;However, we still do not have a good notion of when two tasks should be considered similar or related. Caruana (1998) defines two tasks to be similar if they use the same features to make a decision. Baxter (2000) argues only theoretically that related tasks share a common optimal hypothesis class, i.e. have the same inductive bias. [&lt;sup id="fnref:54"&gt;&lt;a href="http://ruder.io/multi-task/#fn:54" rel="footnote"&gt;54&lt;/a&gt;&lt;/sup&gt;] propose that two tasks are \(\mathcal{F}\)-related if the data for both tasks can be generated from a fixed probability distribution using a set of transformations \(\mathcal{F}\). While this allows to reason over tasks where different sensors collect data for the same classification problem, e.g. object recognition with data from cameras with different angles and lighting conditions, it is not applicable to tasks that do not deal with the same problem. Xue et al. (2007) finally argue that two tasks are similar if their classification boundaries, i.e. parameter vectors are close.&lt;/p&gt;

&lt;p&gt;In spite of these early theoretical advances in understanding task relatedness, we have not made much recent progress towards this goal. Task similarity is not binary, but resides on a spectrum. More similar tasks should help more in MTL, while less similar tasks should help less. Allowing our models to learn what to share with each task might allow us to temporarily circumvent the lack of theory and make better use even of only loosely related tasks. However, we also need to develop a more principled notion of task similarity with regard to multi-task learning in order to know which tasks we should prefer.&lt;/p&gt;

&lt;p&gt;Recent work [&lt;sup id="fnref:55"&gt;&lt;a href="http://ruder.io/multi-task/#fn:55" rel="footnote"&gt;55&lt;/a&gt;&lt;/sup&gt;] have found auxiliary tasks with compact and uniform label distributions to be preferable for sequence tagging problems in NLP, which we have confirmed in experiments (Ruder et al., 2017). In addition, gains have been found to be more likely for main tasks that quickly plateau with non-plateauing auxiliary tasks [&lt;sup id="fnref:56"&gt;&lt;a href="http://ruder.io/multi-task/#fn:56" rel="footnote"&gt;56&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;These experiments, however, have so far been limited in scope and recent findings only provide the first clues towards a deeper understanding of multi-task learning in neural networks. &lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this overview, I have reviewed both the history of literature in multi-task learning as well as more recent work on MTL for Deep Learning. While MTL is being more frequently used, the 20-year old hard parameter sharing paradigm is still pervasive for neural-network based MTL. Recent advances on learning what to share, however, are promising. At the same time, our understanding of tasks -- their similarity, relationship, hierarchy, and benefit for MTL -- is still limited and we need to learn more about them to gain a better understanding of the generalization capabilities of MTL with regard to deep neural networks.&lt;/p&gt;

&lt;p&gt;I hope you found this overview helpful. If I made any error, missed a reference, or misrepresented some aspect, or if you would just like to share your thoughts, please leave a comment below.&lt;/p&gt;

&lt;h1 id="printableversionandcitation"&gt;Printable version and citation&lt;/h1&gt;

&lt;p&gt;This blog post is also available as an &lt;a href="https://arxiv.org/abs/1706.05098"&gt;article on arXiv&lt;/a&gt;, in case you want to refer to it later.&lt;/p&gt;

&lt;p&gt;In case you found it helpful, consider citing the corresponding arXiv article as: &lt;br&gt;
&lt;em&gt;Sebastian Ruder (2017). An Overview of Multi-Task Learning in Deep Neural Networks. arXiv preprint arXiv:1706.05098.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. &lt;a href="https://doi.org/10.1145/1390156.1390177"&gt;https://doi.org/10.1145/1390156.1390177&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Deng, L., Hinton, G. E., &amp;amp; Kingsbury, B. (2013). New types of deep neural network learning for speech recognition and related applications: An overview. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 8599–8603. &lt;a href="https://doi.org/10.1109/ICASSP.2013.6639344"&gt;https://doi.org/10.1109/ICASSP.2013.6639344&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Girshick, R. (2015). Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1440–1448). &lt;a href="https://doi.org/10.1109/iccv.2015.169"&gt;https://doi.org/10.1109/iccv.2015.169&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Ramsundar, B., Kearnes, S., Riley, P., Webster, D., Konerding, D., &amp;amp; Pande, V. (2015). Massively Multitask Networks for Drug Discovery. &lt;a href="https://doi.org/https://arxiv.org/abs/1502.02072"&gt;https://doi.org/https://arxiv.org/abs/1502.02072&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Caruana, R. (1998). Multitask Learning. Autonomous Agents and Multi-Agent Systems, 27(1), 95–133. &lt;a href="https://doi.org/10.1016/j.csl.2009.08.003"&gt;https://doi.org/10.1016/j.csl.2009.08.003&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Caruana, R. "Multitask learning: A knowledge-based source of inductive bias." Proceedings of the Tenth International Conference on Machine Learning. 1993. &lt;a href="http://ruder.io/multi-task/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Baxter, J. (1997). A Bayesian/information theoretic model of learning to learn via multiple task sampling. Machine Learning, 28, 7–39. Retrieved from &lt;a href="http://link.springer.com/article/10.1023/A:1007327622663"&gt;http://link.springer.com/article/10.1023/A:1007327622663&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Duong, L., Cohn, T., Bird, S., &amp;amp; Cook, P. (2015). Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), 845–850.  &lt;a href="http://ruder.io/multi-task/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Yang, Y., &amp;amp; Hospedales, T. M. (2017). Trace Norm Regularised Deep Multi-Task Learning. In Workshop track - ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04038"&gt;http://arxiv.org/abs/1606.04038&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Abu-Mostafa, Y. S. (1990). Learning from hints in neural networks. Journal of Complexity, 6(2), 192–198. &lt;a href="https://doi.org/10.1016/0885-064X(90)90006-Y"&gt;https://doi.org/10.1016/0885-064X(90)90006-Y&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Baxter, J. (2000). A Model of Inductive Bias Learning. Journal of Artificial Intelligence Research, 12, 149–198. &lt;a href="http://ruder.io/multi-task/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Argyriou, A., &amp;amp; Pontil, M. (2007). Multi-Task Feature Learning. In Advances in Neural Information Processing Systems. &lt;a href="http://doi.org/10.1007/s10994-007-5040-8"&gt;http://doi.org/10.1007/s10994-007-5040-8&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;C.Zhang and J.Huang. Model selection consistency of the lasso selection in high-dimensional linear regression. Annals of Statistics, 36:1567–1594, 2008 &lt;a href="http://ruder.io/multi-task/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Yuan, Ming, and Yi Lin. "Model selection and estimation in regression with grouped variables." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68.1 (2006): 49-67. &lt;a href="http://ruder.io/multi-task/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Lounici, K., Pontil, M., Tsybakov, A. B., &amp;amp; van de Geer, S. (2009). Taking Advantage of Sparsity in Multi-Task Learning. Stat, (1). Retrieved from &lt;a href="http://arxiv.org/pdf/0903.1468"&gt;http://arxiv.org/pdf/0903.1468&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Negahban, S., &amp;amp; Wainwright, M. J. (2008). Joint support recovery under high-dimensional scaling : Benefits and perils of \(\ell_{1,\infty}\)-regularization. Advances in Neural Information Processing Systems, 1161–1168.  &lt;a href="http://ruder.io/multi-task/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Jalali, A., Ravikumar, P., Sanghavi, S., &amp;amp; Ruan, C. (2010). A Dirty Model for Multi-task Learning. Advances in Neural Information Processing Systems. Retrieved from &lt;a href="https://papers.nips.cc/paper/4125-a-dirty-model-for-multi-task-learning.pdf"&gt;https://papers.nips.cc/paper/4125-a-dirty-model-for-multi-task-learning.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Liu, S., Pan, S. J., &amp;amp; Ho, Q. (2016). Distributed Multi-task Relationship Learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) (pp. 751–760). Retrieved from &lt;a href="http://arxiv.org/abs/1612.04022"&gt;http://arxiv.org/abs/1612.04022&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Evgeniou, T., Micchelli, C., &amp;amp; Pontil, M. (2005). Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6, 615–637. Retrieved from &lt;a href="http://discovery.ucl.ac.uk/13423/"&gt;http://discovery.ucl.ac.uk/13423/&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Evgeniou, T., &amp;amp; Pontil, M. (2004). Regularized multi-task learning. International Conference on Knowledge Discovery and Data Mining, 109. &lt;a href="https://doi.org/10.1145/1014052.1014067"&gt;https://doi.org/10.1145/1014052.1014067&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Jacob, L., Vert, J., Bach, F. R., &amp;amp; Vert, J. (2009). Clustered Multi-Task Learning: A Convex Formulation. Advances in Neural Information Processing Systems 21, 745–752. Retrieved from &lt;a href="http://eprints.pascal-network.org/archive/00004705/%5Cnhttp://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf"&gt;http://eprints.pascal-network.org/archive/00004705/%5Cnhttp://papers.nips.cc/paper/3499-clustered-multi-task-learning-a-convex-formulation.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Kim, S., &amp;amp; Xing, E. P. (2010). Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity. 27th International Conference on Machine Learning, 1–14. &lt;a href="https://doi.org/10.1214/12-AOAS549"&gt;https://doi.org/10.1214/12-AOAS549&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Chen, X., Kim, S., Lin, Q., Carbonell, J. G., &amp;amp; Xing, E. P. (2010). Graph-Structured Multi-task Regression and an Efficient Optimization Method for General Fused Lasso, 1–21. &lt;a href="https://doi.org/10.1146/annurev.arplant.56.032604.144204"&gt;https://doi.org/10.1146/annurev.arplant.56.032604.144204&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Thrun, S., &amp;amp; O’Sullivan, J. (1996). Discovering Structure in Multiple Learning Tasks: The TC Algorithm. Proceedings of the Thirteenth International Conference on Machine Learning, 28(1), 5–5. Retrieved from &lt;a href="http://scholar.google.com/scholar?cluster=956054018507723832&amp;amp;hl=en"&gt;http://scholar.google.com/scholar?cluster=956054018507723832&amp;amp;hl=en&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Ando, R. K., &amp;amp; Tong, Z. (2005). A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. Journal of Machine Learning Research, 6, 1817–1853. &lt;a href="http://ruder.io/multi-task/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Heskes, T. (2000). Empirical Bayes for Learning to Learn. Proceedings of the Seventeenth International Conference on Machine Learning, 367–364. &lt;a href="http://ruder.io/multi-task/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Lawrence, N. D., &amp;amp; Platt, J. C. (2004). Learning to learn with the informative vector machine. Twenty-First International Conference on Machine Learning  - ICML ’04, 65. &lt;a href="https://doi.org/10.1145/1015330.1015382"&gt;https://doi.org/10.1145/1015330.1015382&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Yu, K., Tresp, V., &amp;amp; Schwaighofer, A. (2005). Learning Gaussian processes from multiple tasks. Proceedings of the International Conference on Machine Learning (ICML), 22, 1012–1019. &lt;a href="https://doi.org/10.1145/1102351.1102479"&gt;https://doi.org/10.1145/1102351.1102479&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Bakker, B., &amp;amp; Heskes, T. (2003). Task Clustering and Gating for Bayesian Multitask Learning. Journal of Machine Learning Research, 1(1), 83–99. &lt;a href="https://doi.org/10.1162/153244304322765658"&gt;https://doi.org/10.1162/153244304322765658&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Xue, Y., Liao, X., Carin, L., &amp;amp; Krishnapuram, B. (2007). Multi-Task Learning for Classification with Dirichlet Process Priors. Journal of Machine Learning Research, 8, 35–63. &lt;a href="http://ruder.io/multi-task/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Daumé III, H. (2009). Bayesian multitask learning with latent hierarchies, 135–142. Retrieved from &lt;a href="http://dl.acm.org.sci-hub.io/citation.cfm?id=1795131"&gt;http://dl.acm.org.sci-hub.io/citation.cfm?id=1795131&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Zhang, Y., &amp;amp; Yeung, D. (2010). A Convex Formulation for Learning Task Relationships in Multi-Task Learning. Uai, 733–442. &lt;a href="http://ruder.io/multi-task/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Cavallanti, G., Cesa-Bianchi, N., &amp;amp; Gentile, C. (2010). Linear Algorithms for Online Multitask Classification. Journal of Machine Learning Research, 11, 2901–2934. &lt;a href="http://ruder.io/multi-task/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Saha, A., Rai, P., Daumé, H., &amp;amp; Venkatasubramanian, S. (2011). Online learning of multiple tasks and their relationships. Journal of Machine Learning Research, 15, 643–651. Retrieved from &lt;a href="http://www.scopus.com/inward/record.url?eid=2-s2.0-84862275213&amp;amp;partnerID=tZOtx3y1"&gt;http://www.scopus.com/inward/record.url?eid=2-s2.0-84862275213&amp;amp;partnerID=tZOtx3y1&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Kang, Z., Grauman, K., &amp;amp; Sha, F. (2011). Learning with whom to share in multi-task feature learning. Proceedings of the 28th International Conference on Machine Learning, (4), 4–5. Retrieved from &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper"&gt;http://machinelearning.wustl.edu/mlpapers/paper&lt;/a&gt;&lt;em&gt;files/ICML2011Kang&lt;/em&gt;344.pdf &lt;a href="http://ruder.io/multi-task/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Kumar, A., &amp;amp; Daumé III, H. (2012). Learning Task Grouping and Overlap in Multi-task Learning. Proceedings of the 29th International Conference on Machine Learning, 1383–1390.  &lt;a href="http://ruder.io/multi-task/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Crammer, K., &amp;amp; Mansour, Y. (2012). Learning Multiple Tasks Using Shared Hypotheses. Neural Information Processing Systems (NIPS), 1484–1492 &lt;a href="http://ruder.io/multi-task/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Long, M., &amp;amp; Wang, J. (2015). Learning Multiple Tasks with Deep Relationship Networks. arXiv Preprint arXiv:1506.02117. Retrieved from &lt;a href="http://arxiv.org/abs/1506.02117"&gt;http://arxiv.org/abs/1506.02117&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Lu, Y., Kumar, A., Zhai, S., Cheng, Y., Javidi, T., &amp;amp; Feris, R. (2016). Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification. Retrieved from &lt;a href="http://arxiv.org/abs/1611.05377"&gt;http://arxiv.org/abs/1611.05377&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Misra, I., Shrivastava, A., Gupta, A., &amp;amp; Hebert, M. (2016). Cross-stitch Networks for Multi-task Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. &lt;a href="https://doi.org/10.1109/CVPR.2016.433"&gt;https://doi.org/10.1109/CVPR.2016.433&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Søgaard, A., &amp;amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. &lt;a href="http://ruder.io/multi-task/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Hashimoto, K., Xiong, C., Tsuruoka, Y., &amp;amp; Socher, R. (2016). A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. arXiv Preprint arXiv:1611.01587. Retrieved from &lt;a href="http://arxiv.org/abs/1611.01587"&gt;http://arxiv.org/abs/1611.01587&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Kendall, A., Gal, Y., &amp;amp; Cipolla, R. (2017). Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. Retrieved from &lt;a href="http://arxiv.org/abs/1705.07115"&gt;http://arxiv.org/abs/1705.07115&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Yang, Y., &amp;amp; Hospedales, T. (2017). Deep Multi-task Representation Learning: A Tensor Factorisation Approach. In ICLR 2017. &lt;a href="https://doi.org/10.1002/joe.20070"&gt;https://doi.org/10.1002/joe.20070&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:45"&gt;&lt;p&gt;Ruder, S., Bingel, J., Augenstein, I., &amp;amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. Retrieved from &lt;a href="http://arxiv.org/abs/1705.08142"&gt;http://arxiv.org/abs/1705.08142&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:45" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:46"&gt;&lt;p&gt;Zhang, Z., Luo, P., Loy, C. C., &amp;amp; Tang, X. (2014). Facial Landmark Detection by Deep Multi-task Learning. In European Conference on Computer Vision (pp. 94–108). &lt;a href="https://doi.org/10.1007/978-3-319-10599-4_7"&gt;https://doi.org/10.1007/978-3-319-10599-4_7&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:46" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:47"&gt;&lt;p&gt;Liu, X., Gao, J., He, X., Deng, L., Duh, K., &amp;amp; Wang, Y.-Y. (2015). Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval. Naacl-2015, 912–921. &lt;a href="http://ruder.io/multi-task/#fnref:47" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:48"&gt;&lt;p&gt;Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., … Shoeybi, M. (2017). Deep Voice: Real-time Neural Text-to-Speech. In ICML 2017. &lt;a href="http://ruder.io/multi-task/#fnref:48" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:49"&gt;&lt;p&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). &lt;a href="http://ruder.io/multi-task/#fnref:49" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:50"&gt;&lt;p&gt;Yu, J., &amp;amp; Jiang, J. (2016). Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 236–246. Retrieved from &lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1023.pdf"&gt;http://www.aclweb.org/anthology/D/D16/D16-1023.pdf&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:50" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:51"&gt;&lt;p&gt;Cheng, H., Fang, H., &amp;amp; Ostendorf, M. (2015). Open-Domain Name Error Detection using a Multi-Task RNN. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 737–746). &lt;a href="http://ruder.io/multi-task/#fnref:51" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:52"&gt;&lt;p&gt;Caruana, R., &amp;amp; Sa, V. R. de. (1997). Promoting poor features to supervisors: Some inputs work better as outputs. Advances in Neural Information Processing Systems 9: Proceedings of The 1996 Conference, 9, 389. Retrieved from &lt;a href="http://scholar.google.com/scholar?start=20&amp;amp;q=author:%22Rich+Caruana%22&amp;amp;hl=en#6"&gt;http://scholar.google.com/scholar?start=20&amp;amp;q=author:%22Rich+Caruana%22&amp;amp;hl=en#6&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:52" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:53"&gt;&lt;p&gt;Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In ACL 2017. &lt;a href="http://ruder.io/multi-task/#fnref:53" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:54"&gt;&lt;p&gt;Ben-David, S., &amp;amp; Schuller, R. (2003). Exploiting task relatedness for multiple task learning. Learning Theory and Kernel Machines, 567–580. &lt;a href="https://doi.org/10.1007/978-3-540-45167-9_41"&gt;https://doi.org/10.1007/978-3-540-45167-9_41&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:54" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:55"&gt;&lt;p&gt;Alonso, H. M., &amp;amp; Plank, B. (2017). When is multitask learning effective? Multitask learning for semantic sequence prediction under varying data conditions. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1612.02251"&gt;http://arxiv.org/abs/1612.02251&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:55" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:56"&gt;&lt;p&gt;Bingel, J., &amp;amp; Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08303"&gt;http://arxiv.org/abs/1702.08303&lt;/a&gt; &lt;a href="http://ruder.io/multi-task/#fnref:56" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Transfer Learning - Machine Learning's Next Frontier</title><description>This blog post gives an overview of transfer learning, outlines why it is important, and presents applications and practical methods.</description><link>http://ruder.io/transfer-learning/</link><guid isPermaLink="false">97502b6e-1a86-443a-a0ce-272bc5d6fa44</guid><category>deep learning</category><category>nlp</category><category>transfer learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Tue, 21 Mar 2017 15:40:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2017/03/transfer_learning_digits.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2017/03/transfer_learning_digits.png" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;&lt;p&gt;Table of contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#whatistransferlearning"&gt;What is Transfer Learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#whytransferlearningnow"&gt;Why Transfer Learning Now?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#adefinitionoftransferlearning"&gt;A Definition of Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#transferlearningscenarios"&gt;Transfer Learning Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#applicationsoftransferlearning"&gt;Applications of Transfer Learning&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#learningfromsimulations"&gt;Learning from simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#adaptingtonewdomains"&gt;Adapting to new domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#transferringknowledgeacrosslanguages"&gt;Transferring knowledge across languages&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#transferlearningmethods"&gt;Transfer Learning Methods&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#usingpretrainedcnnfeatures"&gt;Using pre-trained CNN features&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#learningdomaininvariantrepresentations"&gt;Learning domain-invariant representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#makingrepresentationsmoresimilar"&gt;Making representations more similar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#confusingdomains"&gt;Confusing domains&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#relatedresearchareas"&gt;Related Research Areas&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#semisupervisedlearning"&gt;Semi-supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#usingavailabledatamoreeffectively"&gt;Using available data more effectively&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#improvingmodelsabilitytogeneralize"&gt;Improving models' ability to generalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#makingmodelsmorerobust"&gt;Making models more robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#multitasklearning"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#continuouslearning"&gt;Continuous learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#zeroshotlearning"&gt;Zero-shot learning&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/transfer-learning/#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In recent years, we have become increasingly good at training deep neural networks to learn a very accurate mapping from inputs to outputs, whether they are images, sentences, label predictions, etc. from large amounts of labeled data.&lt;/p&gt;

&lt;p&gt;What our models still frightfully lack is the ability to generalize to conditions that are different from the ones encountered during training. When is this necessary? Every time you apply your model not to a carefully constructed dataset but to the real world. The real world is messy and contains an infinite number of novel scenarios, many of which your model has not encountered during training and for which it is in turn ill-prepared to make predictions. The ability to transfer knowledge to new conditions is generally known as transfer learning and is what we will discuss in the rest of this post.&lt;/p&gt;

&lt;p&gt;Over the course of this blog post, I will first contrast transfer learning with machine learning's most pervasive and successful paradigm, supervised learning. I will then outline reasons why transfer learning warrants our attention. Subsequently, I will give a more technical definition and detail different transfer learning scenarios. I will then provide examples of applications of transfer learning before delving into practical methods that can be used to transfer knowledge. Finally, I will give an overview of related directions and provide an outlook into the future.&lt;/p&gt;

&lt;h1 id="whatistransferlearning"&gt;What is Transfer Learning?&lt;/h1&gt;

&lt;p&gt;In the classic supervised learning scenario of machine learning, if we intend to train a model for some task and domain \(A\), we assume that we are provided with labeled data for the same task and domain. We can see this clearly in Figure 1, where the task and domain of the training and test data of our model \(A\) is the same. We will later define in more detail what exactly a task and a domain are). For the moment, let us assume that a task is the objective our model aims  to perform, e.g. recognize objects in images, and a domain is where our data is coming from, e.g. images taken in San Francisco coffee shops.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/traditional_ml_setup.png" style="width: 70%" title="Traditional ML setup" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 1: The traditional supervised learning setup in ML&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;We can now train a model \(A\) on this dataset and expect it to perform well on unseen data of the same task and domain. On another occasion, when given data for some other task or domain \(B\), we require again labeled data of the same task or domain that we can use to train a new model \(B\) so that we can expect it to perform well on this data.&lt;/p&gt;

&lt;p&gt;The traditional supervised learning paradigm breaks down when we do not have sufficient labeled data for the task or domain we care about to train a reliable model. &lt;br&gt;
If we want to train a model to detect pedestrians on night-time images, we could apply a model that has been trained on a similar domain, e.g. on day-time images. In practice, however, we often experience a deterioration or collapse in performance as the model has inherited the bias of its training data and does not know how to generalize to the new domain. &lt;br&gt;
If we want to train a model to perform a new task, such as detecting bicyclists, we cannot even reuse an existing model, as the labels between the tasks differ.&lt;/p&gt;

&lt;p&gt;Transfer learning allows us to deal with these scenarios by leveraging the already existing labeled data of some related task or domain. We try to store this knowledge gained in solving the source task in the source domain and apply it to our problem of interest as can be seen in Figure 2. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/transfer_learning_setup.png" style="width: 70%" title="Transfer learning setup" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 2: The transfer learning setup&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In practice, we seek to transfer as much knowledge as we can from the source setting to our target task or domain. This knowledge can take on various forms depending on the data: it can pertain to how objects are composed to allow us to more easily identify novel objects; it can be with regard to the general words people use to express their opinions, etc.&lt;/p&gt;

&lt;h1 id="whytransferlearningnow"&gt;Why Transfer Learning Now?&lt;/h1&gt;

&lt;p&gt;Andrew Ng, chief scientist at Baidu and professor at Stanford, said during &lt;a href="http://sebastianruder.com/highlights-nips-2016/index.html#thenutsandboltsofmachinelearning"&gt;his widely popular NIPS 2016 tutorial&lt;/a&gt; that transfer learning will be -- after supervised learning -- the next driver of ML commercial success.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/andrew_ng_nips_2016_transfer_learning-1.png" style="width: 70%" title="Andrew Ng on transfer learning" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 3: Andrew Ng on transfer learning at NIPS 2016&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;In particular, he sketched out a chart on a whiteboard that I've sought to replicate as faithfully as possible in Figure 4 below (sorry about the &lt;a href="https://xkcd.com/833/"&gt;unlabelled axes&lt;/a&gt;). According to Andrew Ng, transfer learning will become a key driver of Machine Learning success in industry.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/andrew_ng_drivers_ml_success-1.png" style="width: 70%" title="Drivers of ML success in industry" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 4: Drivers of ML industrial success according to Andrew Ng&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;It is indisputable that ML use and success in industry has so far been mostly driven by supervised learning. Fuelled by advances in Deep Learning, more capable computing utilities, and large labeled datasets, supervised learning has been largely responsible for the wave of renewed interest in AI, funding rounds and acquisitions, and in particular the applications of machine learning that we have seen in recent years and that have become part of our daily lives. If we disregard naysayers and heralds of another AI winter and instead trust the prescience of Andrew Ng, this success will likely continue.&lt;/p&gt;

&lt;p&gt;It is less clear, however, why transfer learning which has been around for decades and is currently little utilized in industry, will see the explosive growth predicted by Ng. Even more so as transfer learning currently receives relatively little visibility compared to other areas of machine learning such as unsupervised learning and reinforcement learning, which have come to enjoy increasing popularity: Unsupervised learning -- the &lt;a href="http://sebastianruder.com/highlights-nips-2016/index.html#generalartificialintelligence"&gt;key ingredient on the quest to General AI&lt;/a&gt; according to Yann LeCun as can be seen in Figure 5 -- has seen a resurgence of interest, driven in particular by Generative Adversarial Networks. Reinforcement learning, in turn, spear-headed by Google DeepMind has led to advances in game-playing AI exemplified by the success of &lt;a href="https://deepmind.com/research/alphago/"&gt;AlphaGo&lt;/a&gt; and has already seen success in the real world, e.g. by &lt;a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/"&gt;reducing Google's data center cooling bill by 40%&lt;/a&gt;. Both of these areas, while promising, will likely only have a comparatively small commercial impact in the foreseeable future and mostly remain within the confines of cutting-edge research papers as they still face &lt;a href="http://www.maluuba.com/blog/2017/3/14/the-next-challenges-for-reinforcement-learning"&gt;many challenges&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/lecun_nips_2016_cake_slide.png" style="width: 60%" title="Yann LeCun NIPS 2016 keynote cake slide" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 5: Transfer Learning is conspicuously absent as ingredient from Yann LeCun's cake&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;What makes transfer learning different? In the following, we will look at the factors that -- in our opinion -- motivate Ng's prognosis and outline the reasons why just now is the time to pay attention to transfer learning.&lt;/p&gt;

&lt;p&gt;The current use of machine learning in industry is characterised by a dichotomy: &lt;br&gt;
On the one hand, over the course of the last years, we have obtained the ability to train more and more accurate models. We are now at the stage that for many tasks, state-of-the-art models have reached a level where their performance is so good that it is no longer a hindrance for users. How good? The newest residual networks [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] on ImageNet achieve &lt;a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/"&gt;superhuman performance&lt;/a&gt; at recognising objects; Google's Smart Reply [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;] automatically handles 10% of all mobile responses; speech recognition error has consistently dropped and is more accurate than typing [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;]; we can automatically &lt;a href="http://news.stanford.edu/2017/01/25/artificial-intelligence-used-identify-skin-cancer/"&gt;identify skin cancer as well as dermatologists&lt;/a&gt;; Google's NMT system [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;] is used in production for more than 10 language pairs; Baidu can generate realistic sounding speech &lt;a href="https://www.engadget.com/2017/03/09/baidu-deep-voice-natural-sounding-speec/"&gt;in real-time&lt;/a&gt;; the list goes on and on. This level of maturity has allowed the large-scale deployment of these models to millions of users and has enabled widespread adoption.&lt;/p&gt;

&lt;p&gt;On the other hand, these successful models are immensely data-hungry and rely on huge amounts of labeled data to achieve their performance. For some tasks and domains, this data is available as it has been painstakingly gathered over many years. In a few cases, it is public, e.g. ImageNet [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;], but large amounts of labeled data are usually proprietary or expensive to obtain, as in the case of many speech or MT datasets, as they provide an edge over the competition.&lt;/p&gt;

&lt;p&gt;At the same time, when applying a machine learning model in the wild, it is faced with a myriad of conditions which the model has never seen before and does not know how to deal with; each client and every user has their own preferences, possesses or generates data that is different than the data used for training; a model is asked to perform many tasks that are related to but not the same as the task it was trained for. In all of these situations, our current state-of-the-art models, despite exhibiting human-level or even super-human performance on the task and domain they were trained on, suffer a significant loss in performance or even break down completely. &lt;/p&gt;

&lt;p&gt;Transfer learning can help us deal with these novel scenarios and is necessary for production-scale use of machine learning that goes beyond tasks and domains were labeled data is plentiful. So far, we have applied our models to the tasks and domains that -- while impactful -- are the low-hanging fruits in terms of data availability. To also serve the long tail of the distribution, we must learn to transfer the knowledge we have acquired to new tasks and domains.&lt;/p&gt;

&lt;p&gt;To be able to do this, we need to understand the concepts that transfer learning involves. For this reason, we will give a more technical definition in the following section.&lt;/p&gt;

&lt;h1 id="adefinitionoftransferlearning"&gt;A Definition of Transfer Learning&lt;/h1&gt;

&lt;p&gt;For this definition, we will closely follow the excellent survey by Pan and Yang (2010) [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] with binary document classification as a running example. &lt;br&gt;
Transfer learning involves the concepts of a domain and a task. A domain \(\mathcal{D}\) consists of a feature space \(\mathcal{X}\) and a marginal probability distribution \(P(X)\) over the feature space, where \(X = {x_1, \cdots, x_n} \in \mathcal{X}\). For document classification with a bag-of-words representation, \(\mathcal{X}\) is the space of all document representations, \(x_i\) is the \(i\)-th term vector corresponding to some document and \(X\) is the sample of documents used for training.&lt;/p&gt;

&lt;p&gt;Given a domain, \(\mathcal{D} = \{\mathcal{X},P(X)\}\), a task \(\mathcal{T}\) consists of a label space \(\mathcal{Y}\) and a conditional probability distribution \(P(Y|X)\) that is typically learned from the training data consisting of pairs \(x_i \in X\) and \(y_i \in \mathcal{Y}\). In our document classification example, \(\mathcal{Y}\) is the set of all labels, i.e. &lt;em&gt;True&lt;/em&gt;, &lt;em&gt;False&lt;/em&gt; and \(y_i\) is either &lt;em&gt;True&lt;/em&gt; or &lt;em&gt;False&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Given a source domain \(\mathcal{D}_S\), a corresponding source task \(\mathcal{T}_S\), as well as a target domain \(\mathcal{D}_T\) and a target task \(\mathcal{T}_T\), the objective of transfer learning now is to enable us to learn the target conditional probability distribution \(P(Y_T|X_T)\) in \(\mathcal{D}_T\) with the information gained from \(\mathcal{D}_S\) and \(\mathcal{T}_S\) where \(\mathcal{D}_S \neq \mathcal{D}_T\) or \(\mathcal{T}_S \neq \mathcal{T}_T\). In most cases, a limited number of labeled target examples, which is exponentially smaller than the number of labeled source examples are assumed to be available.&lt;/p&gt;

&lt;p&gt;As both the domain \(\mathcal{D}\) and the task \(\mathcal{T}\) are defined as tuples, these inequalities give rise to four transfer learning scenarios, which we will discus below.&lt;/p&gt;

&lt;h1 id="transferlearningscenarios"&gt;Transfer Learning Scenarios&lt;/h1&gt;

&lt;p&gt;Given source and target domains \(\mathcal{D}_S\) and \(\mathcal{D}_T\) where \(\mathcal{D} = \{\mathcal{X},P(X)\}\) and source and target tasks \(\mathcal{T}_S\) and \(\mathcal{T}_T\) where \(\mathcal{T} = \{\mathcal{Y}, P(Y|X)\}\) source and target conditions can vary in four ways, which we will illustrate in the following again using our document classification example:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(\mathcal{X}_S \neq \mathcal{X}_T\). The feature spaces of the source and target domain are different, e.g. the documents are written in two different languages. In the context of natural language processing, this is generally referred to as cross-lingual adaptation.  &lt;/li&gt;
&lt;li&gt;\(P(X_S) \neq P(X_T)\). The marginal probability distributions of source and target domain are different, e.g. the documents discuss different topics. This scenario is generally known as domain adaptation.  &lt;/li&gt;
&lt;li&gt;\(\mathcal{Y}_S \neq \mathcal{Y}_T\). The label spaces between the two tasks are different, e.g. documents need to be assigned different labels in the target task. In practice, this scenario usually occurs with scenario 4, as it is extremely rare for two different tasks to have different label spaces, but exactly the same conditional probability distributions.  &lt;/li&gt;
&lt;li&gt;\(P(Y_S|X_S) \neq P(Y_T|X_T)\). The conditional probability distributions of the source and target tasks are different, e.g. source and target documents are unbalanced with regard to their classes. This scenario is quite common in practice and approaches such as over-sampling, under-sampling, or SMOTE [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;] are widely used.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After we are now aware of the concepts relevant for transfer learning and the scenarios in which it is applied, we will look to different applications of transfer learning that illustrate some of its potential.&lt;/p&gt;

&lt;h1 id="applicationsoftransferlearning"&gt;Applications of Transfer Learning&lt;/h1&gt;

&lt;h2 id="learningfromsimulations"&gt;Learning from simulations&lt;/h2&gt;

&lt;p&gt;One particular application of transfer learning that I'm very excited about and that I assume we'll see more of in the future is learning from simulations. For many machine learning applications that rely on hardware for interaction, gathering data and training a model in the real world is either expensive, time-consuming, or simply too dangerous. It is thus advisable to gather data in some other, less risky way.&lt;/p&gt;

&lt;p&gt;Simulation is the preferred tool for this and is used towards enabling many advanced ML systems in the real world. Learning from a simulation and applying the acquired knowledge to the real world is an instance of transfer learning scenario 2, as the feature spaces between source and target domain are the same (both generally rely on pixels), but the marginal probability distributions between simulation and reality are different, i.e. objects in the simulation and the source &lt;em&gt;look&lt;/em&gt; different, although this difference diminishes as simulations get more realistic. At the same time, the conditional probability distributions between simulation and real wold might be different as the simulation is not able to fully replicate all reactions in the real world, e.g. a physics engine can not completely mimic the complex interactions of real-world objects.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/self_driving_car_city.jpg" style="width: 60%" title="Google self-driving car" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 6: A Google self-driving car (source: &lt;a href="https://googleblog.blogspot.ie/2014/04/the-latest-chapter-for-self-driving-car.html"&gt;Google Research blog&lt;/a&gt;)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Learning from simulations has the benefit of making data gathering easy as objects can be easily bounded and analyzed, while simultaneously enabling fast training, as learning can be parallelized across multiple instances. Consequently, it is a prerequisite for large-scale machine learning projects that need to interact with the real world, such as self-driving cars (Figure 6). According to Zhaoyin Jia, Google's self-driving car tech lead, "Simulation is essential if you really want to do a self-driving car". Udacity has &lt;a href="https://techcrunch.com/2017/02/08/udacity-open-sources-its-self-driving-car-simulator-for-anyone-to-use/"&gt;open-sourced the simulator&lt;/a&gt; it uses for teaching its self-driving car engineer nanodegree, which can be seen in Figure 7 and &lt;a href="https://universe.openai.com/"&gt;OpenAI's Universe&lt;/a&gt; will potentially allows to train a self-driving car &lt;a href="https://techcrunch.com/2017/01/11/training-self-driving-cars-on-the-streets-of-los-santos-with-gta-v-just-got-easier/"&gt;using GTA 5&lt;/a&gt; or other video games.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/udacity_self_driving_car_simulator.png" style="width: 60%" title="Udacity self-driving car simulator" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 7: Udacity's self-driving car simulator (source: &lt;a href="https://techcrunch.com/2017/02/08/udacity-open-sources-its-self-driving-car-simulator-for-anyone-to-use/"&gt;TechCrunch&lt;/a&gt;)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Another area where learning from simulations is key is robotics: Training models on a real robot is too slow and robots are expensive to train. Learning from a simulation and transferring the knowledge to real-world robot alleviates this problem and has recently been garnering additional interest [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;]. An example of a data manipulation task in the real world and in a simulation can be seen in Figure 8.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/robot_and_simulation_images_hadsell.png" style="width: 100%" title="Robot and simulation images" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 8: Robot and simulation images (Rusu et al., 2016)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Finally, another direction where simulation will be an integral part is on the path towards general AI. Training an agent to achieve general artificial intelligence directly in the real world is too costly and hinders learning initially through unnecessary complexity. Rather, learning may be more successful if it is based on a simulated environment such as &lt;a href="https://github.com/facebookresearch/CommAI-env"&gt;CommAI-env&lt;/a&gt; [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;] that is visible in Figure 9.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/commai-env_environment.png" style="width: 30%" title="CommAI-env" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 9: Facebook AI Research's CommAI-env (Mikolov et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="adaptingtonewdomains"&gt;Adapting to new domains&lt;/h2&gt;

&lt;p&gt;While learning from simulations is a particular instance of domain adaptation, it is worth outlining some other examples of domain adaptation.&lt;/p&gt;

&lt;p&gt;Domain adaptation is a common requirement in vision as often the data where labeled information is easily accessible and the data that we actually care about are different, whether this pertains to identifying bikes as in Figure 10 or some other objects in the wild. Even if the training and the the test data &lt;em&gt;look&lt;/em&gt; the same, the training data may still contain a bias that is imperceptible to humans but which the model will exploit to overfit on the training data [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/visual_domain_adaptation_bikes.png" style="width: 60%" title="Visual domains" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 10: Different visual domains (Sun et al., 2016)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Another common domain adaptation scenario pertains to adapting to different text types: Standard NLP tools such as part-of-speech taggers or parsers are typically trained on news data such as the Wall Street Journal, which has historically been used to evaluate these models. Models trained on news data, however, have difficulty coping with more novel text forms such as social media messages and the challenges they present.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/domain_adaptation_text_types-2.png" style="width: 60%" title="Different text types" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 11: Different text types / genres&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Even within one domain such as product reviews, people employ different words and phrases to express the same opinion. A model trained on one type of review should thus be able to disentangle the general and domain-specific opinion words that people use in order not to be confused by the shift in domain. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/domain_adaptation_reviews.png" style="width: 60%" title="" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 12: Different topics&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Finally, while the above challenges deal with general text or image types, problems are amplified if we look at domains that pertain to individual or groups of users: Consider the case of automatic speech recognition (ASR). Speech is poised to become &lt;a href="https://backchannel.com/voice-is-the-next-big-platform-unless-you-have-an-accent-6a787f7e8500#.koqx9pc2h"&gt;the next big platform&lt;/a&gt;, with 50% of all our searches predicted to be performed by voice by 2020. Most ASR systems are evaluated traditionally on the Switchboard dataset, which comprises 500 speakers. Most people with a standard accent are thus fortunate, while immigrants, people with non-standard accents, people with a speech impediment, or children have trouble being understood. Now more than ever do we need systems that are able to adapt to individual users and minorities to ensure that everyone's voice is heard.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/domain_adaptation_accents.png" style="width: 60%" title="Different accents" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 13: Different accents&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="transferringknowledgeacrosslanguages"&gt;Transferring knowledge across languages&lt;/h2&gt;

&lt;p&gt;Finally, learning from one language and applying our knowledge to another language is -- in my opinion -- another killer application of transfer learning, which I have written about before &lt;a href="http://sebastianruder.com/cross-lingual-embeddings/index.html"&gt;here&lt;/a&gt; in the context of cross-lingual embedding models. Reliable cross-lingual adaptation methods would allow us to leverage the vast amounts of labeled data we have in English and apply them to any language, particularly underserved and truly low-resource languages. Given the &lt;a href="http://sebastianruder.com/cross-lingual-embeddings/index.html#evaluation"&gt;current state-of-the-art&lt;/a&gt;, this still seems utopian, but recent advances such as zero-shot translation [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] promise rapid progress in this area.&lt;/p&gt;

&lt;p&gt;While we have so far considered particular applications of transfer learning, we will now look at practical methods and directions in the literature that are used to solve some of the presented challenges.&lt;/p&gt;

&lt;h1 id="transferlearningmethods"&gt;Transfer Learning Methods&lt;/h1&gt;

&lt;p&gt;Transfer learning has a long history of research and techniques exist to tackle each of the four transfer learning scenarios described above. The advent of Deep Learning has led to a range of new transfer learning approaches, some of which we will review in the following. For a survey of earlier methods, refer to [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="usingpretrainedcnnfeatures"&gt;Using pre-trained CNN features&lt;/h2&gt;

&lt;p&gt;In order to motivate the most common way of transfer learning is currently applied, we must understand what accounts for the outstanding success of large convolutional neural networks on ImageNet [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;].  &lt;/p&gt;

&lt;h3 id="understandingconvolutionalneuralnetworks"&gt;Understanding convolutional neural networks&lt;/h3&gt;

&lt;p&gt;While many details of how these models work still remain a mystery, we are by now aware that lower convolutional layers capture low-level image features, e.g. edges (see Figure 14), while higher convolutional layers capture more and more complex details, such as body parts, faces, and other compositional features.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/example_filters.jpeg" style="width: 70%" title="Andrew Ng on transfer learning" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 14: Example filters learned by AlexNet (Krizhevsky et al., 2012). &lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The final fully-connected layers are generally assumed to capture information that is relevant for solving the respective task, e.g. AlexNet's fully-connected layers would indicate which features are relevant to classify an image into one of 1000 object categories.&lt;/p&gt;

&lt;p&gt;However, while knowing that a cat has whiskers, paws, fur, etc. is necessary for identifying an animal as a cat (for an example, see Figure 15), it does not help us with identifying new objects or to solve other common vision tasks such as scene recognition, fine grained recognition, attribute detection and image retrieval.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/cat.jpg" style="width: 70%" title="Token cat" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 15: This post's token cat&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;What can help us, however, are representations that capture general information of how an image is composed and what combinations of edges and shapes it contains. This information is contained in one of the final convolutional layers or early fully-connected layers in large convolutional neural networks trained on ImageNet as we have described above.&lt;/p&gt;

&lt;p&gt;For a new task, we can thus simply use the off-the-shelf features of a state-of-the-art CNN pre-trained on ImageNet and train a new model on these extracted features. In practice, we either keep the pre-trained parameters fixed or tune them with a small learning rate in order to ensure that we do not unlearn the previously acquired knowledge. This simple approach has been shown to achieve astounding results on an array of vision tasks [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] as well as tasks that rely on visual input such as image captioning. A model trained on ImageNet seems to capture details about the way animals and objects are structured and composed that is generally relevant when dealing with images. As such, the ImageNet task seems to be a good proxy for general computer vision problems, as the same knowledge that is required to excel in it is also relevant for many other tasks.&lt;/p&gt;

&lt;h3 id="learningtheunderlyingstructureofimages"&gt;Learning the underlying structure of images&lt;/h3&gt;

&lt;p&gt;A similar assumption is used to motivate generative models: When training generative models, we assume that the ability to generate realistic images requires an understanding of the underlying structure of images, which in turn can be applied to many other tasks. This assumption itself relies on the premise that all images lie on a low-dimensional manifold, i.e. that there is some underlying structure to images that can be extracted by a model. Recent advances in generating photorealistic images with Generative Adversarial Networks [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] indicate that such a structure might indeed exist, as evidenced by the model's ability to show realistic transitions between points in the bedroom image space in Figure 16.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/walking_the_image_manifold.png" style="width: 80%" title="Walking the image manifold" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 16: Walking along the bedroom image manifold&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h3 id="arepretrainedfeaturesusefulbeyondvision"&gt;Are pre-trained features useful beyond vision?&lt;/h3&gt;

&lt;p&gt;Off-the-shelf CNN features have seen unparalleled results in vision, but the question  remains if this success can be replicated in other disciplines using other types of data, such as languages. Currently, there are no off-the-shelf features that achieve results for natural language processing that are as astounding as their vision equivalent. Why is that? Do such features exist at all or -- if not -- why is vision more conducive to this form of transfer than language?&lt;/p&gt;

&lt;p&gt;The output of lower-level tasks such as part-of-speech tagging or chunking can be likened as off-the-shelf features, but these do not capture more fine-grained rules of language use beyond syntax and are not helpful for all tasks. As we have seen, the existence of generalizable off-the-shelf features seems to be intertwined with the existence of a task that can be seen as a prototype for many tasks in the field. In vision, object recognition occupies such a role. In language, the closest analogue might be language modelling: In order to predict the next word or sentence given a sequence of words, a model needs to possess knowledge of how language is structured, needs to understand what words likely are related to and likely follow each other, needs to model long-term dependencies, etc.&lt;/p&gt;

&lt;p&gt;While state-of-the-art language models increasingly approach human levels [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;], their features are only of limited use. At the same time, advances in language modelling have led to positive results for other tasks: Pre-training a model with a language model objective improves performance [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;]. In addition, word embeddings pre-trained on a large unlabelled corpus with an approximated language modelling objective have become pervasive [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;]. While they are not as effective as off-the-shelf features in vision, they still provide sizeable gains [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] and can be seen a simple form of transfer of general domain knowledge derived from a large unlabelled corpus.&lt;/p&gt;

&lt;p&gt;While a general proxy task seems currently out of reach in natural language processing, auxiliary tasks can take the form of local proxies. Whether through multi-task objectives [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] or synthetic task objectives [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;], they can be used to inject additional relevant knowledge into the model.&lt;/p&gt;

&lt;p&gt;Using pre-trained features is currently the most straightforward and most commonly used way to perform transfer learning. However, it is by far not the only one.&lt;/p&gt;

&lt;h2 id="learningdomaininvariantrepresentations"&gt; Learning domain-invariant representations&lt;/h2&gt;

&lt;p&gt;Pre-trained features are in practice mostly used for adaptation scenario 3 where we want to adapt to a new task. For the other scenarios, another way to transfer knowledge enabled by Deep Learning is to learn representations that do not change based on our domain. This approach is conceptually very similar to the way we have been thinking about using pre-trained CNN features: Both encode general knowledge about our domain. However, creating representations that do not change based on the domain is a lot less expensive and more feasible for non-vision tasks than generating representations that are useful for all tasks. ImageNet has taken years and thousands of hours to create, while we typically only need unlabelled data of each domain for creating domain-invariant representations. These representations are generally learned using stacked denoising autoencoders and have seen success in natural language processing [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;] as well as in vision [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="makingrepresentationsmoresimilar"&gt;Making representations more similar&lt;/h2&gt;

&lt;p&gt;In order to improve the transferability of the learned representations from the source to the target domain, we would like the representations between the two domains to be as similar as possible so that the model does not take into account domain-specific characteristics that may hinder transfer but the commonalities between the domains.&lt;/p&gt;

&lt;p&gt;Rather than just letting our autoencoder learn some representation, we can thus actively encourage the representations of both domains to be more similar to each other. We can apply this as a pre-processing step directly to the representations of our data [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;] and can then use the new representations for training. We can also encourage the representations of the domains in our model to be more similar to each other [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h2 id="confusingdomains"&gt;Confusing domains&lt;/h2&gt;

&lt;p&gt;Another way to ensure similarity between the representations of both domains that has recently become more popular is to add another objective to an existing model that encourages it to confuse the two domains [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;]. This domain confusion loss is a regular classification loss where the model tries to predict the domain of the input example. The difference to a regular loss, however, is that gradients that flow from the loss to the rest of the network are reversed, as can be seen in Figure 17. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/confusing_domains_with_gradient_reversal_layer.png" style="width: 80%" title="Confusing domains with gradient reversal layer" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 17: Confusing domains with a gradient reversal layer (Ganin and Lempitsky, 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Instead of learning to minimize the error of the domain classification loss, the gradient reversal layer causes the model to maximize the error. In practice, this means that the model learns representations that allow it to minimize its original objective, while not allowing it to differentiate between the two domains, which is beneficial for knowledge transfer. While a model trained only with the regular objective is shown in Figure 18 to be clearly able to separate domains based on its learned representation, a model whose objective has been augmented with the domain confusion term is unable to do so.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2017/03/domain_confusion_tzeng.png" style="width: 80%" title="Domain confusion" alt="Transfer Learning - Machine Learning's Next Frontier"&gt;
&lt;figcaption&gt;Figure 18: Domain classifier score of a regular and a domain confusion model (Tzeng et al, 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h1 id="relatedresearchareas"&gt;Related Research Areas&lt;/h1&gt;

&lt;p&gt;While this post is about transfer learning, transfer learning is by far not the only area of machine learning that seeks to leverage limited amounts of data, use learned knowledge for new endeavours, and enable models to generalize better to new settings. In the following, we will thus introduce other directions that are related or complementary to the goals of transfer learning.&lt;/p&gt;

&lt;h2 id="semisupervisedlearning"&gt;Semi-supervised learning&lt;/h2&gt;

&lt;p&gt;Transfer learning seeks to leverage unlabelled data in the target task or domain to the most effect. This is also the maxim of semi-supervised learning, which follows the classical machine learning setup but assumes only a limited amount of labeled samples for training. Insofar, semi-supervised domain adaptation is essentially semi-supervised learning under domain shift. Many lessons and insights from semi-supervised learning are thus equally applicable and relevant for transfer learning. Refer to [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] for a great survey on semi-supervised learning.&lt;/p&gt;

&lt;h2 id="usingavailabledatamoreeffectively"&gt; Using available data more effectively&lt;/h2&gt;

&lt;p&gt;Another direction that is related to transfer learning and semi-supervised learning is to enable models to work better with limited amounts of data.&lt;/p&gt;

&lt;p&gt;This can be done in several ways: One can leverage unsupervised or semi-supervised learning to extract information from unlabelled data thereby reducing the reliance on labelled samples; one can give the model access to other features inherent in the data while reducing its tendency to overfit via regularization; finally, one can leverage data that so far remains neglected or rests in non-obvious places.&lt;/p&gt;

&lt;p&gt;Such fortuitous data [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] may be created as a side effect of user-generated content, such as hyperlinks that can be used to improve named entity and part-of-speech taggers; it may come as a by-product of annotation, e.g. annotator disagreement that may improve tagging or parsing; or it may be derived from user behaviour such as eye tracking or keystroke dynamics, which can inform NLP tasks. While such data has only been exploited in limited ways, such examples encourage us to look for data in unexpected places and to investigate new ways of retrieving data.&lt;/p&gt;

&lt;h2 id="improvingmodelsabilitytogeneralize"&gt;Improving models' ability to generalize&lt;/h2&gt;

&lt;p&gt;Related to this is also the direction of making models generalize better. In order to achieve this, we must first better understand the behaviour and intricacies of large neural networks and investigate why and how they generalize. Recent work has taken promising steps towards this end [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;], but many questions are still left unanswered.&lt;/p&gt;

&lt;h2 id="makingmodelsmorerobust"&gt;Making models more robust&lt;/h2&gt;

&lt;p&gt;While improving our models' generalization ability goes a long way, we might generalize well to similar instances but still fail catastrophically on unexpected or atypical inputs. Therefore, a key complementary objective is to make our models more robust. This direction has seen increasing interest recently fuelled by advances in adversarial learning and recent approaches have investigated many ways of how models can be made more robust to worst-case or adversarial examples in different settings [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="multitasklearning"&gt;Multi-task learning&lt;/h2&gt;

&lt;p&gt;In transfer learning, we mainly care about doing well on our target task or domain. In multi-task learning, in contrast, the objective is to do well on all available tasks. Alternatively, we can also use the knowledge acquired by learning from related tasks to do well on a target. Crucially, in contrast to transfer learning, some labeled data is usually assumed for each task. In addition, models are trained jointly on source and target task data, which is not the case for all transfer learning scenarios. However, even if target data is not available during training, insights about tasks that are beneficial for multi-task learning [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] can still inform transfer learning decisions.&lt;/p&gt;

&lt;p&gt;For a more thorough overview of multi-task learning, particularly as applied to deep neural networks, have a look at my other blog post &lt;a href="http://sebastianruder.com/multi-task/index.html"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="continuouslearning"&gt;Continuous learning&lt;/h2&gt;

&lt;p&gt;While multi-task learning allows us to retain the knowledge across many tasks without suffering a performance penalty on our source tasks, this is only possible if all tasks are present at training time. For each new task, we would generally need to retrain our model on all tasks again.&lt;/p&gt;

&lt;p&gt;In the real world, however, we would like an agent to be able to deal with tasks that gradually become more complex by leveraging its past experience. To this end, we need to enable a model to learn continuously without forgetting. This area of machine learning is known as learning to learn [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;], meta-learning, life-long learning, or continuous learning. It has seen some recent developments in the context of RL [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;] most notably by &lt;a href="https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/"&gt;Google DeepMind on their quest towards general learning agents&lt;/a&gt; and is also being applied to sequence-to-sequence models [&lt;sup id="fnref:40"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:40" rel="footnote"&gt;40&lt;/a&gt;&lt;/sup&gt;]. &lt;/p&gt;

&lt;h2 id="zeroshotlearning"&gt;Zero-shot learning&lt;/h2&gt;

&lt;p&gt;Finally, if we take transfer learning to the extreme and aim to learn from only a few, one or even zero instances of a class, we arrive at few-shot, one-shot, and zero-shot learning respectively. Enabling models to perform one-shot and zero-shot learning is admittedly among the hardest problems in machine learning. At the same time, it is something that comes naturally to us humans: Toddlers only need to be told once what a dog is in order to be able to identify any other dog, while adults can understand the essence of an object just by reading about it in context, without ever having encountered it before.&lt;/p&gt;

&lt;p&gt;Recent advances in one-shot learning have leveraged the insight that models need to be trained explicitly to perform one-shot learning in order to achieve good performance at test time [&lt;sup id="fnref:41"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:41" rel="footnote"&gt;41&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:42"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:42" rel="footnote"&gt;42&lt;/a&gt;&lt;/sup&gt;], while the more realistic generalized zero-shot learning setting where training classes are present at test time has garnered attention lately [&lt;sup id="fnref:43"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:43" rel="footnote"&gt;43&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In summary, there are many exciting research directions that transfer learning offers and -- in particular -- many applications that are in need of models that can transfer knowledge to new tasks and adapt to new domains. I hope that I was able to provide you with an overview of transfer learning in this blog post and was able to pique your interest.&lt;/p&gt;

&lt;p&gt;Some of the statements in this blog post are deliberately phrased &lt;em&gt;slightly&lt;/em&gt; controversial. Let me know your thoughts about any contentious issues and any errors that I undoubtedly made in writing this post in the comments below.&lt;/p&gt;

&lt;p&gt;Note: Title image is credited to [&lt;sup id="fnref:44"&gt;&lt;a href="http://ruder.io/transfer-learning/#fn:44" rel="footnote"&gt;44&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Szegedy, C., Ioffe, S., Vanhoucke, V., &amp;amp; Alemi, A. (2016). Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. arXiv preprint arXiv:1602.07261. &lt;a href="http://ruder.io/transfer-learning/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Kannan, A., Kurach, K., Ravi, S., Kaufmann, T., Tomkins, A., Miklos, B., … Ramavajjala, V. (2016). Smart Reply: Automated Response Suggestion for Email. In KDD 2016. &lt;a href="http://doi.org/10.475/123"&gt;http://doi.org/10.475/123&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Ruan, S., Wobbrock, J. O., Liou, K., Ng, A., &amp;amp; Landay, J. (2016). Speech Is 3x Faster than Typing for English and Mandarin Text Entry on Mobile Devices. arXiv preprint arXiv:1608.07323. &lt;a href="http://ruder.io/transfer-learning/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint arXiv:1609.08144. &lt;a href="http://ruder.io/transfer-learning/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Deng, J., Dong, W., Socher, R., Li, L., Li, K., &amp;amp; Fei-fei, L. (2009). ImageNet : A Large-Scale Hierarchical Image Database. In IEEE Conference on Computer Vision and Pattern Recognition. &lt;a href="http://ruder.io/transfer-learning/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Pan, S. J., &amp;amp; Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–1359. &lt;a href="http://ruder.io/transfer-learning/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Chawla, N. V, Bowyer, K. W., Hall, L. O., &amp;amp; Kegelmeyer, W. P. (2002). SMOTE : Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321–357. &lt;a href="http://ruder.io/transfer-learning/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Rusu, A. A., Vecerik, M., Rothörl, T., Heess, N., Pascanu, R., &amp;amp; Hadsell, R. (2016). Sim-to-Real Robot Learning from Pixels with Progressive Nets. arXiv Preprint arXiv:1610.04286. Retrieved from &lt;a href="http://arxiv.org/abs/1610.04286"&gt;http://arxiv.org/abs/1610.04286&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Mikolov, T., Joulin, A., &amp;amp; Baroni, M. (2015). A Roadmap towards Machine Intelligence. arXiv Preprint arXiv:1511.08130. Retrieved from &lt;a href="http://arxiv.org/abs/1511.08130"&gt;http://arxiv.org/abs/1511.08130&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Torralba, A., &amp;amp; Efros, A. A. (2011). Unbiased Look at Dataset Bias. In 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). &lt;a href="http://ruder.io/transfer-learning/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href="http://ruder.io/transfer-learning/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems, 1–9. &lt;a href="http://ruder.io/transfer-learning/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Razavian, A. S., Azizpour, H., Sullivan, J., &amp;amp; Carlsson, S. (2014). CNN features off-the-shelf: An astounding baseline for recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 512–519. &lt;a href="http://ruder.io/transfer-learning/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Radford, A., Metz, L., &amp;amp; Chintala, S. (2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ICLR. Retrieved from &lt;a href="http://arxiv.org/abs/1511.06434"&gt;http://arxiv.org/abs/1511.06434&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp;amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv Preprint arXiv:1602.02410. Retrieved from &lt;a href="http://arxiv.org/abs/1602.02410"&gt;http://arxiv.org/abs/1602.02410&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Ramachandran, P., Liu, P. J., &amp;amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. &lt;a href="http://ruder.io/transfer-learning/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. &lt;a href="http://ruder.io/transfer-learning/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from &lt;a href="http://arxiv.org/abs/1408.5882"&gt;http://arxiv.org/abs/1408.5882&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Bingel, J., &amp;amp; Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL. Retrieved from &lt;a href="http://arxiv.org/abs/1702.08303"&gt;http://arxiv.org/abs/1702.08303&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Plank, B., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. &lt;a href="http://ruder.io/transfer-learning/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Yu, J., &amp;amp; Jiang, J. (2016). Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 236–246. Retrieved from &lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1023.pdf"&gt;http://www.aclweb.org/anthology/D/D16/D16-1023.pdf&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Glorot, X., Bordes, A., &amp;amp; Bengio, Y. (2011). Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach. Proceedings of the 28th International Conference on Machine Learning, 513–520. Retrieved from &lt;a href="http://www.icml-2011.org/papers/342_icmlpaper.pdf"&gt;http://www.icml-2011.org/papers/342_icmlpaper.pdf&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Chen, M., Xu, Z., Weinberger, K. Q., &amp;amp; Sha, F. (2012). Marginalized Denoising Autoencoders for Domain Adaptation. Proceedings of the 29th International Conference on Machine Learning (ICML-12), 767--774. &lt;a href="http://doi.org/10.1007/s11222-007-9033-z"&gt;http://doi.org/10.1007/s11222-007-9033-z&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Zhuang, F., Cheng, X., Luo, P., Pan, S. J., &amp;amp; He, Q. (2015). Supervised Representation Learning: Transfer Learning with Deep Autoencoders. IJCAI International Joint Conference on Artificial Intelligence, 4119–4125. &lt;a href="http://ruder.io/transfer-learning/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Daumé III, H. (2007). Frustratingly Easy Domain Adaptation. Association for Computational Linguistic (ACL), (June), 256–263. &lt;a href="http://doi.org/10.1.1.110.2062"&gt;http://doi.org/10.1.1.110.2062&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Sun, B., Feng, J., &amp;amp; Saenko, K. (2016). Return of Frustratingly Easy Domain Adaptation. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16). Retrieved from &lt;a href="http://arxiv.org/abs/1511.05547"&gt;http://arxiv.org/abs/1511.05547&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., &amp;amp; Erhan, D. (2016). Domain Separation Networks. NIPS. &lt;a href="http://ruder.io/transfer-learning/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., &amp;amp; Darrell, T. (2014). Deep Domain Confusion: Maximizing for Domain Invariance. CoRR. Retrieved from &lt;a href="https://arxiv.org/pdf/1412.3474.pdf"&gt;https://arxiv.org/pdf/1412.3474.pdf&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Ganin, Y., &amp;amp; Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). &lt;a href="http://ruder.io/transfer-learning/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17, 1–35. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf"&gt;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Zhu, X. (2005). Semi-Supervised Learning Literature Survey. &lt;a href="http://ruder.io/transfer-learning/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Plank, B. (2016). What to do about non-standard (or non-canonical) language in NLP. KONVENS 2016. Retrieved from &lt;a href="https://arxiv.org/pdf/1608.07836.pdf"&gt;https://arxiv.org/pdf/1608.07836.pdf&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp;amp; Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. ICLR 2017. &lt;a href="http://ruder.io/transfer-learning/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Kurakin, A., Goodfellow, I., &amp;amp; Bengio, S. (2017). Adversarial examples in the physical world. In ICLR 2017. Retrieved from &lt;a href="http://arxiv.org/abs/1607.02533"&gt;http://arxiv.org/abs/1607.02533&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Huang, S., Papernot, N., Goodfellow, I., Duan, Y., &amp;amp; Abbeel, P. (2017). Adversarial Attacks on Neural Network Policies. In Workshop Track - ICLR 2017. &lt;a href="http://ruder.io/transfer-learning/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Thrun, S., &amp;amp; Pratt, L. (1998). Learning to learn. Springer Science &amp;amp; Business Media. &lt;a href="http://ruder.io/transfer-learning/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., … Hadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. PNAS. &lt;a href="http://ruder.io/transfer-learning/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., ... Deepmind, G. (2016). Progressive Neural Networks. arXiv preprint arXiv:1606.04671. &lt;a href="http://ruder.io/transfer-learning/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A. A., ... Wierstra, D. (2017). PathNet: Evolution Channels Gradient Descent in Super Neural Networks. In arXiv preprint arXiv:1701.08734. &lt;a href="http://ruder.io/transfer-learning/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:40"&gt;&lt;p&gt;Kaiser, Ł., Nachum, O., Roy, A., &amp;amp; Bengio, S. (2017). Learning to Remember Rare Events. In ICLR 2017. &lt;a href="http://ruder.io/transfer-learning/#fnref:40" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:41"&gt;&lt;p&gt;Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp;amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. NIPS 2016. Retrieved from &lt;a href="http://arxiv.org/abs/1606.04080"&gt;http://arxiv.org/abs/1606.04080&lt;/a&gt; &lt;a href="http://ruder.io/transfer-learning/#fnref:41" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:42"&gt;&lt;p&gt;Ravi, S., &amp;amp; Larochelle, H. (2017). Optimization as a Model for Few-Shot Learning. In ICLR 2017. &lt;a href="http://ruder.io/transfer-learning/#fnref:42" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:43"&gt;&lt;p&gt;Xian, Y., Schiele, B., Akata, Z., Campus, S. I., &amp;amp; Machine, A. (2017). Zero-Shot Learning - The Good, the Bad and the Ugly. In CVPR 2017. &lt;a href="http://ruder.io/transfer-learning/#fnref:43" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:44"&gt;&lt;p&gt;Tzeng, E., Hoffman, J., Saenko, K., &amp;amp; Darrell, T. (2017). Adversarial Discriminative Domain Adaptation. &lt;a href="http://ruder.io/transfer-learning/#fnref:44" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title>Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more</title><description>&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/highlights-nips-2016/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I attended NIPS 2016 in Barcelona from Monday, December 5 to Saturday, December 10. The full conference program is available &lt;a href="https://media.nips.cc/Conferences/2016/NIPS-2016-Conference-Book.pdf"&gt;here&lt;/a&gt;. In the following, I will share some of my highlights.&lt;/p&gt;

&lt;h1 id="nips"&gt;NIPS&lt;/h1&gt;

&lt;p&gt;The Conference on Neural Information Processing Systems (NIPS) is (besides&lt;/p&gt;</description><link>http://ruder.io/highlights-nips-2016/</link><guid isPermaLink="false">dca20ced-1519-4c56-bc75-5f8202fe7e16</guid><category>deep learning</category><category>natural language processing</category><category>nlp</category><category>nips</category><category>reinforcement learning</category><category>meta-learning</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Wed, 21 Dec 2016 19:29:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2016/12/nips_entry-2.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2016/12/nips_entry-2.jpg" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;&lt;p&gt;&lt;em&gt;This post originally appeared at the &lt;a href="http://blog.aylien.com/highlights-nips-2016/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I attended NIPS 2016 in Barcelona from Monday, December 5 to Saturday, December 10. The full conference program is available &lt;a href="https://media.nips.cc/Conferences/2016/NIPS-2016-Conference-Book.pdf"&gt;here&lt;/a&gt;. In the following, I will share some of my highlights.&lt;/p&gt;

&lt;h1 id="nips"&gt;NIPS&lt;/h1&gt;

&lt;p&gt;The Conference on Neural Information Processing Systems (NIPS) is (besides ICML) one of the two top conferences in machine learning. It took place for the first time in 1987 and is held every December, historically in close proximity to a ski resort. This year, in slight juxtaposition, it took place in sunny Barcelona.&lt;/p&gt;

&lt;p&gt;Machine Learning seems to become more pervasive every month. However, it is still sometimes hard to keep track of the actual extent of this development. One of the most accurate barometers for this evolution is the growth of NIPS itself. The number of attendees skyrocketed at this year’s conference growing by over 50% year-over-year.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/terry_law.jpg" style="width: 60%" title="Terry's Law" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 1: The growth of the number of attendees at NIPS follows (the newly coined) Terry’s Law (named after Terrence Sejnowski, the president of the NIPS foundation; faster growth than Moore's Law Law)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Unsurprisingly, Deep Learning (DL) was by far the most popular research topic, with about every fourth of more than 2,500 submitted papers (and 568 accepted papers) dealing with deep neural networks.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/submissions_distribution.png" style="width: 100%" title="Distribution of topics across NIPS 2016 submissions" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 2: Distribution of topics across all submitted papers (Source: &lt;a href="http://www.tml.cs.uni-tuebingen.de/team/luxburg/misc/nips2016/index.php"&gt;The review process for NIPS 2016&lt;/a&gt;)  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;On the other hand, the distribution of research paper topics has quite a long tail and reflects the diversity of topics at the conference that span everything from theory to applications, from robotics to neuroscience, and from healthcare to self-driving cars.&lt;/p&gt;

&lt;h1 id="generativeadversarialnetworks"&gt;Generative Adversarial Networks&lt;/h1&gt;

&lt;p&gt;One of the hottest developments within Deep Learning was Generative Adversarial Networks (GANs). The minimax game playing networks have by now won the favor of many luminaries in the field. Yann LeCun hails them as the most exciting development in ML in recent years. The organizers and attendees of NIPS seem to side with him: NIPS featured a tutorial by Ian Goodfellow about his brainchild, which led to a packed main conference hall.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/full_conference_hall_gan_tutorial.jpg" style="width: 70%" title="Full conference hall at GAN tutorial" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 3: A full conference hall at the GAN tutorial  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Though a fairly recent development, there are many cool extensions of GANs among the conference papers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6111-learning-what-and-where-to-draw.pdf"&gt;Reed et al.&lt;/a&gt; propose a model that allows you to specify not only what you want to draw (e.g. a bird) but also where to put it in an image.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf"&gt;Chen et al.&lt;/a&gt; disentangle factors of variation in GANs by representing them with latent codes. The resulting models allow you to adjust e.g. the type of a digit, its breadth and width, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In spite of their popularity, we know alarmingly little about what makes GANs so capable of generating realistic-looking images. In addition, making them work in practice is an arduous endeavour and a lot of (undocumented) hacks are necessary to achieve the best performance. Soumith Chintala presents a collection of these hacks in his &lt;a href="https://github.com/soumith/ganhacks"&gt;"How to train your GAN" talk&lt;/a&gt; at the Adversarial Training workshop.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/soumith_chintala_nips_2016_how_to_train_your_gan_poster.png" style="width: 50%" title="How to train your GAN" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 4: How to train your GAN (Source: &lt;a href="https://twitter.com/soumithchintala/status/805111562503589889"&gt;Soumith Chintala&lt;/a&gt;)  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Yann LeCun muses in his keynote that the development of GANs parallels the history of neural networks themselves: They were poorly understood and hard to get to work in the beginning and only took off once researchers figured out the right tricks and learned how to make them work. At this point, it seems unlikely that GANs will experience a winter anytime soon; the research community is still at the beginning in learning how to make the best use of them and it will be exciting to see what progress we can make in the coming years.&lt;/p&gt;

&lt;p&gt;On the other hand, the success of GANs so far has been limited mostly to Computer Vision due to their difficulty in modelling discrete rather than continuous data. The Adversarial Training workshop showcased some promising work in this direction (see e.g. my colleague &lt;a href="https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_19.pdf"&gt;John Glover’s paper&lt;/a&gt; on modeling documents, &lt;a href="https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_20.pdf"&gt;this&lt;/a&gt; and &lt;a href="https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_27.pdf"&gt;this&lt;/a&gt; paper on generating text, and &lt;a href="https://c4209155-a-62cb3a1a-s-sites.googlegroups.com/site/nips2016adversarial/WAT16_paper_32.pdf"&gt;this paper&lt;/a&gt; on adversarial evaluation of dialogue models). We will see if 2017 will be the year in which GANs break through in NLP.&lt;/p&gt;

&lt;h1 id="thenutsandboltsofmachinelearning"&gt;The Nuts and Bolts of Machine Learning&lt;/h1&gt;

&lt;p&gt;Andrew Ng gave one of the best tutorials of the conference with his take on building AI applications using Deep Learning. Drawing from his experience of managing the 1,300 people AI team at Baidu and hundreds of applied AI projects and equipped solely with two whiteboards, he shared many insights about how to build and deploy AI applications in production.&lt;/p&gt;

&lt;p&gt;Besides better hardware, Ng attributes the success of Deep Learning to two factors: In contrast to traditional methods, deep NNs are able to learn more effectively from large amounts of data. Secondly, end-to-end (supervised) Deep Learning allows us to learn to map from inputs directly to outputs.&lt;/p&gt;

&lt;p&gt;While this approach to training chatbots or self-driving cars is sufficient to write innovative research papers, Ng emphasized end-to-end DL is often not production-ready: A chatbot that maps from text directly to a response is not able to have a coherent conversation or fulfill a request, while mapping from an image directly to a steering command might have literally fatal side effects if the model has not encountered the corresponding part of the input space before. Rather, for a production model, we still want to have intermediate steps: For a chatbot, we prefer to have an inference engine that generates a response, while in a self-driving car, DL is used to identify obstacles, while the steering is performed by a traditional planning algorithm.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/ng_tutorial_end_to_end_dl.jpg" style="width: 100%" title="Andrew Ng on end-to-end Deep Learning" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 5: Andrew Ng on end-to-end DL (right: end-to-end DL chatbot and chatbot with inference engine; left bottom: end-to-end DL self-driving car and self-driving car with intermediate steps)  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Ng also shared that the most common mistake he sees in project teams is that they track the wrong metrics: In an applied machine learning project, the only relevant metrics are the training error, the development error, and the test error. These metrics alone enable the project team to know what steps to take, as he demonstrated in the diagram below:&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/ng_tutorial_bias_variance.jpg" style="width: 100%" title="Andrew Ng's flowchart for applied ML projects" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 6: Andrew Ng’s flowchart for applied ML projects  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;A key facilitator of the recent success of ML have been the advances in hardware that allowed faster computation and storage. Given that Moore's Law will reach its limits sooner or later, one might reason that also the rise of ML might plateau. Ng, however, argued that the &lt;a href="http://fortune.com/2016/11/18/intel-ai-nvidia-analysts/"&gt;commitment by leading hardware manufacturers such as NVIDIA and Intel&lt;/a&gt; and the ensuing performance improvements to ML hardware would fuel further growth.&lt;/p&gt;

&lt;p&gt;Among ML research areas, supervised learning is the undisputed driver of the recent success of ML and will likely continue to drive it for the foreseeable future. In second place, Ng saw neither unsupervised learning nor reinforcement learning, but transfer learning. We at AYLIEN are bullish on transfer learning for NLP and think that it has massive potential.&lt;/p&gt;

&lt;h1 id="recurrentneuralnetworks"&gt;Recurrent Neural Networks&lt;/h1&gt;

&lt;p&gt;The conference also featured a symposium dedicated to Recurrent Neural Networks (RNNs). The symposium coincided with the 20 year anniversary of LSTM...&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/rnn_symposium.png" style="width: 70%" title="Jürgen Schmidhuber at the RNN symposium" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 7: Jürgen Schmidhuber kicking off the RNN symposium  
&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;...being rejected from NIPS 1996. The fact that papers that &lt;em&gt;do not&lt;/em&gt; use LSTMs have been rare in the most recent NLP conferences (see my &lt;a href="http://sebastianruder.com/emnlp-2016-highlights/index.html"&gt;EMNLP 2016 blog post&lt;/a&gt;) is a testament to the perseverance of the authors of the original paper, Sepp Hochreiter and Jürgen Schmidhuber.&lt;/p&gt;

&lt;p&gt;At NIPS, we had several papers that sought to improve RNNs in different ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf"&gt;Ba et al.&lt;/a&gt; and &lt;a href="https://papers.nips.cc/paper/6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf"&gt;Neil et al.&lt;/a&gt; enable RNNs to handle different time scales using slow weights and a phased variant of the LSTM respectively.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6039-sequential-neural-models-with-stochastic-layers.pdf"&gt;Fraccaro et al.&lt;/a&gt; model uncertainty.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other improvements apply to Deep Learning in general:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf"&gt;Salimans and Kingma&lt;/a&gt; propose Weight Normalisation to accelerate training that can be applied in two lines of Python code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6561-improved-dropout-for-shallow-and-deep-learning.pdf"&gt;Li et al.&lt;/a&gt; propose a multinomial variant of dropout that sets neurons to zero depending on the data distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href="https://uclmr.github.io/nampi/"&gt;Neural Abstract Machines &amp;amp; Program Induction (NAMPI) workshop&lt;/a&gt; also featured several speakers talking about RNNs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Alex Graves focused on his recent work on Adaptive Computation Time (ACT) for RNNs that allows to decouple the processing time from the sequence length. He showed that a word-level language model with ACT could reach state-of-the-art with fewer computations.&lt;/li&gt;
&lt;li&gt;Edward Grefenstette outlined several limitations and potential future research directions in the context of RNNs in &lt;a href="https://uclmr.github.io/nampi/talk_slides/grefenstette-nampi.pdf"&gt;his talk&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="improvingclassicalgorithms"&gt;Improving classic algorithms&lt;/h1&gt;

&lt;p&gt;While Deep Learning is a fairly recent development, the conference featured also several improvements to algorithms that have been around for decades:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf"&gt;Ge et al.&lt;/a&gt; show in their best paper that the non-convex objective for matrix completion has no spurious local minima, i.e. every local minimum is a global minimum.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf"&gt;Bachem et al.&lt;/a&gt; present a method that guarantees accurate and fast seedings for large-scale k-means++ clustering. The presentation was one of the most polished ones of the conference and the code is &lt;a href="https://github.com/obachem/kmc2"&gt;open-source&lt;/a&gt; and can be installed via pip.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6449-clustering-with-same-cluster-queries.pdf"&gt;Ashtiani et al.&lt;/a&gt; show that we can make NP-hard k-means clustering problems solvable by allowing the model to pose queries for a few examples to a domain expert.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="reinforcementlearning"&gt;Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;Reinforcement Learning (RL) was another much-discussed topic at NIPS with an &lt;a href="http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf"&gt;excellent tutorial&lt;/a&gt; by Pieter Abbeel and John Schulman. John Schulman also gave some &lt;a href="http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf"&gt;practical advice&lt;/a&gt; for getting started with RL.&lt;/p&gt;

&lt;p&gt;One of the best papers of the conference introduces &lt;a href="https://papers.nips.cc/paper/6046-value-iteration-networks.pdf"&gt;Value Iteration Networks&lt;/a&gt;, which learn to plan by providing a differentiable approximation to a classic planning algorithm via a CNN. This paper was another cool example of one of the major benefits of deep neural networks: They allow us to learn increasingly complex behaviour as long as we can represent it in a differentiable way. I hope to see more approaches in the future that integrate classic algorithms to enhance the capabilities of a neural network.&lt;/p&gt;

&lt;p&gt;During the week of the conference, several research environments for RL were simultaneously released, among them &lt;a href="https://openai.com/blog/universe/"&gt;OpenAI’s Universe&lt;/a&gt;, &lt;a href="https://deepmind.com/blog/open-sourcing-deepmind-lab/"&gt;Deep Mind Lab&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/1611.00625"&gt;FAIR’s Torchcraft&lt;/a&gt;. These will likely be a key driver in future RL research and should open up new research opportunities.&lt;/p&gt;

&lt;h1 id="learningtolearnmetalearning"&gt;Learning-to-learn / Meta-learning&lt;/h1&gt;

&lt;p&gt;Another topic that came up in several discussions over the course of the conference was Learning-to-learn or Meta-learning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6461-learning-to-learn-by-gradient-descent-by-gradient-descent.pdf"&gt;Andrychowicz et al.&lt;/a&gt; learn an optimizer in a paper with the ingenious title "Learning to learn by gradient descent by gradient descent".&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf"&gt;Vinyals et al.&lt;/a&gt; learn how to one shot-learn in a paper that frames one-shot learning in the sequence-to-sequence framework and has inspired new approaches for one-shot learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of the existing papers on meta-learning demonstrate that wherever you are doing something that gives you gradients, you can optimize them using another algorithm via gradient descent. Prepare for a surge of “Meta-learning for X” and “(Meta-)+learning” papers in 2017. It’s LSTMs all the way down!&lt;/p&gt;

&lt;p&gt;Meta-learning was also one of the key talking points at the RNN symposium. Jürgen Schmidhuber argued that a true meta-learner would be able to learn in the space of all programs and would have the ability to modify itself and elaborated on these ideas at &lt;a href="https://uclmr.github.io/nampi/talk_slides/schmidhuber-nampi.pdf"&gt;his talk&lt;/a&gt; at the NAMPI workshop. Ilya Sutskever remarked that we currently have no good meta-learning models. However, there is hope as the plethora of new research environments should also bring progress in this area.&lt;/p&gt;

&lt;h1 id="generalartificialintelligence"&gt;General Artificial Intelligence&lt;/h1&gt;

&lt;p&gt;Learning how to learn also plays a role in the pursuit of the elusive goal of attaining General Artificial Intelligence, which was a topic in several keynotes. Yann LeCun argued that in order to achieve General AI, machines need to learn common sense. While common sense is often vaguely mentioned in research papers, Yann LeCun gave a succinct explanation of what common sense is: "Predicting any part of the past, present or future percepts from whatever information is available." He called this predictive learning, but notes that this is really unsupervised learning.&lt;/p&gt;

&lt;p&gt;His talk also marked the appearance of a controversial and often tongue-in-cheek copied image of a cake, which he used to demonstrate that unsupervised learning is the most challenging task where we should concentrate our efforts, while RL is only the cherry on the icing of the cake.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/lecun_nips_2016_cake_slide.png" style="width: 70%" title="Yann LeCun's Cake slide" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 8: The Cake slide of &lt;a href="https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view"&gt;Yann LeCun's keynote&lt;/a&gt;&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Drew Purves focused on the bilateral relationship between the environment and AI in what was probably the most aesthetically pleasing keynote of the conference (just look at those illustrations!).&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/drew_purves_agent_illustrations.jpg" style="width: 70%" title="Drew Purves keynote illustrations" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 9: Illustrations by Max Cant of Drew Purves' keynote (Source: &lt;a href="https://twitter.com/DrewPurves/status/806427029306560512"&gt;Drew Purves&lt;/a&gt;)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;He emphasized that while simulations of ecological tasks in naturalistic environments could be an important test bed for General AI, General AI is needed to maintain the biosphere in a state that will allow the continued existence of our civilization.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/drew_purves_nips_2016_nature_needs_ai_slide.png" style="width: 60%" title="Nature needs AI" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 10: Nature needs AI and AI needs Nature from Drew Purves' keynote&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;While it is frequently — and incorrectly — claimed that neural networks work so well because they emulate the brain’s behaviour, Saket Navlakha argued during his keynote that we can still learn a great deal from the engineering principles of the brain. For instance, rather than pre-allocating a large number of neurons, the brain generates 1000s of synapses per minutes until its second year. Afterwards, until adolescence, the number of synapses is pruned and decreases by ~50%.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/saket_navlakha_slide.jpg" style="width: 70%" title="Saket Navlakha keynote" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 11: Saket Navlakha’s keynote&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;It will be interesting to see how neuroscience can help us to advance our field further.&lt;/p&gt;

&lt;p&gt;In the context of the &lt;a href="https://mainatnips.github.io/"&gt;Machine Intelligence workshop&lt;/a&gt;, another environment was introduced in the form of FAIR’s CommAI-env that allows to train agents through interaction with a teacher. During the panel discussion, the ability to learn hierarchical representations and to identify patterns was emphasized. However, although the field is making rapid progress on standard tasks such as object recognition, it is unclear if the focus on such specific tasks brings us indeed closer to General AI.&lt;/p&gt;

&lt;h1 id="naturallanguageprocessing"&gt;Natural Language Processing&lt;/h1&gt;

&lt;p&gt;While NLP is more of a niche topic at NIPS, there were a few papers with improvements relevant to NLP:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6469-dual-learning-for-machine-translation.pdf"&gt;He et al.&lt;/a&gt; propose a dual learning framework for MT that has two agents translating in opposite directions teaching each other via reinforcement learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6134-stochastic-structured-prediction-under-bandit-feedback.pdf"&gt;Sokolov et al.&lt;/a&gt; explore how to use structured prediction under bandit feedback.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6139-supervised-word-movers-distance.pdf"&gt;Huang et al.&lt;/a&gt; extend Word Mover’s Distance, an unsupervised document similarity metric to the supervised setting.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://papers.nips.cc/paper/6362-beyond-exchangeability-the-chinese-voting-process.pdf"&gt;Lee et al.&lt;/a&gt; model the helpfulness of reviews by taking into account position and presentation biases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, a &lt;a href="http://letsdiscussnips2016.weebly.com/"&gt;workshop on learning methods for dialogue&lt;/a&gt; explored how end-to-end DL, linguistics and ML methods can be used to create dialogue agents.&lt;/p&gt;

&lt;h1 id="miscellaneous"&gt;Miscellaneous&lt;/h1&gt;

&lt;h2 id="schmidhuber"&gt;Schmidhuber&lt;/h2&gt;

&lt;p&gt;Jürgen Schmidhuber, the father of the LSTM was not only present on several panels, but did his best to remind everyone that whatever your idea, he had had a similar idea two decades ago and you should better cite him lest he interrupt your tutorial.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" data-lang="de"&gt;&lt;p lang="en" dir="ltr"&gt;NIPS2016 Day 1: Poor &lt;a href="https://twitter.com/goodfellow_ian"&gt;@Goodfellow_Ian&lt;/a&gt; gets Schmidhuber&amp;#39;ed during educational GAN Tutorial session. &lt;a href="https://t.co/IeQzKcJYiv"&gt;pic.twitter.com/IeQzKcJYiv&lt;/a&gt;&lt;/p&gt;&amp;mdash; hardmaru (@hardmaru) &lt;a href="https://twitter.com/hardmaru/status/805789874813026304"&gt;5. Dezember 2016&lt;/a&gt;&lt;/blockquote&gt;  

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;h2 id="robotics"&gt;Robotics&lt;/h2&gt;

&lt;p&gt;Boston Robotics’ Spot proved that — even though everyone is excited by learning and learning-to-learn — traditional planning algorithms are enough to win the admiration of a hall full of learning enthusiasts.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/boston_dynamics_spot.png" style="width: 70%" title="Boston Dynamics Spot" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 12: Boston Robotics’ Spot amid a crowd of fascinated onlookers&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="apple"&gt;Apple&lt;/h2&gt;

&lt;p&gt;Apple, one of the most secretive companies in the world, has decided to be more open, to publish, and to engage with academia. This can only be good for the community. I'm particularly looking forward to more &lt;a href="https://arxiv.org/abs/1610.08120v1"&gt;apple research papers&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/russ_salakhutdinov_apple_nips_2016_slide.png" style="width: 60%" title="Ruslan Salakhutdinov at Apple lunch event" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 13: Ruslan Salakhutdinov at the Apple lunch event&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="uber"&gt;Uber&lt;/h2&gt;

&lt;p&gt;Uber announced their acquisition of Cambridge-based AI startup Geometric Intelligence and threw one of the most popular parties of NIPS.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/12/geometric_intelligence_logo.jpg" style="width: 30%" title="Geometric Intelligence Logo" alt="Highlights of NIPS 2016: Adversarial learning, Meta-learning, and more"&gt;
&lt;figcaption&gt;Figure 14: The Geometric Intelligence logo&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;h2 id="rocketai"&gt;Rocket AI&lt;/h2&gt;

&lt;p&gt;Talking about startups, the "launch" of Rocket AI and their patented Temporally Recurrent Optimal Learning had some people fooled (note the acronyms in the below tweets). Riva-Melissa Tez finally &lt;a href="https://medium.com/the-mission/rocket-ai-2016s-most-notorious-ai-launch-and-the-problem-with-ai-hype-d7908013f8c9#.9l7sozt7a"&gt;cleared up the confusion&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" data-lang="de"&gt;&lt;p lang="en" dir="ltr"&gt;&lt;a href="https://twitter.com/hashtag/rocketai?src=hash"&gt;#rocketai&lt;/a&gt; just drove me home. the team is just mind-blowing. so excited about Temporally Recurrent Optimal Learning, the next GAN!&lt;/p&gt;&amp;mdash; Soumith Chintala (@soumithchintala) &lt;a href="https://twitter.com/soumithchintala/status/807762876245176320"&gt;11. Dezember 2016&lt;/a&gt;&lt;/blockquote&gt;  

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;blockquote class="twitter-tweet" data-lang="de"&gt;&lt;p lang="en" dir="ltr"&gt;&lt;a href="https://twitter.com/hashtag/rocketai?src=hash"&gt;#rocketai&lt;/a&gt; definitely has the most popular Jacobian-Optimized Kernel Expansion of NIPS 2016&lt;/p&gt;&amp;mdash; Ian Goodfellow (@goodfellow_ian) &lt;a href="https://twitter.com/goodfellow_ian/status/807721903976841216"&gt;10. Dezember 2016&lt;/a&gt;&lt;/blockquote&gt;  

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;These were my impressions from NIPS 2016. I had a blast and hope to be back in 2017!&lt;/p&gt;</content:encoded></item><item><title>A survey of cross-lingual embedding models</title><description>&lt;p&gt;Note: If you are looking for a survey paper, this blog post is also available as an &lt;a href="https://arxiv.org/abs/1706.04902"&gt;article on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In past blog posts, we discussed different &lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;models&lt;/a&gt;, &lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;objective functions&lt;/a&gt;, and &lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;hyperparameter choices&lt;/a&gt; that allow us to learn accurate word embeddings. However, these models are generally restricted to capture representations&lt;/p&gt;</description><link>http://ruder.io/cross-lingual-embeddings/</link><guid isPermaLink="false">9337835e-1bdf-4fa1-acd1-f23a262380ac</guid><category>word embeddings</category><category>deep learning</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 28 Nov 2016 10:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2016/10/zou_et_al_2013.png" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2016/10/zou_et_al_2013.png" alt="A survey of cross-lingual embedding models"&gt;&lt;p&gt;Note: If you are looking for a survey paper, this blog post is also available as an &lt;a href="https://arxiv.org/abs/1706.04902"&gt;article on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In past blog posts, we discussed different &lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;models&lt;/a&gt;, &lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;objective functions&lt;/a&gt;, and &lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;hyperparameter choices&lt;/a&gt; that allow us to learn accurate word embeddings. However, these models are generally restricted to capture representations of words in the language they were trained on. The availability of resources, training data, and benchmarks in English leads to a disproportionate focus on the English language and a negligence of the plethora of other languages that are spoken around the world. &lt;br&gt;
In our globalised society, where national borders increasingly blur, where the Internet gives everyone equal access to information, it is thus imperative that we do not only seek to eliminate bias pertaining to &lt;a href="http://arxiv.org/abs/1607.06520"&gt;gender or race&lt;/a&gt; inherent in our representations, but also aim to address our bias towards &lt;em&gt;language&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To remedy this and level the linguistic playing field, we would like to leverage our existing knowledge in English to equip our models with the capability to process other languages. &lt;br&gt;
Perfect machine translation (MT) would allow this. However, we do not need to actually translate examples, as long as we are able to project examples into a common subspace such as the one in Figure 1.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/luong_et_al_2015.jpg" style="width: 100%" title="Bilingual embedding space" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 1: A shared embedding space between two languages (&lt;a href="http://stanford.edu/~lmthang/bivec/"&gt;Luong et al., 2015&lt;/a&gt;)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Ultimately, our goal is to learn a shared embedding space between words in &lt;em&gt;all&lt;/em&gt; languages. Equipped with such a vector space, we are able to train our models on data in any language. By projecting examples available in one language into this space, our model simultaneously obtains the capability to perform predictions in all other languages (we are glossing over some considerations here; for these, refer to &lt;a href="http://ruder.io/cross-lingual-embeddings/#challenges"&gt;this section&lt;/a&gt;). This is the promise of cross-lingual embeddings. &lt;/p&gt;

&lt;p&gt;Over the course of this blog post, I will give an overview of models and algorithms that have been used to come closer to this elusive goal of capturing the relations between words in multiple languages in a common embedding space.&lt;/p&gt;

&lt;p&gt;Note: While neural MT approaches &lt;em&gt;implicitly&lt;/em&gt; learn a shared cross-lingual embedding space by optimizing for the MT objective, we will focus on models that &lt;em&gt;explicitly&lt;/em&gt; learn cross-lingual word representations throughout this blog post. These methods generally do so at a much lower cost than MT and can be considered to be to MT what word embedding models (word2vec, GloVe, etc.) are to language modelling. &lt;/p&gt;

&lt;h1 id="typesofcrosslingualembeddingmodels"&gt;Types of cross-lingual embedding models&lt;/h1&gt;

&lt;p&gt;In recent years, various models for learning cross-lingual representations have been proposed. In the following, we will order them by the type of approach that they employ. &lt;br&gt;
Note that while the nature of the parallel data used is equally discriminatory and has been shown to account for inter-model performance differences [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;], we consider the type of approach more conducive to understanding the assumptions a model makes and -- consequently -- its advantages and deficiencies. &lt;br&gt;
Cross-lingual embedding models generally use four different approaches:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Monolingual mapping&lt;/strong&gt;: These models initially train monolingual word embeddings on large monolingual corpora. They then learn a linear mapping between monolingual representations in different languages to enable them to map unknown words from the source language to the target language.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pseudo-cross-lingual&lt;/strong&gt;: These approaches create a pseudo-cross-lingual corpus by mixing contexts of different languages. They then train an off-the-shelf word embedding model on the created corpus. The intuition is that the cross-lingual contexts allow the learned representations to capture cross-lingual relations.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-lingual training&lt;/strong&gt;: These models train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Joint optimization&lt;/strong&gt;: These approaches train their models on parallel (and optionally monolingual data). They jointly optimise a combination of monolingual and cross-lingual losses.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In terms of parallel data, methods may use different supervision signals that depend on the type of data used. These are, from most to least expensive:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Word-aligned data&lt;/strong&gt;: A parallel corpus with word alignments that is commonly used for machine translation; this is the most expensive type of parallel data to use.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sentence-aligned data&lt;/strong&gt;: A parallel corpus without word alignments. If not otherwise specified, the model uses the &lt;a href="http://www.statmt.org/europarl/"&gt;Europarl corpus&lt;/a&gt; consisting of sentence-aligned text from the proceedings of the European parliament that is generally used for training Statistical Machine Translation models.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Document-aligned data&lt;/strong&gt;: A corpus containing documents in different languages. The documents can be topic-aligned (e.g. Wikipedia) or label/class-aligned (e.g. sentiment analysis and multi-class classification datasets).  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lexicon&lt;/strong&gt;: A bilingual or cross-lingual dictionary with pairs of translations between words in different languages.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No parallel data&lt;/strong&gt;: No parallel data whatsoever. Learning cross-lingual representations from only monolingual resources would enable zero-shot learning across languages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To make the distinctions clearer, we provide the following table, which serves equally as the table of contents and a springboard to delve deeper into the different cross-lingual models:&lt;/p&gt;

&lt;style type="text/css"&gt;  
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;  

&lt;table class="tg"&gt;  
  &lt;tr&gt;
    &lt;th class="tg-9hbo"&gt;Approach&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;Method&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;Parallel data&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l" rowspan="9"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#monolingualmapping"&gt;Mono-lingual mapping&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#linearprojection"&gt;Linear projection (Mikolov et al., 2013)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="4"&gt;Lexicon&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#projectionviacca"&gt;Projection via CCA (Faruqui and Dyer, 2014)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#normalisationandorthogonaltransformation"&gt;Normalisation and orthogonal transformation (Xing et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#maxmarginandintruders"&gt;Max-margin and intruders (Lazaridou et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#alignmentbasedprojection"&gt;Alignment-based projection (Guo et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Word-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#multilingualcca"&gt;Multilingual CCA (Ammar et al., 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Lexicon&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#hybridmappingwithsymmetricseedlexicon"&gt;Hybrid mapping with symmetric seed lexicon (Vulić and Korhonen, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Lexicon, document-aligned&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#orthogonaltransformationnormalisationandmeancentering"&gt;Orthogonal transformation, normalisation, and mean centering (Artetxe et al., 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Lexicon&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#adversarialautoencoder"&gt;Adversarial auto-encoder (Barone, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l" rowspan="5"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#pseudocrosslingual"&gt;Pseudo-cross-lingual&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#mappingoftranslationstosamerepresentation"&gt;Mapping of translations to same representation (Xiao and Guo, 2014)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="4"&gt;Lexicon&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#randomtranslationreplacement"&gt;Random translation replacement (Gouws and Sogaard, 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#ontheflyreplacementandpolysemyhandling"&gt;On-the-fly replacement and polysemy handling (Duong et al., 2016)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#multilingualcluster"&gt;Multilingual cluster (Ammar et al., 2016)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#documentmergeandshuffle"&gt;Document merge and shuffle (Vulić and Moens, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Document-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l" rowspan="8"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#crosslingualtraining"&gt;Cross-lingual training&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualcompositionalsentencemodel"&gt;Bilingual compositional sentence model (Hermann and Blunsom, 2013)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="2"&gt;Sentence-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualbagofwordsautoencoder"&gt;Bilingual bag-of-words autoencoder (Lauly et al., 2013)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#distributedwordalignment"&gt;Distributed word alignment (Kočiský et al., 2014)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="4"&gt;Sentence-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualcompositionaldocumentmodel"&gt;Bilingual compositional document model (Hermann and Blunsom, 2014)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bagofwordsautoencoderwithcorrelation"&gt;Bag-of-words autoencoder with correlation (Chandar et al., 2014)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualparagraphvectors"&gt;Bilingual paragraph vectors (Pham et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#translationinvariantlsa"&gt;Translation-invariant LSA (Gardner et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Lexicon&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#invertedindexingonwikipedia"&gt;Inverted indexing on Wikipedia (Søgaard et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Document-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l" rowspan="8"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#jointoptimisation"&gt;Joint optimisation&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#multitasklanguagemodel"&gt;Multi-task language model (Klementiev et al., 2012)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="3"&gt;Word-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualmatrixfactorisation"&gt;Bilingual matrix factorisation (Zou et al., 2013)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualskipgram"&gt;Bilingual skip-gram (Luong et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualbagofwordswithoutwordalignments"&gt;Bilingual bag-of-words without word alignments (Gouws et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l" rowspan="3"&gt;Sentence-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualskipgramwithoutwordalignments"&gt;Bilingual skip-gram without word alignments (Coulmance et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#jointmatrixfactorisation"&gt;Joint matrix factorisation (Shi et al., 2015)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;tr&gt;  
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualsparserepresentations"&gt;Bilingual sparse representations (Vyas and Carpuat, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Word-aligned&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualparagraphvectorswithoutparalleldata"&gt;Bilingual paragraph vectors (without parallel data) (Mogadala and Rettinger, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;Sentence-aligned/-&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;After the discussion of cross-lingual embedding models, we will additionally look into how to &lt;a href="http://ruder.io/cross-lingual-embeddings/#incorporatingvisualinformation"&gt;incorporate visual information&lt;/a&gt; into word representations, discuss the &lt;a href="http://ruder.io/cross-lingual-embeddings/#challenges"&gt;challenges&lt;/a&gt; that still remain in learning cross-lingual representations, and finally summarize which models perform best and how to &lt;a href="http://ruder.io/cross-lingual-embeddings/#evaluation"&gt;evaluate&lt;/a&gt; them.&lt;/p&gt;

&lt;h1 id="monolingualmapping"&gt;Monolingual mapping&lt;/h1&gt;

&lt;p&gt;Methods that employ monolingual mapping train monolingual word representations independently on large monolingual corpora. They then seek to learn a transformation matrix that maps representations in one language to the representations of the other language. They usually employ a set of source word-target word pairs that are translations of each other, which are used as anchor words for learning the mapping.&lt;/p&gt;

&lt;p&gt;Note that all of the following methods presuppose that monolingual embedding spaces have already been trained. If not stated otherwise, these embedding spaces have been learned using the word2vec variants, skip-gram with negative sampling (SGNS) or continuous bag-of-words (CBOW) on large monolingual corpora.&lt;/p&gt;

&lt;h2 id="linearprojection"&gt;Linear projection&lt;/h2&gt;

&lt;p&gt;Mikolov et al. have popularised the notion that vector spaces can encode meaningful relations between words. In addition, they notice that the geometric relations that hold between words are similar across languages [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;], e.g. numbers and animals in English show a similar geometric constellation as their Spanish counterparts in Figure 2.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/mikolov_et_al_linear_relationship.png" style="width: 90%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 2: Similar geometric relations between numbers and animals in English and Spanish (Mikolov et al., 2013)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;This suggests that it might be possible to transform one language's vector space into the space of another simply by utilising a linear projection with a transformation matrix \(W\).&lt;/p&gt;

&lt;p&gt;In order to achieve this, they translate the 5,000 most frequent words from the source language and use these 5,000 translations pairs as bilingual dictionary. They then learn \(W\) using stochastic gradient descent by minimising the distance between the previously learned monolingual representations \(x_i\) of the source word \(w_i\) that is transformed using \(W\) and its translation \(z_i\) in the bilingual dictionary:&lt;/p&gt;

&lt;p&gt;\(\min\limits_W \sum\limits^n_{i=1} \|Wx_i - z_i\|^2 \).&lt;/p&gt;

&lt;h2 id="projectionviacca"&gt;Projection via CCA&lt;/h2&gt;

&lt;p&gt;Faruqui and Dyer [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] propose to use another technique to learn the linear mapping. They use canonical correlation analysis (CCA) to project words from two languages into a shared embedding space. Different to linear projection, CCA learns a transformation matrix for every language, as can be seen in Figure 3, where the transformation matrix \(V\) is used to project word representations from the embedding space \(\Sigma\) to a new space \(\Sigma^\ast\), while \(W\) transforms words from \(\Omega\) to \(\Omega^\ast\). Note that \(\Sigma^\ast\) and \(\Omega^\ast\) can be seen as the same shared embedding space.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/cca_projection.png" style="width: 60%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 3: Cross-lingual projection using CCA (Faruqui and Dyer, 2014)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Similar to linear projection, CCA also requires a number of translation pairs in \(\Sigma'\) and \(\Omega'\) whose correlation can be maximised. Faruqui and Dyer obtain these pairs by selecting for each source word the target word to which it has been aligned most often in a parallel corpus. Alternatively, they could have also used a bilingual dictionary. &lt;br&gt;
As CCA sorts the correlation vectors in \(V\) and \(W\) in descending order, Faruqui and Dyer perform experiments using only the top \(k\) correlated projection vectors and find that using the \(80\) % projection vectors with the highest correlation generally yields the highest performance. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/synonym_antonym_projections.png" style="width: 80%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 4: Monolingual (top) and multi-lingual (bottom; marked with apostrophe) projections of the synonyms and antonyms of "beautiful" (Faruqui and Dyer, 2014)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Interestingly, they find that using multilingual projection helps to separate synonyms and antonyms in the source language, as can be seen in Figure 4, where the unprotected antonyms of "beautiful" are in two clusters in the top, whereas the CCA-projected vectors of the synonyms and antonyms form two distinct clusters in the bottom.&lt;/p&gt;

&lt;h2 id="normalisationandorthogonaltransformation"&gt;Normalisation and orthogonal transformation&lt;/h2&gt;

&lt;p&gt;Xing et al. [&lt;sup id="fnref:33"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:33" rel="footnote"&gt;33&lt;/a&gt;&lt;/sup&gt;] notice inconsistencies in the linear projection method by Mikolov et al. (2013), which they set out to resolve. Recall that Mikolov et al. initially learn monolingual word embeddings. For this, they use the skip-gram objective, which is the following:&lt;/p&gt;

&lt;p&gt;\(\dfrac{1}{N} \sum\limits_{i=1}^N \sum\limits_{-C \leq j \leq C, j \neq 0} \text{log} P(w_{i+j} \:|\: w_i) \)&lt;/p&gt;

&lt;p&gt;where \(C\) is the context length and \(P(w_{i+j} \:|\: w_i)\) is computed using the softmax:&lt;/p&gt;

&lt;p&gt;\(P(w_{i+j} \:|\: w_i) = \dfrac{\text{exp}(c_{w_{i+j}}^T c_{w_i})}{\sum_w \text{exp}(c_w^T c_{w_i})}\).&lt;/p&gt;

&lt;p&gt;They then learn a linear transformation between the two monolingual vector spaces with:&lt;/p&gt;

&lt;p&gt;\(\text{min} \sum\limits_i \|Wx_i - z_i\|^2 \)&lt;/p&gt;

&lt;p&gt;where \(W\) is the projection matrix that should be learned and \(x_i\) and \(z_i\) are word vectors in the source and target language respectively that are similar in meaning.&lt;/p&gt;

&lt;p&gt;Xing et al. argue that there is a mismatch between the objective function used to learn word representations (maximum likelihood based on inner product), the distance measure for word vectors (cosine similarity), and the objective function used to learn the linear transformation (mean squared error), which may lead to degradation in performance.&lt;/p&gt;

&lt;p&gt;They subsequently propose a method to resolve each of these inconsistencies: In order to fix the mismatch between the inner product similarity measure \(c_w^T c_{w'}\) during training and the cosine similarity measure \(\dfrac{c_w^T c_w'}{\|c_w\| \|c_{w'}\|}\) for testing, the inner product could also be used for testing. Cosine similarity, however, is used conventionally as an evaluation measure in NLP and generally performs better than the inner product. For this reason, they propose to normalise the word vectors to be unit length during training, which makes the inner product the same as cosine similarity and places all word vectors on a hypersphere as a side-effect, as can be seen in Figure 5.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/hypersphere_word_representations.png" style="width: 80%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 5: Word representations before (left) and after (right) normalisation (Xing et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;They resolve the inconsistency between the cosine similarity measure now used in training and the mean squared error employed for learning the transformation by replacing the mean squared error with cosine similarity for learning the mapping, which yields:&lt;/p&gt;

&lt;p&gt;\(\max\limits_W \sum\limits_i (Wx_i)^T z_i \).&lt;/p&gt;

&lt;p&gt;Finally, in order to also normalise the projected vector \(Wx_i\) to be unit length, they constrain \(W\) to be an orthogonal matrix by solving a separate optimisation problem.&lt;/p&gt;

&lt;h2 id="maxmarginandintruders"&gt;Max-margin and intruders&lt;/h2&gt;

&lt;p&gt;Lazaridou et al. [&lt;sup id="fnref:28"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:28" rel="footnote"&gt;28&lt;/a&gt;&lt;/sup&gt;] identify another issue with the linear transformation objective of Mikolov et al. (2013): They discover that using least-squares as objective for learning a projection matrix leads to &lt;em&gt;hubness&lt;/em&gt;, i.e. some words tend to appear as nearest neighbours of many other words. To resolve this, they use a margin-based (max-margin) ranking loss (Collobert et al. [&lt;sup id="fnref:34"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:34" rel="footnote"&gt;34&lt;/a&gt;&lt;/sup&gt;]) to train the model to rank the correct translation vector \(y_i\) of a source word \(x_i\) that is projected to \(\hat{y_i}\) higher than any other target words \(y_j\):&lt;/p&gt;

&lt;p&gt;\(\sum\limits^k_{j\neq i} \max \{ 0, \gamma + cos(\hat{y_i}, y_i) - cos(\hat{y_i}, y_j) \} \)&lt;/p&gt;

&lt;p&gt;where \(k\) is the number of negative examples and \(\gamma\) is the margin. &lt;/p&gt;

&lt;p&gt;They show that selecting max-margin over the least-squares loss consistently improves performance and reduces hubness. In addition, the choice of the negative examples, i.e. the target words compared to which the model should rank the correct translation higher, is important. They hypothesise that an informative negative example is an &lt;em&gt;intruder&lt;/em&gt; ("truck" in the example), i.e. it is near the current projected vector \(\hat{y_i}\) but far from the actual translation vector \(y_i\) ("cat") as depicted in Figure 6.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/intruders_in_the_vector_space.png" style="width: 20%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 6: The intruder "truck" is selected over "dog" as the negative example for "cat". (Lazaridou et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;These intruders should help the model identify cases where it is failing considerably to approximate the target function and should thus allow it to correct its behaviour. At every step of gradient descent, they compute \(s_j = cos(\hat{y_i}, y_j) - cos(y_i, y_j) \) for all vectors \(y_t\) in the target embedding space with \(j \neq i\) and choose the vector with the largest \(s_j\) as negative example for \(x_i\). Using intruders instead of random negative examples yields a small improvement of 2 percentage points on their comparison task.&lt;/p&gt;

&lt;h2 id="alignmentbasedprojection"&gt;Alignment-based projection&lt;/h2&gt;

&lt;p&gt;Guo et al. [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;] propose another projection method that solely relies on word alignments. They count the number of times each word in the source language is aligned with each word in the target language in a parallel corpus and store these counts in an alignment matrix \(\mathcal{A}\).&lt;/p&gt;

&lt;p&gt;In order to project a word \(w_i\) from its source representation \(v(w_i^S)\) to its representation in the target embedding space \(v(w_i)^T\) in the target embedding space, they simply take the average of the embeddings of its translations \(v(w_j)^T\) weighted by their alignment probability with the source word:&lt;/p&gt;

&lt;p&gt;\(v(w_i)^T = \sum\limits_{i, j \in \mathcal{A}} \dfrac{c_{i, j}}{\sum_j c_{i,j}} \cdot v(w_j)^T\)&lt;/p&gt;

&lt;p&gt;where \(c_{i,j}\) is the number of times the \(i^{th}\) source word has been aligned to the \(j^{th}\) target word.&lt;/p&gt;

&lt;p&gt;The problem with this method is that it only assigns embeddings for words that are aligned in the reference parallel corpus. Gou et al. thus propagate alignments from in-vocabulary to OOV words by using edit distance as a metric for morphological similarity. They set the projected vector of an OOV source word \(v(w_{OOV}^T)\) as the average of the projected vectors of source words that are similar to it in edit distance:&lt;/p&gt;

&lt;p&gt;\(v(w_{OOV}^T) = Avg(v(w_T))\)&lt;/p&gt;

&lt;p&gt;where \(C = \{ w \:|\: EditDist(w_{OOV}^T, w) \leq \tau \} \). They set the threshold \(\tau\) empirically to \(1\). &lt;br&gt;
Even though this approach seems simplistic, they actually observe significant improvements over projection via CCA in their experiments.&lt;/p&gt;

&lt;h2 id="multilingualcca"&gt;Multilingual CCA&lt;/h2&gt;

&lt;p&gt;Ammar et al. [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;] extend the bilingual CCA projection method of Faruqui and Dyer (2014) to the multi-lingual setting using the English embedding space as the foundation for their multilingual embedding space.&lt;/p&gt;

&lt;p&gt;They learn the two projection matrices for every other language with English. The transformation from each target language space \(\Omega\) to the English embedding space \(\Sigma\) can then be obtained by projecting the vectors in \(\Omega\) into the CCA space \(\Omega^\ast\) using the transformation matrix \(W\) as in Figure 3. As \(\Omega^\ast\) and \(\Sigma^\ast\) lie in the same space, vectors in \(\Sigma^\ast\) can be projected into the English embedding space \(\Sigma\) using the inverse of \(V\).&lt;/p&gt;

&lt;h2 id="hybridmappingwithsymmetricseedlexicon"&gt;Hybrid mapping with symmetric seed lexicon&lt;/h2&gt;

&lt;p&gt;The previous mapping approaches used a bilingual dictionary as inherent component of their model, but did not pay much attention to the quality of the dictionary entries, using either automatic translations of frequent words or word alignments of all words. &lt;/p&gt;

&lt;p&gt;Vulić and Korhonen [&lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] in turn emphasise the role of the seed lexicon that is used for learning the projection matrix. They propose a hybrid model that initially learns a first shared bilingual embedding space based on an existing cross-lingual embedding model. They then use this initial vector space to obtain translations for a list of frequent source words by projecting them into the space and using the nearest neighbour in the target language as translation. With these translation pairs as seed words, they learn a projection matrix analogously to Mikolov et al. (2013). &lt;br&gt;
In addition, they propose a symmetry constraint, which enforces that words are only included if their projections are neighbours of each other in the first embedding space. Additionally, one can retain pairs whose second nearest neighbours are less similar than the first nearest neighbours up to some threshold. &lt;br&gt;
They run experiments showing that their model with the symmetry constraint outperforms comparison models and that a small threshold of \(0.01\) or \(0.025\) leads to slightly improved performance.&lt;/p&gt;

&lt;h2 id="orthogonaltransformationnormalisationandmeancentering"&gt;Orthogonal transformation, normalisation, and mean centering&lt;/h2&gt;

&lt;p&gt;The previous approaches have introduced models that imposed different constraints for mapping monolingual representations of different languages to each other. The relation between these methods and constraints, however, is not clear.&lt;/p&gt;

&lt;p&gt;Artetxe et al. [&lt;sup id="fnref:32"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:32" rel="footnote"&gt;32&lt;/a&gt;&lt;/sup&gt;] thus propose to generalise previous work on learning a linear transformation between monolingual vector spaces: Starting with the basic optimisation objective, they propose several constraints that should intuitively help to improve the quality of the learned cross-lingual representations. Recall that the linear transformation learned by Mikolov et al. (2013) aims to find a parameter matrix \(W\) that satisfies:&lt;/p&gt;

&lt;p&gt;\(\DeclareMathOperator*{\argmin}{argmin} \argmin\limits_W \sum\limits_i \|Wx_i - z_i\|^2 \)&lt;/p&gt;

&lt;p&gt;where \(x_i\) and \(z_i\) are similar words in the source and target language respectively.&lt;/p&gt;

&lt;p&gt;If the performance of the embeddings on a monolingual evaluation task should not be degraded, the dot products need to be preserved after the mapping. This can be guaranteed by requiring \(W\) to be an orthogonal matrix.&lt;/p&gt;

&lt;p&gt;Secondly, in order to ensure that all embeddings contribute equally to the objective, embeddings in both languages can be normalised to be unit vectors:&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i \| W \dfrac{x_i}{\|x_i\|} - \dfrac{z_i}{\|z_i\|}\|^2 \).&lt;/p&gt;

&lt;p&gt;As the norm of an orthogonal matrix is \(1\), if \(W\) is orthogonal, we can add it to the denominator and move \(W\) to the numerator:&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i \| \dfrac{Wx_i}{\|Wx_i\|} - \dfrac{z_i}{\|z_i\|}\|^2 \).&lt;/p&gt;

&lt;p&gt;Through expansion of the above binomial, we obtain:&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i \|\dfrac{Wx_i}{\|Wx_i\|}\|^2  + \|\dfrac{z_i}{\|z_i||}\|^2 - 2 \dfrac{Wx_i}{\|Wx_i\|}^T \dfrac{z_i}{\|z_i\|}  \).&lt;/p&gt;

&lt;p&gt;As the norm of a unit vector is \(1\) the first two terms reduce to \(1\), which leaves us with the following:&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i 2 - 2 \dfrac{Wx_i}{\|Wx_i\|}^T \dfrac{z_i}{\|z_i\|} ) \).&lt;/p&gt;

&lt;p&gt;The latter term now is just the cosine similarity of \(Wx_i\) and \(z_i\):&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i 2 - 2 \: \text{cos}(Wx_i, z_i) \).&lt;/p&gt;

&lt;p&gt;As we are interested in finding parameters \(W\) that minimise our objective, we can remove the constants above:&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i - \: \text{cos}(Wx_i, z_i) \).&lt;/p&gt;

&lt;p&gt;Minimising the sum of negative cosine similarities is then equal to maximising the sum of cosine similarities, which gives us the following:&lt;/p&gt;

&lt;p&gt;\(\DeclareMathOperator*{\argmax}{argmax} \argmax\limits_W \sum\limits_i \text{cos}(Wx_i, z_i) \).&lt;/p&gt;

&lt;p&gt;This is equal to the objective by Xing et al. (2015), although they motivated it via an inconsistency of the objectives.&lt;/p&gt;

&lt;p&gt;Finally, Artetxe et al. argue that two randomly selected words are generally expected not to be similar. For this reason, the cosine of their embeddings in any dimension -- as well as their cosine similarity -- should be zero. They capture this intuition by performing dimension-wise mean centering with a centering matrix \(C_m\):&lt;/p&gt;

&lt;p&gt;\(\argmin\limits_W \sum\limits_i ||C_mWx_i - C_mz_i||^2 \).&lt;/p&gt;

&lt;p&gt;This reduces to maximizing the sum of dimension-wise covariance as long as \(W\) is orthogonal similar as above:&lt;/p&gt;

&lt;p&gt;\(\argmax\limits_W \sum\limits_i \text{cov}(Wx_i, z_i) \).&lt;/p&gt;

&lt;p&gt;Interestingly, the method by Faruqui and Dyer (2014) is similar to this objective, as CCA maximizes the dimension-wise covariance of both projections. This is equivalent to the single projection here, as it is constrained to be orthogonal. The only difference is that, while CCA changes the monolingual embeddings so that different dimensions have the same variance and are uncorrelated -- which might degrade performance -- Artetxe et al. enforce monolingual invariance. &lt;/p&gt;

&lt;h2 id="adversarialautoencoder"&gt;Adversarial auto-encoder&lt;/h2&gt;

&lt;p&gt;All previous approaches to learning a transformation matrix between monolingual representations in different languages require either a dictionary or word alignments as a source of parallel data.&lt;/p&gt;

&lt;p&gt;Barone [&lt;sup id="fnref:7"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:7" rel="footnote"&gt;7&lt;/a&gt;&lt;/sup&gt;], in contrast, seeks to get closer to the elusive goal of creating cross-lingual representations without parallel data. He proposes to use an adversarial auto-encoder to transform source embeddings into the target embedding space. The auto-encoder is then trained to reconstruct the source embeddings, while the discriminator is trained to differentiate the projected source embeddings from the actual target embeddings as in Figure 7.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/adversarial_autoencoder-1.png" style="width: 80%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 7: Cross-lingual mapping with an adversarial auto-encoder (Barone, 2016)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;While intriguing, learning a transformation between languages without any parallel data at all seems unfeasible at this point. However, future approaches that aim to learn a mapping with fewer and fewer parallel data may bring us closer to this goal.&lt;/p&gt;

&lt;p&gt;More generally, however, it remains unclear if a projection can reliably transform the embedding space of one language into the embedding space of another language. Additionally, the reliance on lexicon data or word alignment information is expensive.&lt;/p&gt;

&lt;h1 id="pseudocrosslingual"&gt;Pseudo-cross-lingual&lt;/h1&gt;

&lt;p&gt;The second type of cross-lingual models seeks to construct a pseudo-cross-lingual corpus that captures interactions between the words in different languages. Most approaches aim to identify words that can be translated to each other in monolingual corpora of different languages and replace these with placeholders to ensure that translations of the same word have the same vector representation.&lt;/p&gt;

&lt;h2 id="mappingoftranslationstosamerepresentation"&gt;Mapping of translations to same representation&lt;/h2&gt;

&lt;p&gt;Xiao and Guo [&lt;sup id="fnref:8"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:8" rel="footnote"&gt;8&lt;/a&gt;&lt;/sup&gt;] propose the first pseudo-cross-lingual method that leverages translation pairs: They first translate all words that appear in the source language corpus into the target language using Wiktionary. As these translation pairs are still very noisy, they filter them by removing polysemous words in the source and target language and translations that do not appear in the target language corpus. From this bilingual dictionary, they now create a joint vocabulary, in which each translation pair has the same vector representation.&lt;/p&gt;

&lt;p&gt;For training, they use the margin-based ranking loss of Collobert et al. (2008) to rank correct word windows higher than corrupted ones, where the middle word is replaced by an arbitrary word. &lt;br&gt;
In contrast to the subsequent methods, they do not construct a pseudo-cross-lingual corpus explicitly. Instead, they feed windows of both the source and target corpus into the model during training, thereby essentially interpolating source and target language. &lt;br&gt;
It is thus most likely that, for ease of training, the authors replace translation pairs in source and target corpus with a placeholder to ensure a common vector representation, similar to the procedure of subsequent models.&lt;/p&gt;

&lt;h2 id="randomtranslationreplacement"&gt;Random translation replacement&lt;/h2&gt;

&lt;p&gt;Gouws and Søgaard [&lt;sup id="fnref:9"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:9" rel="footnote"&gt;9&lt;/a&gt;&lt;/sup&gt;] in turn explicitly create a pseudo-cross-lingual corpus: They leverage translation pairs of words in the source and in the target language obtained via Google Translate. They concatenate the source and target corpus and replace each word that is part of a translation pair with its translation equivalent with a probability of 50%. They then train CBOW on this corpus. &lt;br&gt;
It is interesting to note that they also experiment with replacing words not based on translation but part-of-speech equivalence, i.e. words with the same part-of-speech in different languages will be replaced with one another. While replacement based on part-of-speech leads to small improvements for cross-lingual part-of-speech tagging, replacement based on translation equivalences yields even better performance for the task. &lt;/p&gt;

&lt;h2 id="ontheflyreplacementandpolysemyhandling"&gt;On-the-fly replacement and polysemy handling&lt;/h2&gt;

&lt;p&gt;Duong et al. [&lt;sup id="fnref:10"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:10" rel="footnote"&gt;10&lt;/a&gt;&lt;/sup&gt;] propose a similar approach to Gouws and Søgaard (2015). They also use CBOW, which predicts the centre word in a window given the surrounding words. Instead of randomly replacing every word in the corpus with its translation during pre-processing, they replace each centre word with a translation on-the-fly during training.&lt;/p&gt;

&lt;p&gt;In addition to past approaches, they also seek to handle polysemy explicitly by proposing an EM-inspired method that chooses as replacement the translation \(\bar{w_i}\) whose representation is most similar to the combination of the representations of the source word \(v_{w_i}\) and the context vector \(h_i\):&lt;/p&gt;

&lt;p&gt;\(\bar{w_i} = \text{argmax}_{w \: \in \: \text{dict}(w_i)} \: \text{cos}(v_{w_i} + h_i, v_w) \)&lt;/p&gt;

&lt;p&gt;where \(\text{dict}(w_i)\) contains the translations of \(w_i\).&lt;/p&gt;

&lt;p&gt;They then jointly learn to predict both the words and their appropriate translations. They use PanLex as bilingual dictionary, which covers around 1,300 language with about 12 million expressions. Consequently, translations are high coverage but often noisy.&lt;/p&gt;

&lt;h2 id="multilingualcluster"&gt;Multilingual cluster&lt;/h2&gt;

&lt;p&gt;Ammar et al. (2016) propose another approach that is similar to the previous method by Gouws and Søgaard (2015): They use bilingual dictionaries to find clusters of synonymous words in different languages. They then concatenate the monolingual corpora of different languages and replace tokens in the same cluster with the cluster ID. They then train SGNS on the concatenated corpus.&lt;/p&gt;

&lt;h2 id="documentmergeandshuffle"&gt;Document merge and shuffle&lt;/h2&gt;

&lt;p&gt;The previous methods all use a bilingual dictionary or a translation tool as a source of translation pairs that can be used for replacement. &lt;/p&gt;

&lt;p&gt;Vulić and Moens [&lt;sup id="fnref:11"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:11" rel="footnote"&gt;11&lt;/a&gt;&lt;/sup&gt;] present a model that  does without translation pairs and learns cross-lingual embeddings only from document-aligned data. In contrast to the previous methods, the authors propose not to merge two monolingual corpora but two aligned documents of different languages into a pseudo-bilingual document.&lt;/p&gt;

&lt;p&gt;They concatenate the documents and then shuffle them by randomly permutating the words. The intuition is that as most methods rely on learning word embeddings based on their context, shuffling the documents would lead to bilingual contexts for each word that will enable the creation of a robust embedding space. As shuffling is necessarily random, however, it might lead to sub-optimal configurations. &lt;br&gt;
For this reason, they propose another merging strategy that assumes that the structures of the document are similar: They then alternatingly insert words from each language into the pseudo-bilingual document in the order in which they appear in their monolingual document and based on the mono-lingual documents' length ratio.&lt;/p&gt;

&lt;p&gt;While pseudo-cross-lingual approaches are attractive due to their simplicity and ease of implementation, relying on naive replacement and permutation does not allow them to capture more sophisticated facets of cross-lingual relations.&lt;/p&gt;

&lt;h1 id="crosslingualtraining"&gt; Cross-lingual training&lt;/h1&gt;

&lt;p&gt;Cross-lingual training approaches focus exclusively on optimising the cross-lingual objective. These approaches typically rely on sentence alignments rather than a bilingual lexicon and require a parallel corpus for training. &lt;/p&gt;

&lt;h2 id="bilingualcompositionalsentencemodel"&gt; Bilingual compositional sentence model&lt;/h2&gt;

&lt;p&gt;The first approach that optimizes only a cross-lingual objective is the bilingual compositional sentence model by Hermann and Blunsom [&lt;sup id="fnref:12"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:12" rel="footnote"&gt;12&lt;/a&gt;&lt;/sup&gt;]. They train two models to produce sentence representations of aligned sentences in two languages and use the distance between the two sentence representations as objective. They minimise the following loss:&lt;/p&gt;

&lt;p&gt;\(E_{dist}(a,b) = \|a_{\text{root}} - b_{\text{root}} \|^2 \)&lt;/p&gt;

&lt;p&gt;where \(a_{\text{root}}\) and \(b_{\text{root}}\) are the representations of two aligned sentences from different languages. They compose \(a_{\text{root}}\) and \(b_{\text{root}}\) simply as the sum of the embeddings of the words in the corresponding sentence. The full model is depicted in Figure 8.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/10/hermann_blunsom_2013.png" style="width: 80%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 8: The bilingual compositional sentence model (Hermann and Blunsom, 2013)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;They train the model then to output a higher score for correct translations than for randomly sampled incorrect translations using the max-margin hinge loss of Collobert et al. (2008).&lt;/p&gt;

&lt;h2 id="bilingualbagofwordsautoencoder"&gt;Bilingual bag-of-words autoencoder&lt;/h2&gt;

&lt;p&gt;Instead of minimising the distance between two sentence representations in different languages, Lauly et al. [&lt;sup id="fnref:13"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:13" rel="footnote"&gt;13&lt;/a&gt;&lt;/sup&gt;] aim to reconstruct the target sentence from the original source sentence. They start with a monolingual autoencoder that encodes an input sentence as a sum of its word embeddings and tries to reconstruct the original source sentence. For efficient reconstruction, they opt for a tree-based decoder that is similar to a hierarchical softmax. They then augment this autoencoder with a second decoder that reconstructs the aligned target sentence from the representation of the source sentence as in Figure 9.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/lauly_autoencoder.png" style="width: 70%" title="Similar geometric relations between two languages" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 9: A bilingual autoencoder (Lauly et al., 2013)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Encoders and decoders have language-specific parameters. For an aligned sentence pair, they then train the model with four reconstruction losses: for each of the two sentences, they reconstruct from the sentence to itself and to its equivalent in the other language.&lt;/p&gt;

&lt;h2 id="distributedwordalignment"&gt; Distributed word alignment&lt;/h2&gt;

&lt;p&gt;While the previous approaches required word alignments as a prerequisite for learning cross-lingual embeddings, Kočiský et al. [&lt;sup id="fnref:15"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:15" rel="footnote"&gt;15&lt;/a&gt;&lt;/sup&gt;] simultaneously learn word embeddings and alignments. Their model, Distributed Word Alignment, combines a distributed version of FastAlign (Dyer et al. [&lt;sup id="fnref:35"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:35" rel="footnote"&gt;35&lt;/a&gt;&lt;/sup&gt;]) with a language model. Similar to other bilingual approaches, they use the word in the source language sentence of an aligned sentence pair to predict the word in the target language sentence.&lt;/p&gt;

&lt;p&gt;They replace the standard  multinomial translation probability of FastAlign with an energy function that tries to bring the representation of a target word \(f\) close to the sum of the context words around the word \(e_i\) in the source sentence:&lt;/p&gt;

&lt;p&gt;\(E(f, e_i) = - ( \sum\limits_{s=-k}^k r^T_{e_{i+s}} T_s) r_f - b_r^T r_f - b_f \)&lt;/p&gt;

&lt;p&gt;where \(r_{e_{i+s}}\) and \(r_f\) are vector representations for source and target words, \(T_s\) is a projection matrix, and \(b_r\) and \(b_f\) are representation and target biases respectively. For calculating the translation probability \(p(f|e_i)\), we then simply need to apply the softmax to the translation probabilities between the source word and all words in the target language.&lt;/p&gt;

&lt;p&gt;In addition, the authors speed up training by using a class factorisation strategy similar to the hierarchical softmax and predict frequency-based class representations instead of word representations. For training, they also use EM but fix the alignment counts learned by FastAlign that was initially trained for 5 epochs during the E-step and optimise the translation probabilities in the M-step only.&lt;/p&gt;

&lt;h2 id="bilingualcompositionaldocumentmodel"&gt; Bilingual compositional document model&lt;/h2&gt;

&lt;p&gt;Hermann and Blunsom [&lt;sup id="fnref:16"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:16" rel="footnote"&gt;16&lt;/a&gt;&lt;/sup&gt;] extend their approach (Hermann and Blunsom, 2013) to documents, by applying their composition and objective function recursively to compose sentences into documents. First, sentence representations are computed as &lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualcompositionalsentencemodel"&gt;before&lt;/a&gt;. These sentence representations are then fed into  a document-level compositional vector model, which integrates the sentence representations in the same way as can be seen in Figure 10.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/hermann_blunsom_2014.png" style="width: 80%" title="Compositional document model" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 10: A bilingual compositional document model (Hermann and Blunsom, 2014)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The advantage of this method is that weaker supervision in the form of document-level alignment can be used instead of or in conjunction with sentence-level alignment. The authors run experiments both on Europarl as well as on a newly created corpus of multilingual aligned TED talk transcriptions and find that the document signal helps considerably.&lt;/p&gt;

&lt;p&gt;In addition, they propose another composition function that -- instead of summing the representations -- applies a non-linearity to bigram pairs:&lt;/p&gt;

&lt;p&gt;\(f(x) = \sum\limits_{i=1}^n \text{tanh}(x_{i-1} + x_i)\)&lt;/p&gt;

&lt;p&gt;They find that this composition slightly outperforms addition, but underperforms it on smaller training datasets.&lt;/p&gt;

&lt;h2 id="bagofwordsautoencoderwithcorrelation"&gt; Bag-of-words autoencoder with correlation&lt;/h2&gt;

&lt;p&gt;Chandar et al. [&lt;sup id="fnref:17"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:17" rel="footnote"&gt;17&lt;/a&gt;&lt;/sup&gt;] extend the approach by Lauly et al. (2013) in two ways: Instead of using a tree-based decoder for calculating the reconstruction loss, they reconstruct a sparse binary vector of word occurrences as in Figure 11. Due to the high-dimensionality of the binary bag-of-words vector, reconstruction is slower. As they perform training using mini-batch gradient descent, where each mini-batch consists of adjacent sentences, they propose to merge the bags-of-words of the mini-batch into a single bag-of-words and to perform updates based on the merged bag-of-words. They find that this yields good performance and even outperforms the tree-based decoder.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/binary_bow_autoencoder.png" style="width: 50%" title="Compositional document model" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 11: A bilingual autoencoder with binary reconstruction error (Chandar et al., 2014)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;Secondly, they propose to add a term \(cor(a(x), a(y))\) to the objective function that encourages correlation between the representations \(a(x)\) , \(a(y)\) of the source and target language respectively by summing the scalar correlations between all dimensions of the two vectors.&lt;/p&gt;

&lt;h2 id="bilingualparagraphvectors"&gt; Bilingual paragraph vectors&lt;/h2&gt;

&lt;p&gt;Similar to the previous methods, Pham et al. [&lt;sup id="fnref:18"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:18" rel="footnote"&gt;18&lt;/a&gt;&lt;/sup&gt;] learn sentence representations as a means for learning cross-lingual word embeddings. They extend paragraph vectors (Mikolov et al. [&lt;sup id="fnref:36"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:36" rel="footnote"&gt;36&lt;/a&gt;&lt;/sup&gt;]) to the multilingual setting by forcing aligned sentences of different languages to share the same vector representation as in Figure 12 where \(sent\) is the shared sentence representation. The shared sentence representation is concatenated with the sum of the previous \(N\) words in the sentence and the model is trained to predict the next word in the sentence.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/bilingual_paragraph_vector.png" style="width: 80%" title="Compositional document model" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 12: Bilingual paragraph vectors (Pham et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The authors use a hierarchical softmax to speed-up training. As the model only learns representations for the sentences it has seen during training, at test time for an unknown sentence, the sentence representation is randomly initialised and the model is trained to predict only the words in the sentence. Only the sentence vector is updated, while the other model parameters are frozen.&lt;/p&gt;

&lt;h2 id="translationinvariantlsa"&gt; Translation-invariant LSA&lt;/h2&gt;

&lt;p&gt;Besides word embedding models such as skip-gram, matrix factorisation approaches have historically been used successfully to learn representations of words. One of the most popular methods is LSA, which Gardner et al. [&lt;sup id="fnref:19"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:19" rel="footnote"&gt;19&lt;/a&gt;&lt;/sup&gt;] extend as translation-invariant LSA to to learn cross-lingual word embeddings. They factorise a multilingual co-occurrence matrix with the restriction that it should be invariant to translation, i.e. it should stay the same if multiplied with the respective word or context dictionary.&lt;/p&gt;

&lt;h2 id="invertedindexingonwikipedia"&gt; Inverted indexing on Wikipedia&lt;/h2&gt;

&lt;p&gt;All previous approaches to learn cross-lingual representations have been based on some form of language model or matrix factorisation. In contrast, Søgaard et al. [&lt;sup id="fnref:20"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:20" rel="footnote"&gt;20&lt;/a&gt;&lt;/sup&gt;] propose an approach that does without any of these methods, but instead relies on the structure of the multilingual knowledge base Wikipedia, which they exploit by inverted indexing. Their method is based on the intuition that similar words will be used to describe the same concepts across different languages.&lt;/p&gt;

&lt;p&gt;In Wikipedia, articles in multiple languages deal with the same concept. We would typically represent every concept with the terms that are used to describe it across different languages. To learn cross-lingual word representations, we can now simply invert the index and instead represent a word by the Wikipedia concepts it is used to describe. This way, we are directly provided with cross-lingual representations of words without performing any optimisation whatsoever. As a post-processing step, we can perform dimensionality reduction on the produced word representations.&lt;/p&gt;

&lt;p&gt;While the previous methods are able to make effective use of parallel sentence and documents to learn cross-lingual word representations, they neglect the monolingual quality of the learned representations. Ultimately, we do not only want to embed languages into a shared embedding space, but also want the monolingual representations do well on the task at hand.&lt;/p&gt;

&lt;h1 id="jointoptimisation"&gt;Joint optimisation&lt;/h1&gt;

&lt;p&gt;Models that use joint optimisation aim to do exactly this: They not only consider a cross-lingual constraint, but jointly optimize mono-lingual and cross-lingual objectives.&lt;/p&gt;

&lt;p&gt;In practice, for two languages \(l_1\) and \(l_2\), these models optimize a monolingual loss \(\mathcal{M}\) for each language and one or multiple terms \(\Omega\) that regularize the transfer from language \(l_1\) to \(l_2\) (and vice versa):&lt;/p&gt;

&lt;p&gt;\(\mathcal{M}_{l_1} + \mathcal{M}_{l_2} + \lambda (\Omega_{l_1 \rightarrow l_2} + \Omega_{l_2 \rightarrow l_1}) \)&lt;/p&gt;

&lt;p&gt;where \(\lambda\) is an interpolation parameter that adjusts the impact of the cross-lingual regularization.&lt;/p&gt;

&lt;h2 id="multitasklanguagemodel"&gt;Multi-task language model&lt;/h2&gt;

&lt;p&gt;The first jointly optimised model for learning cross-lingual representations was created by Klementiev et al. [&lt;sup id="fnref:23"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:23" rel="footnote"&gt;23&lt;/a&gt;&lt;/sup&gt;]. They train a neural language model for each language and jointly optimise the monolingual maximum likelihood objective of each language model with a word-alignment based MT regularization term as the cross-lingual objective. The monolingual objective is thus to maximise the probability of the current word \(w_t\) given its \(n\) surrounding words:&lt;/p&gt;

&lt;p&gt;\(\mathcal{M} = \text{log} \: P(w_t^ \: | \: w_{t-n+1:t-1}) \).&lt;/p&gt;

&lt;p&gt;This is optimised using the classic language model of Bengio et al. [&lt;sup id="fnref:37"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:37" rel="footnote"&gt;37&lt;/a&gt;&lt;/sup&gt;]. The cross-lingual regularisation term in turn encourages the representations of words that are often aligned to each other to be similar:&lt;/p&gt;

&lt;p&gt;\(\Omega = \dfrac{1}{2} c^T (A \otimes I) c\)&lt;/p&gt;

&lt;p&gt;where \(A\) is the matrix capturing alignment scores, \(I\) is the identity matrix, \(\otimes\) is the Kronecker product, and \(c\) is the representation of word \(w_t\).&lt;/p&gt;

&lt;h2 id="bilingualmatrixfactorisation"&gt; Bilingual matrix factorisation&lt;/h2&gt;

&lt;p&gt;Zou et al. [&lt;sup id="fnref:14"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:14" rel="footnote"&gt;14&lt;/a&gt;&lt;/sup&gt;] use a matrix factorisation approach in the spirit of GloVe (Pennington et al. [&lt;sup id="fnref:38"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:38" rel="footnote"&gt;38&lt;/a&gt;&lt;/sup&gt;]) to learn cross-lingual word representations for English and Chinese. They create two alignment matrices \(A_{en \rightarrow zh}\) and \(A_{zh \rightarrow en}\) using alignment counts automatically learned from the Chinese Gigaword corpus. In \(A_{en \rightarrow zh}\), each element \(a_{ij}\) contains the number of times the \(i\)-th Chinese word was aligned with the \(j\)-th English word, with each row normalised to sum to \(1\). &lt;br&gt;
Intuitively, if a word in the source language is only aligned with one word in the target language, then those words should have the same representation. If the target word is aligned with more than one source word, then its representation should be a combination of the representations of its aligned words. Consequently, the authors represent the embeddings in the target language as the product of the source embeddings \(V_{en}\) and their corresponding alignment counts \(A_{en \rightarrow zh}\). They then minimise the squared difference between these two terms:&lt;/p&gt;

&lt;p&gt;\(\Omega_{en\rightarrow zh} = || V_{zh} - A_{en \rightarrow zh} V_{en}||^2 \)&lt;/p&gt;

&lt;p&gt;\(\Omega_{zh\rightarrow en} = || V_{en} - A_{zh \rightarrow en} V_{zh}||^2 \)&lt;/p&gt;

&lt;p&gt;where \(V_{en}\) and \(V_{zh}\) are the embedding matrices of the English and Chinese word embeddings respectively.&lt;/p&gt;

&lt;p&gt;They employ the max-margin hinge loss objective by Collobert et al. (2008) as monolingual objective \(\mathcal{M}\) and train the English and Chinese word embeddings to minimise the corresponding objective above together with a monolingual objective. For instance, for English, the training objective is:&lt;/p&gt;

&lt;p&gt;\(\mathcal{M}_{en} + \lambda \Omega_{zh\rightarrow en} \).&lt;/p&gt;

&lt;p&gt;It is interesting to observe that the authors learn embeddings using a curriculum, training different frequency bands of the vocabulary at a time. The entire training process takes 19 days.&lt;/p&gt;

&lt;h2 id="bilingualskipgram"&gt;Bilingual skip-gram&lt;/h2&gt;

&lt;p&gt;Luong et al. [&lt;sup id="fnref:24"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:24" rel="footnote"&gt;24&lt;/a&gt;&lt;/sup&gt;] in turn extend skip-gram to the cross-lingual setting and use the skip-gram objectives as monolingual and cross-lingual objectives. Rather than just predicting the surrounding words in the source language, they use the words in the source language to additionally predict their aligned words in the target language as in Figure 13.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/bilingual_skipgram.png" style="width: 80%" title="Bilingual skip-gram" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 13: Bilingual skip-gram (Luong et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;For this, they require word alignment information. They propose two ways to predict aligned words: For their first method, they automatically learn alignment information; if a word is unaligned, the alignments of its neighbours are used for prediction. In their second method, they assume that words in the source and target sentence are monotonically aligned, with each source word at position \(i\) being aligned to the target word at position \(i \cdot T/S\) where \(S\) and \(T\) are the source and target sentence lengths. They find that a simple monotonic alignment is comparable to the unsupervisedly learned alignment in performance.&lt;/p&gt;

&lt;h2 id="bilingualbagofwordswithoutwordalignments"&gt;Bilingual bag-of-words without word alignments&lt;/h2&gt;

&lt;p&gt;Gouws et al. [&lt;sup id="fnref:25"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:25" rel="footnote"&gt;25&lt;/a&gt;&lt;/sup&gt;] propose a Bilingual Bag-of-Words without Word Alignments (BilBOWA) that leverages additional monolingual data. They use the skip-gram objective as a monolingual objective and a novel sampled \(l_2\) loss as cross-lingual regularizer as in Figure 14.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/bilbowa.png" style="width: 80%" title="The BilBOWA model" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 14: The BilBOWA model (Gouws et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;More precisely, instead of relying on expensive word alignments, they simply assume that each word in a source sentence is aligned with &lt;em&gt;every&lt;/em&gt; word in the target sentence under a uniform alignment model. Thus, instead of minimising the distance between words that were aligned to each other, they minimise the distance between the means of the word representations in the aligned sentences, which is shown in Figure 15, where \(s^e\) and \(s^f\) are the sentences in source and target language respectively. &lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/approxmating_alignments_bilbowa.png" style="width: 80%" title="Approximating word alignments" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 15: Approximating word alignments with uniform alignments (Gouws et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;The cross-lingual objective in the BilBOWA model is thus:&lt;/p&gt;

&lt;p&gt;\(\Omega = \|\dfrac{1}{m} \sum\limits_{w_i \in s^{l_1}}^m r_i^{l_1} - \dfrac{1}{n} \sum\limits_{w_j \in s^{l_2}}^n r_j^{l_2} &lt;br&gt;
\|^2 \)&lt;/p&gt;

&lt;p&gt;where \(r_i\) and \(r_j\) are the word embeddings of word \(w_i\) and \(w_j\) in each sentence \(s^{l_1}\) and \(s^{l_2}\) of length \(m\) and \(n\) in languages \(l_1\) and \(l_2\) respectively.&lt;/p&gt;

&lt;h2 id="bilingualskipgramwithoutwordalignments"&gt;Bilingual skip-gram without word alignments&lt;/h2&gt;

&lt;p&gt;Another extension of skip-gram to learning cross-lingual representations is proposed by Coulmance et al. [&lt;sup id="fnref:26"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:26" rel="footnote"&gt;26&lt;/a&gt;&lt;/sup&gt;]. They also use the regular skip-gram objective as monolingual objective. For the cross-lingual objective, they make a similar assumption as Gouws et al. (2015) by supposing that every word in the source sentence is uniformly aligned to every word in the target sentence.&lt;/p&gt;

&lt;p&gt;Under the skip-gram formulation, they treat every word in the target sentence as context of every word in the source sentence and thus train their model to predict all words in the target sentence with the following skip-gram objective:&lt;/p&gt;

&lt;p&gt;\(\Omega_{e,f} = \sum\limits_{(s_{l_1}, s_{l_2}) \in C_{l_1, l_2}} \sum\limits_{w_{l_1} \in s_{l_1}} \sum\limits_{c_{l_2} \in s_{l_2}} - \text{log} \: \sigma(w_{l_1}, c_{l_2}) \)&lt;/p&gt;

&lt;p&gt;where \(s\) is the sentence in the respective language, \(C\) is the sentence-aligned corpus, \(w\) are word and \(c\) are context representations respectively, and \( - \text{log} \: \sigma(\centerdot)\) is the standard skip-gram loss function.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/transgram.png" style="width: 80%" title="Trans-gram" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 16: The Trans-gram model (Coulmance et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;As the cross-lingual objective is asymmetric, they use one cross-lingual objective for the source-to-target and another one for the target-to-source direction. The complete Trans-gram objective including two monolingual and two cross-lingual skip-gram objectives is displayed in Figure 16.&lt;/p&gt;

&lt;h2 id="jointmatrixfactorisation"&gt;Joint matrix factorisation&lt;/h2&gt;

&lt;p&gt;Shi et al. [&lt;sup id="fnref:27"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:27" rel="footnote"&gt;27&lt;/a&gt;&lt;/sup&gt;] use a joint matrix factorisation model to learn cross-lingual representations. In contrast to Zou et al. (2013), they also take into account additional monolingual data. Similar to the former, they also use the GloVe objective (Pennington et al., 2014) as monolingual objective:&lt;/p&gt;

&lt;p&gt;\(\mathcal{M}_{l_i} = \sum\limits_{j,k} f(X_{jk}^{l_i})(w_j^{l_i} \cdot c_k^{l_i} + b_{w_j}^{l_i} + b_{c_k}^{l_i} + b^{l_i} - M_{jk}^{l_{i}}) \)&lt;/p&gt;

&lt;p&gt;where \(w_j^{l_i}\) and \(c_k^{l_i}\) are the embeddings and \(M_{jk}^{l_{i}}\) the PMI value of a word-context pair \((j,k) \) in language \(l_{i}\), while \( b_{w_j}^{l_i}\) and \(b_{c_k}^{l_i}\) and \(b^{l_i}\) are the word-specific and language-specific bias terms respectively.&lt;/p&gt;

&lt;figure&gt;  
      &lt;img src="http://ruder.io/content/images/2016/11/crosslingual_matrix_factorisation.png" style="width: 50%" title="Cross-lingual matrix factorisation" alt="A survey of cross-lingual embedding models"&gt;
&lt;figcaption&gt;Figure 17: Learning cross-lingual word representations via matrix factorisation (Shi et al., 2015)&lt;/figcaption&gt;  
&lt;/figure&gt;

&lt;p&gt;They then place cross-lingual constraints on the monolingual representations as can be seen in Figure 17. The authors propose two cross-lingual regularisation objectives: The first one is based on calculating cross-lingual co-occurrence counts. These co-occurrences can be calculated without alignment information using a uniform alignment model as in Gouws et al. (2015). Alternatively, co-occurrence counts can also be calculated by leveraging automatically learned word alignments. The co-occurrence counts are then stored in a matrix \(X^{\text{bi}}\) where every entry \(X_{jk}^{\text{bi}}\) contains the number of times the source word \(j\) occurred with the target word \(k\) in an aligned sentence pair in the parallel corpus. &lt;br&gt;
For optimisation, a PMI matrix \(M^{\text{bi}}_{jk}\) can be calculated based on the co-occurrence counts in \(X^{\text{bi}}\). This matrix can again be factorised as in the GloVe objective, where now the context word representation \(c_k^{l_i}\) is replaced with the representation of the word in the target language \(w_k^{l_2}\):&lt;/p&gt;

&lt;p&gt;\(\Omega = \sum\limits_{j \in V^{l_1}, k \in V^{l_2}} f(X_{jk}^{l_1})(w_j^{l_1} \cdot w_k^{l_2} + b_{w_j}^{l_1} + b_{w_k}^{l_2} + b^{\text{bi}} - M_{jk}^{\text{bi}}) \).&lt;/p&gt;

&lt;p&gt;The second cross-lingual regularisation term they propose leverages the translation probabilities produced by a machine translation system and involves minimising the distances of the representations of related words in the two languages weighted by their similarities:&lt;/p&gt;

&lt;p&gt;\(\Omega = \sum\limits_{j \in V^{l_1}, k \in V^{l_2}} sim(j,k) \cdot ||w_j^{l_1} - w_k^{l_2}||^2\)&lt;/p&gt;

&lt;p&gt;where \(j\) and \(k\) are words in the source and target language respectively and \(sim(j,k)\) is their translation probability.&lt;/p&gt;

&lt;h2 id="bilingualsparserepresentations"&gt; Bilingual sparse representations&lt;/h2&gt;

&lt;p&gt;Vyas and Carpuat [&lt;sup id="fnref:21"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:21" rel="footnote"&gt;21&lt;/a&gt;&lt;/sup&gt;] propose another method based on matrix factorisation that -- in contrast to previous approaches -- allows learning sparse cross-lingual representations. They first independently train two monolingual word representations \(X_e\) and \(X_f\) in two different languages using GloVe (Pennington et al., 2014) on two large monolingual corpora.&lt;/p&gt;

&lt;p&gt;They then learn monolingual sparse representations from these dense representations by decomposing \(X\) into two matrices \(A\) and \(D\) such that the \(l_2\) reconstruction error is minimised, with an additional constraint on \(A\) for sparsity:&lt;/p&gt;

&lt;p&gt;\(\mathcal{M}_{l_i} = \sum\limits_{i=1}^{v_{l_i}} \|A_{l_ii}D_{l_i}^T - X_{l_ii}\| + \lambda_{l_i} \|A_{l_ii}\|_1 \)&lt;/p&gt;

&lt;p&gt;where \(v_{l_i}\) is the number of dense word representations in language \(l_i\).&lt;/p&gt;

&lt;p&gt;The above equation, however, only creates sparse monolingual embeddings. To learn bilingual embeddings, they add another constraint based on automatically learned word alignment that minimises the \(l_2\) reconstruction error between words that were strongly aligned to each other: &lt;/p&gt;

&lt;p&gt;\(\Omega = \sum\limits_{i=1}^{v_{l_1}} \sum\limits_{j=1}^{v_{l_2}} \dfrac{1}{2} \lambda_x S_{ij} \|A_{l_1i} - A_{l_2j}\|_2^2 \)&lt;/p&gt;

&lt;p&gt;where \(S\) is the alignment matrix where each entry \(S_{ij}\) contains the alignment score of source word \(X_{l_1i}\) with target word \(X_{l_2j}\).&lt;/p&gt;

&lt;p&gt;The complete objective function is thus the following:&lt;/p&gt;

&lt;p&gt;\(\mathcal{M}_{l_1} + \mathcal{M}_{l_2} + \Omega\).&lt;/p&gt;

&lt;h2 id="bilingualparagraphvectorswithoutparalleldata"&gt; Bilingual paragraph vectors (without parallel data)&lt;/h2&gt;

&lt;p&gt;Mogadala and Rettinger [&lt;sup id="fnref:22"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:22" rel="footnote"&gt;22&lt;/a&gt;&lt;/sup&gt;] use an approach similar to Pham et al. (2015), but extend it to also work without parallel data. They use the paragraph vectors objective as monolingual objective \(\mathcal{M}\). They jointly optimise this objective together with a cross-lingual regularization function \(\Omega\) that encourages the representations of words in languages \(l_1\) and \(l_2\) to be close to each other.&lt;/p&gt;

&lt;p&gt;Their main innovation is that the cross-lingual regularizer \(\Omega\) is adjusted based on the nature of the training corpus. In addition to regularising the mean of word vectors in a sentence to be close to the mean of word vectors in the aligned sentence similar to Gouws et al. (2015) (the second term in the below equation), they also regularise the paragraph vectors \(SP^{l_1}\) and \(SP^{l_2}\) of aligned sentences in languages \(l_1\) and \(l_2\) to be close to each other. The complete cross-lingual objective then uses elastic net regularization to combine both terms:&lt;/p&gt;

&lt;p&gt;\(\Omega = \alpha ||SP^{l_1}_j - SP^{l_2}_j||^2 + (1-\alpha) \dfrac{1}{m} \sum\limits_{w_i \in s_j^{l_1}}^m W_i^{l_1} - \dfrac{1}{n} \sum\limits_{w_k \in s_j^{l_2}}^n W_k^{l_2} \)&lt;/p&gt;

&lt;p&gt;where \(W_i^{l_1}\) and \(W_k^{l_2}\) are the word embeddings of word \(w_i\) and \(w_k\) in each sentence \(s_j\) of length \(m\) and \(n\) in languages \(l_1\) and \(l_2\) respectively. &lt;/p&gt;

&lt;p&gt;To leverage data that is not sentence-aligned, but where an alignment is still present on the document level, they propose a two-step approach: They use &lt;a href="https://en.wikipedia.org/wiki/Procrustes_analysis"&gt;Procrustes analysis&lt;/a&gt;, a method for statistical shape analysis, to find for each document in language \(l_1\) the most similar document in language \(l_2\). This is done by first learning monolingual representations of the documents in each language using paragraph vectors on each corpus. Subsequently, Procrustes analysis aims to learn a transformation between the two vector spaces by translating, rotating, and scaling the embeddings in the first space until they most closely align to the document representations in the second space. &lt;br&gt;
In the second step, they then simply use the previously described method to learn cross-lingual word representations from the alignment documents, this time treating the entire documents as paragraphs.&lt;/p&gt;

&lt;h1 id="incorporatingvisualinformation"&gt;Incorporating visual information&lt;/h1&gt;

&lt;p&gt;A recent branch of research proposes to incorporate visual information to improve the performance of monolingual [&lt;sup id="fnref:29"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:29" rel="footnote"&gt;29&lt;/a&gt;&lt;/sup&gt;] or cross-lingual [&lt;sup id="fnref:30"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:30" rel="footnote"&gt;30&lt;/a&gt;&lt;/sup&gt;] representations. These methods show good performance on comparison tasks. They additionally demonstrate application for zero-shot learning and might thus ultimately be helpful in learning cross-lingual representations without (linguistic) parallel data.&lt;/p&gt;

&lt;h1 id="challenges"&gt;Challenges&lt;/h1&gt;

&lt;h2 id="functionalmodeling"&gt;Functional modeling&lt;/h2&gt;

&lt;p&gt;Models for learning cross-linguistic representations share weaknesses with other vector space models of language: While they are very good at modelling the conceptual aspect of meaning evaluated in word similarity tasks, they fail to properly model the functional aspect of meaning, e.g. to distinguish whether one remarks "Give me &lt;em&gt;a&lt;/em&gt; pencil" or "Give me &lt;em&gt;that&lt;/em&gt; pencil".&lt;/p&gt;

&lt;h2 id="wordorder"&gt;Word order&lt;/h2&gt;

&lt;p&gt;Secondly, due to the reliance on bag-of-words representations, current models for learning cross-lingual word embeddings completely ignore word order. Models that are oblivious to word order, for instance, assign to the following sentence pair (Landauer &amp;amp; Dumais [&lt;sup id="fnref:39"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:39" rel="footnote"&gt;39&lt;/a&gt;&lt;/sup&gt;]) the exact same representation as they contain the same set of words, even though they are completely different in meaning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;"That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious."&lt;/li&gt;
&lt;li&gt;"It was not the sales manager, who hit the bottle that day, but the office worker with a serious drinking problem".&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="compositionality"&gt; Compositionality&lt;/h2&gt;

&lt;p&gt;Most approaches for learning cross-lingual representations focus on word representations. These approaches are not able to easily compose word representations to form representations of sentences and documents. Even approaches that learn jointly learn word and sentence representations do so by via simple summation of words in the sentence. In the future, it will be interesting to see if LSTMs or CNNs that can form more composable sentence representations can be applied efficiently to learn cross-lingual representations.&lt;/p&gt;

&lt;h2 id="polysemy"&gt;Polysemy&lt;/h2&gt;

&lt;p&gt;While conflating multiple senses of a word is already problematic for learning mono-lingual word representations, this issue is amplified in a cross-lingual embedding space: Monosemous words in one language might align with polysemous words in another language and thus fail to capture the entirety of the cross-lingual relations. There has already been promising work on learning monolingual multi-sense embeddings. We hypothesize that learning cross-lingual multi-sense embeddings will become increasingly relevant, as it enables us to capture more fine-grained cross-lingual meaning.&lt;/p&gt;

&lt;h2 id="feasibility"&gt;Feasibility&lt;/h2&gt;

&lt;p&gt;The final challenge pertains to the feasibility of the venture of learning cross-lingual embeddings itself: Languages are incredibly complex, human artefacts. Learning a monolingual embedding space is already difficult; sharing such a vector space between two languages and expecting that inter-language and intra-language relations are reliably reflected then seems utopian. &lt;br&gt;
Additionally, some languages show linguistic features, which other languages lack. The ease of constructing a shared embedding space between languages and consequently the success of cross-lingual transfer is intuitively proportional to the similarity of the languages: An embedding space shared between Spanish and Portuguese tends to capture more linguistic nuances of meaning than an embedding space populated with English and Chinese representations. Furthermore, if two languages are too dissimilar, cross-linguistic transfer might not be possible at all -- similar to the negative transfer that occurs in domain adaptation between very dissimilar domains.&lt;/p&gt;

&lt;h1 id="evaluation"&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;Having surveyed models to learn cross-lingual word representations, we would now like to know which is the best method to use for the task we care about. Cross-lingual representation models have been evaluated on a wide range of tasks such as cross-lingual document classification (CLDC), Machine Translation (MT), word similarity, as well as cross-lingual variations of the following tasks: named entity recognition, part-of-speech tagging, super sense tagging, dependency parsing, and dictionary induction. &lt;br&gt;
In the context of the CLDC evaluation setup by Klementiev et al. (2012) \(40\)-dimensional cross-lingual word embeddings are learned to classify documents in one language and evaluated on the documents of another language. As CLDC is among the most widely used, we show below exemplarily the evaluation table of Mogadala and Rettinger (2016) for this task:&lt;/p&gt;

&lt;style type="text/css"&gt;  
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;  

&lt;table class="tg"&gt;  
  &lt;tr&gt;
    &lt;th class="tg-9hbo"&gt;Method&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;en -&amp;gt; de&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;de -&amp;gt; en&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;en -&amp;gt; fr&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;fr -&amp;gt; en&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;en -&amp;gt; es&lt;/th&gt;
    &lt;th class="tg-9hbo"&gt;es -&amp;gt; en&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Majority class&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;46.8&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;46.8&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;22.5&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;25.0&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;15.3&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;22.2&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;MT&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;68.1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;67.4&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;76.3&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;71.1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;52.0&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;58.4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#multitasklanguagemodel"&gt;Multi-task language model (Klementiev et al., 2012)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;77.6&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;71.1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;74.5&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;61.9&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;31.3&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;63.0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bagofwordsautoencoderwithcorrelation"&gt;Bag-of-words autoencoder with correlation (Chandar et al., 2014)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;91.8&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;74.2&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;84.6&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;74.2&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;49.0&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;64.4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualcompositionaldocumentmodel"&gt;Bilingual compositional document model (Hermann and Blunsom, 2014)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;86.4&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;74.7&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#distributedwordalignment"&gt;Distributed word alignment (Kočiský et al., 2014)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;83.1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;75.4&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualbagofwordswithoutwordalignments"&gt;Bilingual bag-of-words without word alignments (Gouws et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;86.5&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;75.0&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualskipgram"&gt;Bilingual skip-gram (Luong et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;87.6&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;77.8&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualskipgramwithoutwordalignments"&gt;Bilingual skip-gram without word alignments (Coulmance et al., 2015)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;87.8&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;78.7&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#bilingualparagraphvectorswithoutparalleldata"&gt;Bilingual paragraph vectors (without parallel data) (Mogadala and Rettinger, 2016)&lt;/a&gt;&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;88.1&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;78.9&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;79.2&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;77.8&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;56.9&lt;/td&gt;
    &lt;td class="tg-9hbo"&gt;67.6&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;These results, however, should not be considered as representative of the general performance of cross-lingual embedding models as different methods tend to well on different tasks depending on the type of approach and the type of data used. &lt;br&gt;
Upadhyay et al. [&lt;sup id="fnref:31"&gt;&lt;a href="http://ruder.io/cross-lingual-embeddings/#fn:31" rel="footnote"&gt;31&lt;/a&gt;&lt;/sup&gt;] evaluate cross-lingual embedding models that require different forms of supervision on various tasks. They find that on word similarity datasets, models that require cheaper forms of supervision (sentence-aligned and document-aligned data) are almost as good as models with more expensive supervision in the form of word alignments. For cross-lingual classification and dictionary induction, more informative supervision is better. Finally, for parsing, models with word-level alignment are able to capture syntax more accurately and thus perform better overall.&lt;/p&gt;

&lt;p&gt;The findings by Upadhyay et al. are further proof for the intuition that the choice of the data is important. Levy et al. (2016) go even further than this in comparing models for learning cross-lingual word representations to traditional alignment models on dictionary induction and word alignment tasks. They argue that whether or not an algorithm uses a particular feature set is more important than the choice of the algorithm. In their experiments, using sentence ids, i.e. creating a sentence's language-independent representation (for instance with doc2vec) achieves better results than just using the source and target words.&lt;/p&gt;

&lt;p&gt;Finally, to facilitate evaluation of cross-lingual word embeddings, Ammar et al. (2016) make a &lt;a href="http://128.2.220.95/multilingual"&gt;website&lt;/a&gt; available where learned representations can be uploaded and automatically evaluated on a wide range of tasks.&lt;/p&gt;

&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Models that allow us to learn cross-lingual representations have already been useful in a variety of tasks such as Machine Translation (decoding and evaluation), automated bilingual dictionary generation, cross-lingual information retrieval, parallel corpus extraction and generation, as well as cross-language plagiarism detection. It will be interesting to see what further progress the future will bring.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let me know your thoughts about this post and about any errors you found in the comments below.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id="printableversionandcitation"&gt;Printable version and citation&lt;/h1&gt;

&lt;p&gt;This blog post is also available as an &lt;a href="https://arxiv.org/abs/1706.04902"&gt;article on arXiv&lt;/a&gt;, in case you want to refer to it later.&lt;/p&gt;

&lt;p&gt;In case you found it helpful, consider citing the corresponding arXiv article as: &lt;br&gt;
&lt;em&gt;Sebastian Ruder (2017). A survey of cross-lingual embedding models. arXiv preprint     arXiv:1706.04902.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id="otherblogpostsonwordembeddings"&gt;Other blog posts on word embeddings&lt;/h1&gt;

&lt;p&gt;If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;On word embeddings - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;On word embeddings - Part 2: Approximating the softmax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/secret-word2vec/index.html"&gt;On word embeddings - Part 3: The secret ingredients of word2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/index.html"&gt;Unofficial Part 5: Word embeddings in 2017 -  Trends and future directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Levy, O., Søgaard, A., &amp;amp; Goldberg, Y. (2016). Reconsidering Cross-lingual Word Embeddings. arXiv Preprint arXiv:1608.05426. Retrieved from &lt;a href="http://arxiv.org/abs/1608.05426"&gt;http://arxiv.org/abs/1608.05426&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Mikolov, T., Le, Q. V., &amp;amp; Sutskever, I. (2013). Exploiting Similarities among Languages for Machine Translation. Retrieved from &lt;a href="http://arxiv.org/abs/1309.4168"&gt;http://arxiv.org/abs/1309.4168&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Faruqui, M., &amp;amp; Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 462 – 471. Retrieved from &lt;a href="http://repository.cmu.edu/lti/31"&gt;http://repository.cmu.edu/lti/31&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Guo, J., Che, W., Yarowsky, D., Wang, H., &amp;amp; Liu, T. (2015). Cross-lingual Dependency Parsing Based on Distributed Representations. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1234–1244. Retrieved from &lt;a href="http://www.aclweb.org/anthology/P15-1119"&gt;http://www.aclweb.org/anthology/P15-1119&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Ammar, W., Mulcaire, G., Tsvetkov, Y., Lample, G., Dyer, C., &amp;amp; Smith, N. A. (2016). Massively Multilingual Word Embeddings. Retrieved from &lt;a href="http://arxiv.org/abs/1602.01925"&gt;http://arxiv.org/abs/1602.01925&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Vulic, I., &amp;amp; Korhonen, A. (2016). On the Role of Seed Lexicons in Learning Bilingual Word Embeddings. Proceedings of ACL, 247–257. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:7"&gt;&lt;p&gt;Barone, A. V. M. (2016). Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders. Proceedings of the 1st Workshop on Representation Learning for NLP, 121–126. Retrieved from &lt;a href="http://arxiv.org/pdf/1608.02996.pdf"&gt;http://arxiv.org/pdf/1608.02996.pdf&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:7" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:8"&gt;&lt;p&gt;Xiao, M., &amp;amp; Guo, Y. (2014). Distributed Word Representation Learning for Cross-Lingual Dependency Parsing. CoNLL. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:8" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:9"&gt;&lt;p&gt;Gouws, S., &amp;amp; Søgaard, A. (2015). Simple task-specific bilingual word embeddings. NAACL, 1302–1306. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:9" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:10"&gt;&lt;p&gt;Duong, L., Kanayama, H., Ma, T., Bird, S., &amp;amp; Cohn, T. (2016). Learning Crosslingual Word Embeddings without Bilingual Corpora. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16). &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:10" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:11"&gt;&lt;p&gt;Vulic, I., &amp;amp; Moens, M.-F. (2016). Bilingual Distributed Word Representations from Document-Aligned Comparable Data. Journal of Artificial Intelligence Research, 55, 953–994. Retrieved from &lt;a href="http://arxiv.org/abs/1509.07308"&gt;http://arxiv.org/abs/1509.07308&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:11" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:12"&gt;&lt;p&gt;Hermann, K. M., &amp;amp; Blunsom, P. (2013). Multilingual Distributed Representations without Word Alignment. arXiv Preprint arXiv:1312.6173. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:12" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:13"&gt;&lt;p&gt;Lauly, S., Boulanger, A., &amp;amp; Larochelle, H. (2013). Learning Multilingual Word Representations using a Bag-of-Words Autoencoder. NIPS WS on Deep Learning, 1–8. Retrieved from &lt;a href="http://arxiv.org/abs/1401.1803"&gt;http://arxiv.org/abs/1401.1803&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:13" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:14"&gt;&lt;p&gt;Zou, W. Y., Socher, R., Cer, D., &amp;amp; Manning, C. D. (2013). Bilingual Word Embeddings for Phrase-Based Machine Translation. EMNLP. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:14" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:15"&gt;&lt;p&gt;Kočiský, T., Hermann, K. M., &amp;amp; Blunsom, P. (2014). Learning Bilingual Word Representations by Marginalizing Alignments. Retrieved from &lt;a href="http://arxiv.org/abs/1405.0947"&gt;http://arxiv.org/abs/1405.0947&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:15" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:16"&gt;&lt;p&gt;Hermann, K. M., &amp;amp; Blunsom, P. (2014). Multilingual Models for Compositional Distributed Semantics. Acl, 58–68. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:16" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:17"&gt;&lt;p&gt;Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V., &amp;amp; Saha, A. (2014). An Autoencoder Approach to Learning Bilingual Word Representations. Advances in Neural Information Processing Systems. Retrieved from &lt;a href="http://arxiv.org/abs/1402.1454"&gt;http://arxiv.org/abs/1402.1454&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:17" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:18"&gt;&lt;p&gt;Pham, H., Luong, M.-T., &amp;amp; Manning, C. D. (2015). Learning Distributed Representations for Multilingual Text Sequences. Workshop on Vector Modeling for NLP, 88–94. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:18" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:19"&gt;&lt;p&gt;Gardner, M., Huang, K., Paplexakis, E., Fu, X., Talukdar, P., Faloutsos, C., … Sidiropoulos, N. (2015). Translation Invariant Word Embeddings. EMNLP. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:19" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:20"&gt;&lt;p&gt;Søgaard, A., Agic, Z., Alonso, H. M., Plank, B., Bohnet, B., &amp;amp; Johannsen, A. (2015). Inverted indexing for cross-lingual NLP. The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2015), 1713–1722. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:20" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:21"&gt;&lt;p&gt;Vyas, Y., &amp;amp; Carpuat, M. (2016). Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. NAACL, 1187–1197. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:21" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:22"&gt;&lt;p&gt;Mogadala, A., &amp;amp; Rettinger, A. (2016). Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification. NAACL, 692–702. Retrieved from &lt;a href="http://www.aifb.kit.edu/images/b/b4/NAACL-HLT-2016-Camera-Ready.pdf"&gt;http://www.aifb.kit.edu/images/b/b4/NAACL-HLT-2016-Camera-Ready.pdf&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:22" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:23"&gt;&lt;p&gt;Klementiev, A., Titov, I., &amp;amp; Bhattarai, B. (2012). Inducing Crosslingual Distributed Representations of Words. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:23" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:24"&gt;&lt;p&gt;Luong, M.-T., Pham, H., &amp;amp; Manning, C. D. (2015). Bilingual Word Representations with Monolingual Quality in Mind. Workshop on Vector Modeling for NLP, 151–159. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:24" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:25"&gt;&lt;p&gt;Gouws, S., Bengio, Y., &amp;amp; Corrado, G. (2015). BilBOWA: Fast Bilingual Distributed Representations without Word Alignments. Proceedings of The 32nd International Conference on Machine Learning, 748–756. Retrieved from &lt;a href="http://jmlr.org/proceedings/papers/v37/gouws15.html"&gt;http://jmlr.org/proceedings/papers/v37/gouws15.html&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:25" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:26"&gt;&lt;p&gt;Coulmance, J., Marty, J.-M., Wenzek, G., &amp;amp; Benhalloum, A. (2015). Trans-gram, Fast Cross-lingual Word-embeddings. EMNLP 2015, (September), 1109–1113. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:26" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:27"&gt;&lt;p&gt;Shi, T., Liu, Z., Liu, Y., &amp;amp; Sun, M. (2015). Learning Cross-lingual Word Embeddings via Matrix Co-factorization. Annual Meeting of the Association for Computational Linguistics, 567–572. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:27" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:28"&gt;&lt;p&gt;Lazaridou, A., Dinu, G., &amp;amp; Baroni, M. (2015). Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 270–280. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:28" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:29"&gt;&lt;p&gt;Lazaridou, A., Nghia, T. P., &amp;amp; Baroni, M. (2015). Combining Language and Vision with a Multimodal Skip-gram Model. Proceedings of Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, Denver, Colorado, May 31 – June 5, 2015, 153–163. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:29" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:30"&gt;&lt;p&gt;Vulić, I., Kiela, D., Clark, S., &amp;amp; Moens, M.-F. (2016). Multi-Modal Representations for Improved Bilingual Lexicon Learning. ACL. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:30" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:31"&gt;&lt;p&gt;Upadhyay, S., Faruqui, M., Dyer, C., &amp;amp; Roth, D. (2016). Cross-lingual Models of Word Embeddings: An Empirical Comparison. Retrieved from &lt;a href="http://arxiv.org/abs/1604.00425"&gt;http://arxiv.org/abs/1604.00425&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:31" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:32"&gt;&lt;p&gt;Artetxe, M., Labaka, G., &amp;amp; Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289–2294. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:32" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:33"&gt;&lt;p&gt;Xing, C., Liu, C., Wang, D., &amp;amp; Lin, Y. (2015). Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015, 1005–1010. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:33" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:34"&gt;&lt;p&gt;Collobert, R., &amp;amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. &lt;a href="http://doi.org/10.1145/1390156.1390177"&gt;http://doi.org/10.1145/1390156.1390177&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:34" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:35"&gt;&lt;p&gt;Dyer, C., Victor Ch., &amp;amp; Smith, N. A. (2013). A simple, fast, and effective reparameterization of ibm model 2. Association for Computational Linguistics. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:35" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:36"&gt;&lt;p&gt;Le, Q. V., &amp;amp; Mikolov, T. (2014). Distributed Representations of Sentences and Documents. International Conference on Machine Learning - ICML 2014, 32, 1188–1196. Retrieved from &lt;a href="http://arxiv.org/abs/1405.4053"&gt;http://arxiv.org/abs/1405.4053&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:36" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:37"&gt;&lt;p&gt;Bengio, Y., Ducharme, R., Vincent, P., &amp;amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. &lt;a href="http://doi.org/10.1162/153244303322533223"&gt;http://doi.org/10.1162/153244303322533223&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:37" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:38"&gt;&lt;p&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. &lt;a href="http://doi.org/10.3115/v1/D14-1162"&gt;http://doi.org/10.3115/v1/D14-1162&lt;/a&gt; &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:38" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:39"&gt;&lt;p&gt;Landauer, T. K. &amp;amp; Dumais, S. T. (1997). A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge, Psychological Review, 104(2), 211-240. &lt;a href="http://ruder.io/cross-lingual-embeddings/#fnref:39" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Cover image courtesy of Zou et al. (2013)&lt;/p&gt;</content:encoded></item><item><title>Highlights of EMNLP 2016: Dialogue, deep learning, and more</title><description>&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href="http://blog.aylien.com/highlights-emnlp-2016-dialogue-deeplearning-and-more/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I spent the past week in Austin, Texas at &lt;a href="http://www.emnlp2016.net/"&gt;EMNLP 2016&lt;/a&gt;, the Conference on Empirical Methods in Natural Language Processing.&lt;/p&gt;

&lt;p&gt;There were a lot of papers at the conference (179 long papers, 87 short papers, and 9 TACL papers all in all)&lt;/p&gt;</description><link>http://ruder.io/emnlp-2016-highlights/</link><guid isPermaLink="false">f6de1480-bbe8-4681-b822-d06e812602a1</guid><category>natural language processing</category><category>deep learning</category><category>machine learning</category><category>word embeddings</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Mon, 14 Nov 2016 12:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2016/11/emnlp_cover_image.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2016/11/emnlp_cover_image.jpg" alt="Highlights of EMNLP 2016: Dialogue, deep learning, and more"&gt;&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href="http://blog.aylien.com/highlights-emnlp-2016-dialogue-deeplearning-and-more/"&gt;AYLIEN blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I spent the past week in Austin, Texas at &lt;a href="http://www.emnlp2016.net/"&gt;EMNLP 2016&lt;/a&gt;, the Conference on Empirical Methods in Natural Language Processing.&lt;/p&gt;

&lt;p&gt;There were a lot of papers at the conference (179 long papers, 87 short papers, and 9 TACL papers all in all) -- too many to read each single one. The entire program can be found &lt;a href="http://www.emnlp2016.net/proceedings/2016-emnlp-handbook.pdf"&gt;here&lt;/a&gt;. In the following, I will highlight some trends and papers that caught my eye:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt;: One thing that stood out was that RL seems to be slowly finding its footing in NLP, with more and more people using it to solve complex problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/emnlp-2016-highlights/(https://arxiv.org/pdf/1608.05604.pdf"&gt;Hahn and Keller&lt;/a&gt; model human reading;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1609.07317.pdf"&gt;Miao and Blunsom&lt;/a&gt; summarise sentences;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.01541.pdf"&gt;Li et al.&lt;/a&gt; generate dialogues;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.03667.pdf"&gt;He et al.&lt;/a&gt; predict reddit threads;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1609.08667.pdf"&gt;Clark and Manning&lt;/a&gt; perform coreference resolution;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1603.07954.pdf"&gt;Narasimhan et al.&lt;/a&gt; query the web for additional evidence to improve information extraction in one of the two best papers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Dialogue&lt;/strong&gt;: Dialogue was a focus of the conference with all of the three keynote speakers dealing with different aspects of dialogue: Christopher Potts talked about pragmatics and how to reason about the intentions of the conversation partner; Stefanie Tellex concentrated on how to use dialogue for human-robot collaboration; finally, Andreas Stolcke focused on the problem of addressee detection in his talk. Among the papers, a few that dealt with dialogue stood out:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1604.00562.pdf"&gt;Andreas and Klein&lt;/a&gt; model pragmatics in dialogue with neural speakers and listeners;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1603.08023.pdf"&gt;Liu et al.&lt;/a&gt; show how &lt;em&gt;not&lt;/em&gt; to evaluate your dialogue system;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1231.pdf"&gt;Ouchi and Tsuboi&lt;/a&gt; select addressees and responses in multi-party conversations;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1606.03352"&gt;Wen et al.&lt;/a&gt; study diverse architectures for dialogue modelling. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Sequence-to-sequence&lt;/strong&gt;: Seq2seq models were again front and center. It is not common for a method to have its own session two years after its introduction &lt;a href="http://arxiv.org/abs/1409.3215\nhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks"&gt;(Sutskever et al., 2014)&lt;/a&gt;. While in the past years, many papers employed seq2seq e.g. for Neural Machine Translation, some papers this year focused on improving the seq2seq framework:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.02960.pdf"&gt;Wiseman and Rush&lt;/a&gt; extend seq2seq to learn global sequence scores;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1609.08194"&gt;Yu et al.&lt;/a&gt; perform online seq2seq learning;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.07947"&gt;Kim and Rush&lt;/a&gt; extend Knowledge Distillation (&lt;a href="https://arxiv.org/pdf/1503.02531.pdf"&gt;Hinton et al., 2015&lt;/a&gt;) to seq2seq learning;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1609.09552.pdf"&gt;Kikuchi et al.&lt;/a&gt; propose a method to control the output length in seq2seq models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Semantic parsing&lt;/strong&gt;: Since seq2seq's use for dialogue modelling was popularised by &lt;a href="https://arxiv.org/pdf/1506.05869.pdf (http://arxiv.org/pdf/1506.05869.pdf)"&gt;Vinyals and Le&lt;/a&gt;, it is harder to get it to work with goal-oriented tasks that require an intermediate representation on which to act. Semantic parsing is used to convert a message into a more meaningful representation that can be used by another component of the system. As this technique is useful for sophisticated dialogue systems, it is great to see progress in this area:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/D/D16/D16-1015.pdf"&gt;Yaruz et al.&lt;/a&gt; infer answer types for semantic parsing;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.07046.pdf"&gt;Krishnamurthy et al.&lt;/a&gt; parse to probabilistic programs;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1609.09315.pdf"&gt;Kocisky et al.&lt;/a&gt; perform semantic parsing with semi-supervised sequential auto encoders.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;X-to-text (or natural language generation)&lt;/strong&gt;: While mapping from text-to-text with the seq2seq paradigm is still prevalent, EMNLP featured some cool papers on natural language generation from other inputs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/D/D16/D16-1032.pdf"&gt;Kiddon et al.&lt;/a&gt; map from a recipe name and ingredients to a recipe by ticking of items off a checklist;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aclweb.org/anthology/D/D16/D16-1126.pdf"&gt;Ghazvininejad et al.&lt;/a&gt; generate a poem based on a topic;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.03821.pdf"&gt;Monroe et al.&lt;/a&gt; map from a color to its name;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1610.06210.pdf"&gt;Koncel-Kedziorski et al.&lt;/a&gt; re-write the theme of an algebra problem (think: boring physics book to Star Wars);&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1603.07771"&gt;Lebret et al.&lt;/a&gt; generate biographical sentences from Wikipedia fact tables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Parsing&lt;/strong&gt;: Parsing and syntax are a mainstay of every NLP conference and the community seems to particularly appreciate innovative models that push the state-of-the-art in parsing: The ACL '16 outstanding paper by &lt;a href="https://arxiv.org/pdf/1603.06042.pdf"&gt;Andor et al.&lt;/a&gt; introduced a globally normalized model for parsing, while the best EMNLP ‘16 paper by &lt;a href="https://arxiv.org/pdf/1607.01432.pdf"&gt;Lee et al.&lt;/a&gt; combines a global parsing model with a local search over subtrees.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Word embeddings&lt;/strong&gt;: There were still papers on word embeddings, but it felt less overwhelming than at the past EMNLP or ACL, with most methods trying to fix a particular flaw rather than training embeddings for embeddings' sake. &lt;a href="https://arxiv.org/pdf/1608.01961.pdf"&gt;Pilevhar and Collier&lt;/a&gt; de-conflate senses in word embeddings, while &lt;a href="https://arxiv.org/pdf/1607.02789.pdf"&gt;Wieting et al.&lt;/a&gt; achieve state-of-the-art results for character-based embeddings.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sentiment analysis&lt;/strong&gt;: Sentiment analysis has been popular in recent years (as attested by the introductions of many recent papers on sentiment analysis). Sadly, many of the conference papers on sentiment analysis reduce to leveraging the latest deep neural network for the task to beat the previous state-of-the-art without providing additional insights. There are, however, some that break the mold: &lt;a href="http://www.aclweb.org/anthology/D/D16/D16-1169.pdf"&gt;Teng et al.&lt;/a&gt; find an effective way to incorporate sentiment lexicons into a neural network, while &lt;a href="https://www.aclweb.org/anthology/D/D16/D16-1173.pdf"&gt;Hu et al.&lt;/a&gt; incorporate structured knowledge into their sentiment analysis model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;: By now, it is clear to everyone: Deep Learning is here to stay. In fact, &lt;em&gt;deep learning&lt;/em&gt; and &lt;em&gt;neural networks&lt;/em&gt; claimed the two top spots of keywords that were used to describe the submitted papers. The majority of papers used at least an LSTM; using no neural network seems almost contrarian now and is something that needs to be justified. However, there are still many things that need to be improved -- which leads us to...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Uphill Battles&lt;/strong&gt;: While making incremental progress is important to secure grants and publish papers, we should not lose track of the long-term goals. In this spirit, one of the best workshops that I've attended was the &lt;em&gt;Uphill Battles in Language Processing&lt;/em&gt; workshop, which featured 12 talks and not one, but &lt;em&gt;four&lt;/em&gt; all-star panels on text understanding, natural language generation, dialogue and speech, and grounded language. Summaries of the panel discussions should be available soon at the &lt;a href="http://www.coli.uni-saarland.de/~mroth/UphillBattles/"&gt;workshop website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was my brief review of some of the trends of EMNLP 2016. I hope it was helpful.&lt;/p&gt;

&lt;p&gt;Cover image credit: &lt;a href="https://www.flickr.com/photos/148956535@N07/"&gt;Jackie Cheung&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title>On word embeddings - Part 3: The secret ingredients of word2vec</title><description>&lt;p&gt;Table of Contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#glove"&gt;GloVe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#wordembeddingsvsdistributionalsemanticsmodels"&gt;Word embeddings vs. distributional semantic models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#models"&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#hyperparameters"&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#results"&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Excuse the rather clickbait-y title. This is a blog post that I meant to write for a while. In this post, I want to highlight the factors, i.e. the secret ingredients that account for the success&lt;/p&gt;</description><link>http://ruder.io/secret-word2vec/</link><guid isPermaLink="false">782bde93-e9c5-407a-9655-0eceaab5cd7f</guid><category>word embeddings</category><category>deep learning</category><category>nlp</category><dc:creator>Sebastian Ruder</dc:creator><pubDate>Sat, 24 Sep 2016 10:48:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2016/09/merge_from_ofoct--2-.jpg" medium="image"/><content:encoded>&lt;img src="http://ruder.io/content/images/2016/09/merge_from_ofoct--2-.jpg" alt="On word embeddings - Part 3: The secret ingredients of word2vec"&gt;&lt;p&gt;Table of Contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#glove"&gt;GloVe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#wordembeddingsvsdistributionalsemanticsmodels"&gt;Word embeddings vs. distributional semantic models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#models"&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#hyperparameters"&gt;Hyperparameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#results"&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Excuse the rather clickbait-y title. This is a blog post that I meant to write for a while. In this post, I want to highlight the factors, i.e. the secret ingredients that account for the success of word2vec. &lt;br&gt;
In particular, I want to focus on the connection between word embeddings trained via neural models and those produced by traditional distributional semantics models (DSMs). By showing how these ingredients can be transferred to DSMs, I will demonstrate that distributional methods are in no way inferior to the popular word embedding methods. &lt;br&gt;
Even though this is no new insight, I feel that traditional methods are frequently overshadowed amid the deep learning craze and their relevancy consequently deserves to be mentioned more often.&lt;/p&gt;

&lt;p&gt;To this effect, the paper on which this blog post is based is &lt;em&gt;Improving Distributional Similarity with Lessons Learned from Word Embeddings&lt;/em&gt; [&lt;sup id="fnref:1"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;] by Levy et al. (2015). If you haven't read it, I recommend you to check it out.&lt;/p&gt;

&lt;p&gt;Over the course of this blog post, I will first introduce GloVe, a popular word embedding model. I will then highlight the connection between word embedding models and distributional semantic methods. Subsequently, I will introduce the four models that will be used to measure the impact of the different factors. I will then give an overview of all additional factors that play a role in learning word representations, besides the choice of the algorithm. I will finally present the results by Levy et al., their takeaways and recommendations.&lt;/p&gt;

&lt;h1 id="glove"&gt;GloVe&lt;/h1&gt;

&lt;p&gt;In a &lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;previous blog post&lt;/a&gt;, we have given an overview of popular word embedding models. One model that we have omitted so far is GloVe [&lt;sup id="fnref:2"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;].&lt;/p&gt;

&lt;p&gt;Briefly, GloVe seeks to make explicit what SGNS does implicitly: Encoding meaning as vector offsets in an embedding space -- seemingly only a serendipitous by-product of word2vec -- is the specified goal of GloVe. &lt;br&gt;
Specifically, the authors of Glove show that the ratio of the co-occurrence probabilities of two words (rather than their co-occurrence probabilities themselves) is what contains information and aim to encode this information as vector differences. &lt;br&gt;
To achieve this, they propose a weighted least squares objective \(J\) that directly aims to minimise the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:&lt;/p&gt;

&lt;p&gt;\(J = \sum\limits_{i, j=1}^V f(X_{ij}) \: (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \text{log} \: X_{ij})^2 \)&lt;/p&gt;

&lt;p&gt;where \(w_i\) and \(b_i\) are the word vector and bias respectively of word \(i\), \(\tilde{w}_j\) and \(b_j\) are the context word vector and bias respectively of word \(j\), \(X_{ij}\) is the number of times word \(i\) occurs in the context of word \(j\), and \(f\) is a weighting function that assigns relatively lower weight to rare and frequent co-occurrences.&lt;/p&gt;

&lt;p&gt;As co-occurrence counts can be directly encoded in a word-context co-occurrence matrix, GloVe takes such a matrix rather than the entire corpus as input.&lt;/p&gt;

&lt;p&gt;If you want to know more about GloVe, the best reference is likely &lt;a href="http://www.aclweb.org/anthology/D14-1162"&gt;the paper&lt;/a&gt; and the &lt;a href="http://nlp.stanford.edu/projects/glove/"&gt;accompanying website&lt;/a&gt;. Besides that, you can find some additional intuitions on GloVe and its difference to word2vec by the author of gensim &lt;a href="http://rare-technologies.com/making-sense-of-word2vec/"&gt;here&lt;/a&gt;, in &lt;a href="https://www.quora.com/How-is-GloVe-different-from-word2vec"&gt;this Quora thread&lt;/a&gt;, and in &lt;a href="https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="wordembeddingsvsdistributionalsemanticsmodels"&gt;Word embeddings vs. distributional semantics models&lt;/h1&gt;

&lt;p&gt;The reason why word embedding models, particularly word2vec and GloVe, became so popular is that they seemed to continuously and significantly outperform DSMs. Many attributed this to the neural architecture of word2vec or the fact that it predicts words, which seemed to have a natural edge over solely relying on co-occurrence counts.&lt;/p&gt;

&lt;p&gt;We can view DSMs as &lt;em&gt;count&lt;/em&gt; models as they "count" co-occurrences among words by operating on co-occurrence matrices. In contrast, neural word embedding models can be seen as &lt;em&gt;predict&lt;/em&gt; models, as they try to predict surrounding words.&lt;/p&gt;

&lt;p&gt;In 2014, Baroni et al. [&lt;sup id="fnref:3"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;] showed that predict models consistently outperform count models in almost all tasks, thus providing a clear verification for the apparent superiority of word embedding models. Is this the end? No.&lt;/p&gt;

&lt;p&gt;Already with GloVe we've seen that the differences are not as clear-cut: While GloVe is considered a predict model by Levy et al. (2015), it is clearly factorizing a word-context co-occurrence matrix, which brings it close to traditional methods such as PCA and LSA. Even more, Levy et al. [&lt;sup id="fnref:4"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;] demonstrate that word2vec implicitly factorizes a word-context PMI matrix.&lt;/p&gt;

&lt;p&gt;Consequently, while on the surface DSMs and word embedding models use different algorithms to learn word representations -- the first count, the latter predict -- fundamentally, both types of models act on the same underlying statistics of the data, i.e. the co-occurrence counts between words.&lt;/p&gt;

&lt;p&gt;Thus, the question that still remains and which we will dedicate the rest of this blog post to answering is the following: &lt;br&gt;
&lt;em&gt;Why do word embedding models still perform better than DSM with almost the same information?&lt;/em&gt;&lt;/p&gt;

&lt;h1 id="models"&gt;Models&lt;/h1&gt;

&lt;p&gt;Following Levy et al. (2015), we will isolate and identify the factors that account for the success of neural word embedding models and show how these can be transferred to traditional methods by comparing the following four models:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Positive Pointwise Mutual Information (PPMI)&lt;/strong&gt;: PMI is a common measure for the strength of association between two words. It is defined as the log ratio between the joint probability of two words \(w\) and \(c\) and the product of their marginal probabilities: \(PMI(w,c) = \text{log} \: \dfrac{P(w,c)}{P(w)\:P(c)} \). As \( PMI(w,c) = \text{log} \: 0 = - \infty \) for pairs \( (w,c) \) that were never observed, PMI is in practice often replaced with &lt;em&gt;positive&lt;/em&gt; PMI (PPMI), which replaces negative values with \(0\), yielding \(PPMI(w,c) = \text{max}(PMI(w,c),0)\).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Singular Value Decomposition (SVD):&lt;/strong&gt; SVD is one of the most popular methods for dimensionality reduction and found its into NLP originally via latent semantic analysis (LSA). SVD factories the word-context co-occurrence matrix into the product of three matrices \(U \cdot \Sigma \times V^T \) where \(U\) and \(V\) are orthonormal matrices (i.e. square matrices whose rows and columns are orthogonal unit vectors) and \(\Sigma\) is a diagonal matrix of eigenvalues in
decreasing order. In practice, SVD is often used to factorize the matrix produced by PPMI. Generally, only the top \(d\) elements of \(\Sigma\) are kept, yielding \(W^{SVD} = U_d \cdot \Sigma_d\) and \(C^{SVD} = V_d\), which are typically used as the word and context representations respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Skip-gram with Negative Sampling (SGNS)&lt;/strong&gt; aka word2vec: To learn more about the skip-gram architecture and the negative sampling refer to my previous blog posts &lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;here&lt;/a&gt; and &lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;here&lt;/a&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Global Vectors (GloVe)&lt;/strong&gt; as presented in the previous section.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="hyperparameters"&gt;Hyperparameters&lt;/h1&gt;

&lt;p&gt;We will look at the following hyper-parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#preprocessing"&gt;Pre-processing&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#dynamiccontextwindow"&gt;Dynamic context window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#subsamplingfrequentwords"&gt;Subsampling frequent words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#deletingrarewords"&gt;Deleting rare words&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#associationmetric"&gt;Association metric&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#shiftedpmi"&gt;Shifted PMI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#contextdistributionsmoothing"&gt;Context distribution smoothing&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#postprocessing"&gt;Post-processing&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#addingcontextvectors"&gt;Adding context vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#eigenvalueweighting"&gt;Eigenvalue weighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/secret-word2vec/#vectornormalisation"&gt;Vector normalisation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="preprocessing"&gt;Pre-processing&lt;/h2&gt;

&lt;p&gt;Word2vec introduces three ways of pre-processing a corpus, which can be easily applied to DSMs. &lt;/p&gt;

&lt;h3 id="dynamiccontextwindow"&gt;Dynamic context window&lt;/h3&gt;

&lt;p&gt;In DSMs traditionally, the context window is unweighted and of a constant size. Both SGNS and GloVe, however, use a scheme that assigns more weight to closer words, as closer words are generally considered to be more important to a word's meaning. Additionally, in SGNS, the window size is not fixed, but the actual window size is dynamic and sampled uniformly between \(1\) and the maximum window size during training.&lt;/p&gt;

&lt;h3 id="subsamplingfrequentwords"&gt;Subsampling frequent words&lt;/h3&gt;

&lt;p&gt;SGNS dilutes very frequent words by randomly removing words whose frequency \(f\) is higher than some threshold \(t\) with a probability \(p = 1 - \sqrt{\dfrac{t}{f}}\). As this subsampling is done &lt;em&gt;before&lt;/em&gt; actually creating the windows, the context windows used by SGNS in practice are larger than indicated by the context window size.&lt;/p&gt;

&lt;h3 id="deletingrarewords"&gt; Deleting rare words&lt;/h3&gt;

&lt;p&gt;In the pre-processing of SGNS, rare words are also deleted &lt;em&gt;before&lt;/em&gt; creating the context windows, which increases the actual size of the context windows further. Levy et al. (2015) find this not to have a significant performance impact, though.&lt;/p&gt;

&lt;h2 id="associationmetric"&gt;Association metric&lt;/h2&gt;

&lt;p&gt;PMI has been shown to be an effective metric for measuring the association between words. Since Levy and Goldberg (2014) have shown SGNS to implicitly factorize a PMI matrix, two variations stemming from this formulation can be introduced to regular PMI.&lt;/p&gt;

&lt;h3 id="shiftedpmi"&gt;Shifted PMI&lt;/h3&gt;

&lt;p&gt;In SGNS, the higher the number of negative samples \(k\), the more data is being used and the better should be the estimation of the parameters. \(k\) affects the shift of the PMI matrix that is implicitly factorized by word2vec, i.e. \(k\) k shifts the PMI values by log \(k\). &lt;br&gt;
If we transfer this to regular PMI, we obtain Shifted PPMI (SPPMI): \(SPPMI(w,c) = \text{max}(PMI(w,c) - \text{log} \: k,0)\).&lt;/p&gt;

&lt;h3 id="contextdistributionsmoothing"&gt;Context distribution smoothing&lt;/h3&gt;

&lt;p&gt;In SGNS, the negative samples are sampled according to a &lt;em&gt;smoothed&lt;/em&gt; unigram distribution, i.e. an unigram distribution raised to the power of \(\alpha\), which is empirically set to \(\dfrac{3}{4}\). This leads to frequent words being sampled relatively less often than their frequency would indicate. &lt;br&gt;
We can transfer this to PMI by equally raising the frequency of the context words \(f(c)\) to the power of \(\alpha\): &lt;br&gt;
\(PMI(w, c) = \text{log} \dfrac{p(w,c)}{p(w)p_\alpha(c)}\) where \(p_\alpha(c) = \dfrac{f(c)^\alpha}{\sum_c f(c)^\alpha}\) and \(f(x)\) is the frequency of word \(x\).&lt;/p&gt;

&lt;h2 id="postprocessing"&gt;Post-processing&lt;/h2&gt;

&lt;p&gt;Similar as in pre-processing, three methods can be used to modify the word vectors produced by an algorithm.&lt;/p&gt;

&lt;h3 id="addingcontextvectors"&gt;Adding context vectors&lt;/h3&gt;

&lt;p&gt;The authors of GloVe propose to add word vectors and context vectors to create the final output vectors, e.g. \(\vec{v}_{\text{cat}} = \vec{w}_{\text{cat}} + \vec{c}_{\text{cat}}\). This adds first-order similarity terms, i.e \(w \cdot v\). However, this method cannot be applied to PMI, as the vectors produced by PMI are sparse.&lt;/p&gt;

&lt;h3 id="eigenvalueweighting"&gt;Eigenvalue weighting&lt;/h3&gt;

&lt;p&gt;SVD produces the following matrices: \(W^{SVD} = U_d \cdot \Sigma_d \) and \(C^{SVD} = V_d\). These matrices, however, have different properties: \(C^{SVD}\) is orthonormal, while \(W^{SVD}\) is not. &lt;br&gt;
SGNS, in contrast, is more symmetric. We can thus weight the eigenvalue matrix \(\Sigma_d\) with an additional parameter \(p\), which can be tuned, to yield the following: &lt;br&gt;
\(W^{SVD} = U_d \cdot \Sigma_d^p\).&lt;/p&gt;

&lt;h3 id="vectornormalisation"&gt;Vector normalisation&lt;/h3&gt;

&lt;p&gt;Finally, we can also normalise all vectors to unit length.&lt;/p&gt;

&lt;h1 id="results"&gt;Results&lt;/h1&gt;

&lt;p&gt;Levy et al. (2015) train all models on a dump of the English wikipedia and evaluate them on the commonly used word similarity and analogy datasets. You can read more about the experimental setup and training details in their paper. We summarise the most important results and takeaways below.&lt;/p&gt;

&lt;h2 id="takeaways"&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Levy et al. find that SVD -- and not one of the word embedding algorithms -- performs best on similarity tasks, while SGNS performs best on analogy datasets. They furthermore shed light on the importance of hyperparameters compared to other choices:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Hyperparameters vs. algorithms: &lt;br&gt;
Hyperparameter settings are often more important than algorithm choice. &lt;br&gt;
No single algorithm consistently outperforms the other methods.  &lt;/li&gt;
&lt;li&gt;Hyperparameters vs. more data: &lt;br&gt;
Training on a larger corpus helps for some tasks. &lt;br&gt;
In 3 out of 6 cases, tuning hyperparameters is more beneficial.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="debunkingpriorclaims"&gt;Debunking prior claims&lt;/h2&gt;

&lt;p&gt;Equipped with these insights, we can now debunk some generally held claims: &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Are embeddings superior to distributional methods? &lt;br&gt;
With the right hyperparameters, no approach has a consistent advantage over another.  &lt;/li&gt;
&lt;li&gt;Is GloVe superior to SGNS? &lt;br&gt;
SGNS outperforms GloVe on all tasks.  &lt;/li&gt;
&lt;li&gt;Is CBOW a good word2vec configuration? &lt;br&gt;
CBOW does not outperform SGNS on any task.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id="recommendations"&gt;Recommendations&lt;/h1&gt;

&lt;p&gt;Finally -- and one of the things I like most about the paper -- we can give concrete practical recommendations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DON'T&lt;/strong&gt; use shifted PPMI with SVD.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DON'T&lt;/strong&gt; use SVD "correctly", i.e. without eigenvector weighting (performance drops 15 points compared to with eigenvalue weighting with \(p = 0.5\)).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt; use PPMI and SVD with short contexts (window size of \(2\)).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt; use many negative samples with SGNS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt; always use context distribution smoothing (raise unigram distribution to the power of \(\alpha = 0.75\)) for all methods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt; use SGNS as a baseline (robust, fast and cheap to train).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DO&lt;/strong&gt; try adding context vectors in SGNS and GloVe.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="conclusions"&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;These results run counter to what is generally assumed, namely that word embeddings are superior to traditional methods and indicate that it generally makes &lt;em&gt;no difference whatsoever&lt;/em&gt; whether you use word embeddings or distributional methods -- what matters is that you tune your hyperparameters and employ the appropriate pre-processing and post-processing steps.&lt;/p&gt;

&lt;p&gt;Recent papers from Jurafsky's group [&lt;sup id="fnref:5"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:5" rel="footnote"&gt;5&lt;/a&gt;&lt;/sup&gt;, &lt;sup id="fnref:6"&gt;&lt;a href="http://ruder.io/secret-word2vec/#fn:6" rel="footnote"&gt;6&lt;/a&gt;&lt;/sup&gt;] echo these findings and show that SVD -- not SGNS -- is often the preferred choice when you care about accurate word representations.&lt;/p&gt;

&lt;p&gt;I hope this blog post was useful in highlighting cool research that sheds light on the link between traditional distributional semantic and in-vogue embedding models. As we've seen, knowledge of distributional semantics allows us to improve upon our current methods and develop entirely new variations of existing ones. For this reason, I hope that the next time you train word embeddings, you will consider adding distributional methods to your toolbox or lean on them for inspiration.&lt;/p&gt;

&lt;p&gt;As always, feel free to ask questions and point out the mistakes I made in this blog post in the comments below.&lt;/p&gt;

&lt;h1 id="otherblogpostsonwordembeddings"&gt;Other blog posts on word embeddings&lt;/h1&gt;

&lt;p&gt;If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-1/index.html"&gt;On word embeddings - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/word-embeddings-softmax/index.html"&gt;On word embeddings - Part 2: Approximating the softmax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianruder.com/cross-lingual-embeddings/index.html"&gt;Unofficial Part 4: A survey of cross-lingual embedding models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ruder.io/word-embeddings-2017/index.html"&gt;Unofficial Part 5: Word embeddings in 2017 -  Trends and future directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="references"&gt;References&lt;/h1&gt;

&lt;div class="footnotes"&gt;&lt;ol&gt;&lt;li class="footnote" id="fn:1"&gt;&lt;p&gt;Levy, O., Goldberg, Y., &amp;amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from &lt;a href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570"&gt;https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570&lt;/a&gt; &lt;a href="http://ruder.io/secret-word2vec/#fnref:1" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:2"&gt;&lt;p&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. &lt;a href="http://doi.org/10.3115/v1/D14-1162"&gt;http://doi.org/10.3115/v1/D14-1162&lt;/a&gt; &lt;a href="http://ruder.io/secret-word2vec/#fnref:2" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:3"&gt;&lt;p&gt;Baroni, M., Dinu, G., &amp;amp; Kruszewski, G. (2014). Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. ACL, 238–247. &lt;a href="http://doi.org/10.3115/v1/P14-1023"&gt;http://doi.org/10.3115/v1/P14-1023&lt;/a&gt; &lt;a href="http://ruder.io/secret-word2vec/#fnref:3" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:4"&gt;&lt;p&gt;Levy, O., &amp;amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from &lt;a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization"&gt;http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization&lt;/a&gt; &lt;a href="http://ruder.io/secret-word2vec/#fnref:4" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:5"&gt;&lt;p&gt;Hamilton, W. L., Clark, K., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Retrieved from &lt;a href="http://arxiv.org/abs/1606.02820"&gt;http://arxiv.org/abs/1606.02820&lt;/a&gt; &lt;a href="http://ruder.io/secret-word2vec/#fnref:5" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li class="footnote" id="fn:6"&gt;&lt;p&gt;Hamilton, W. L., Leskovec, J., &amp;amp; Jurafsky, D. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. arXiv Preprint arXiv:1605.09096. &lt;a href="http://ruder.io/secret-word2vec/#fnref:6" title="return to article"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;

&lt;p&gt;Cover images are courtesy of &lt;a href="http://nlp.stanford.edu/projects/glove/"&gt;Stanford&lt;/a&gt;.&lt;/p&gt;</content:encoded></item></channel></rss>
