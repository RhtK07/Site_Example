<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a PhD student in Natural Language Processing and a researcher at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.]]></description><link>http://sebastianruder.com/</link><generator>Ghost 0.7</generator><lastBuildDate>Fri, 08 Jan 2016 12:33:19 GMT</lastBuildDate><atom:link href="http://sebastianruder.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[An overview of gradient descent algorithms]]></title><description><![CDATA[<p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentvariants">Gradient descent variants</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchgradientdescent">Batch gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#stochasticgradientdescent">Stochastic gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#mini-batchgradientdescent">Mini-batch gradient descent</a></li></ul></li>
<li>Challenges</li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#additionalconsiderations">Additional considerations</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#order of training examples">Order of training examples</a></li></ul></li>
</ul>

<p>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.</p>

<p>In</p>]]></description><link>http://sebastianruder.com/optimizing-gradient-descent/</link><guid isPermaLink="false">f991a148-274f-497e-97b9-598455211f57</guid><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Wed, 23 Dec 2015 11:10:43 GMT</pubDate><content:encoded><![CDATA[<p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentvariants">Gradient descent variants</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchgradientdescent">Batch gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#stochasticgradientdescent">Stochastic gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#mini-batchgradientdescent">Mini-batch gradient descent</a></li></ul></li>
<li>Challenges</li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#additionalconsiderations">Additional considerations</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#order of training examples">Order of training examples</a></li></ul></li>
</ul>

<p>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.</p>

<p>In this blog post, we are going to look at different variants of gradient descent and the algorithms that are commonly used to optimize them.</p>

<p>Gradient descent is a way to minimize an objective function \(J\) by updating a model's parameters \(\theta \in \mathbb{R}^d \) in direction of the gradient of the objective function \(\Delta_\theta J(\theta)\). The learning rate \(\alpha\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.</p>

<p>If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks <a href="http://cs231n.github.io/optimization-1/">here</a>.</p>

<h1 id="gradientdescentvariants">Â Gradient descent variants</h1>

<p>There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.  </p>

<h2 id="batchgradientdescent">Batch gradient descent</h2>

<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \(\theta\) for the entire training dataset:</p>

<p>\(\theta = \theta - \alpha \cdot \Delta_\theta J( \theta)\)</p>

<p>As we need to calculate the gradients for the whole dataset to perform just <em>one</em> update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model <em>online</em>, i.e. with new examples on-the-fly.</p>

<p>In code, batch gradient descent looks something like this:</p>

<pre><code class="language-python">while True:
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad</code></pre>

<p>We first compute the gradient vector <code>weights_grad</code> for the whole dataset w.r.t. our parameter vector <code>params</code>.  Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. We then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform.</p>

<h2 id="stochasticgradientdescent">Stochastic gradient descent</h2>

<p>Stochastic gradient descent (SGD) in contrast performs a parameter update for <em>each</em> training example \(x^{(i)}\) and label \(y^{(i)}\):</p>

<p>\(\theta = \theta - \alpha \cdot \Delta_\theta J( \theta; x^{(i)}; y^{(i)})\)</p>

<p>It can thus also be used to learn online, one example at a time. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in image 1 but can still lead to fast convergence.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/sgd_fluctuation.png" style="width: 50%; height: 50%" title="SGD fluctuation">
<figcaption>Image 1: SGD fluctuation (Source: <a href="https://upload.wikimedia.org/wikipedia/en/f/f3/Stogra.png">Wikipedia</a>)</figcaption>  
</figure>  

<p>Its code fragment simply adds a loop over the training examples and evaluates the gradient w.r.t. each example. Note that we shuffle the training data at every epoch as we explain in <a href="http://sebastianruder.com/optimizing-gradient-descent/#shuffling">this section</a>.</p>

<pre><code class="language-python">while True:
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad</code></pre>

<h2 id="minibatchgradientdescent">Mini-batch gradient descent</h2>

<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \(n\) training examples:</p>

<p>\(\theta = \theta - \alpha \cdot \Delta_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\)</p>

<p>This way, it a) reduces the variance of the parameter updates, which can lead to more stable convergence; and b) can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used.</p>

<p>In code, instead of iterating over examples, we now iterate over mini-batches of size 50:</p>

<pre><code class="language-python">while True:
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad</code></pre>

<h1 id="challenges">Challenges</h1>

<p>Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed.</p>

<h2 id="learningrateselection">Learning rate selection</h2>

<p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum.</p>

<p>Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's idiosyncracies.</p>

<p>Additionally, the same learning rate applies for all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p>

<h1 id="gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</h1>

<p>In the following, we will outline some algorithms that are widely used by the deep learning community to deal with the aforementioned challenges. We will not discuss algorithms that are theoretically optimal but infeasible to compute in practice, e.g. second-order methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton's method</a>.</p>

<p>Batch methods</p>

<h1 id="nesterov">Nesterov</h1>

<p>For instance the Nesterov and FISTA accelerated methods seem very interesting (but they are batch methods and might not scale as Stochastic approximation do on datasets with a very large number of samples). Adapting accelerated methods to an online setting might be possible though (see Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization by Lin Xiao).</p>

<p>see here about Nesterov: <a href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></p>

<h2 id="learningrateannealing">Learning rate annealing</h2>

<p>H. Robinds and S. Monro, âA stochastic approximation method,â Annals of Mathematical Statistics, vol. 22, pp. 400â407, 1951.</p>

<h2 id="sgdondistributedsystems">SGD on distributed systems</h2>

<p>In a distributed system: Run multiple SGD's in parallel. Merge model updates using a parameter server or an all-reduce framework like MPI. Running synchronously gives good convergence but is slow. Running async (must for large datasets), is fast but gives poor convergence. Some tricks:</p>

<p>AdaGrad : Adaptively change learning values for every parameter. <br>
Bounded-Staleness (USENIX ATC'14) : Run parallel SGD replicas asynchronously. Stall fore-runners if  updates are arriving too slowly. <br>
Delay-Tolerant (NIPS'14): Run  SGD replicas asynchronously. Let parallel SGD replicas know if their update was useful. Works surprisingly well in practice. <br>
Downpour SGD (NIPS'12): Run parallel SGD replicas asynchronously across workers and maintain a parameter server. Split models by making every machine responsible for only x% of parameters. Zero worker communication -- all workers send their updates to the server. Works rather poorly since no worker every has or trains over the whole model. Commonly used as baseline in recent papers.</p>

<p>See also <a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf">http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf</a> slide 66 for quick overview  </p>

<h2 id="momentum">Momentum</h2>

<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [1], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in image 1.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/without_momentum.gif" style="width: 90%; height: 90%" title="SGD without momentum">
<figcaption>Image 1: SGD without momentum</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/with_momentum.gif" style="width: 90%; height: 90%" title="SGD with momentum">
<figcaption>Image 2: SGD with momentum</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>Momentum [2] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as in image 2. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>

<p>\(v_t = \gamma v_{t-1} + \alpha \Delta_\theta J( \theta; x^{(i)}; y^{(i)})\)</p>

<p>\(\theta = \theta - v_t\)</p>

<p>This increases updates for dimensions whose gradients point in the same directions, while reducing updates for dimensions whose gradients change directions, consequently speeding up convergence and reducing oscillation.</p>

<h2 id="adagrad">Adagrad</h2>

<p>Adagrad [3] is an algorithm for gradient-based optimization such as SGD. It adapts the learning rate to the features, performing larger updates for infrequent and smaller updates for frequent features. Pennington et al. [4] have shown that Adagrad greatly improves the robustness of SGD, while [5] use Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.</p>

<p>Before, we performed an update for all parameters \(\theta\) at once as every parameter \(\theta_i\) used the same learning rate \(\alpha\). As Adagrad uses a different learning rate for every parameter \(\theta_i\) at every time step \(t\), we first show Adagrad's per-parameter update, which we then vectorize.</p>

<p>For brevity, we set \(g_{t, i}\) to be the gradient of the objective function w.r.t. to the parameter \(\theta_i\) at time step \(t\):</p>

<p>\(g_{t, i} = \Delta_\theta J( \theta_i; x^{(j)}; y^{(j)})\).</p>

<p>The SGD update for every parameter \(\theta_i\) at each time step \(t\) then becomes:</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \alpha \cdot g_{t, i}\).</p>

<p>In its update rule, Adagrad modifies the general learning rate \(\alpha\) at each time step \(t\) for every parameter \(\theta_i\) based on the past gradients that have been computed for \(\theta_i\):</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \frac{\alpha}{\sqrt{G_{t, ii}}} \cdot g_{t, i}\).</p>

<p>\(G_{t} \in \mathbb{R}^{d \times d} \) here is a diagonal matrix where each diagonal element \(i, i\) is the sum of the squares of the gradients w.r.t. \(\theta_i\) up to time step \(t\) <sup id="fnref:1"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:1" rel="footnote">1</a></sup>. </p>

<p>As \(G_{t}\) contains the sum of the squares of the past gradients w.r.t. to all parameters \(\theta\) along its diagonal, we can now vectorize our implementation by performing an element-wise matrix-vector multiplication \(\odot\) between \(G_{t}\) and \(g_{t}\):</p>

<p>\(\theta_{t+1} = \theta_{t} - \alpha G^{-1/2}_{t} \odot g_{t}\).</p>

<h2 id="adadelta">Adadelta</h2>

<p>Adagrad accumulates the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.</p>

<p>Adadelta [6] is an extension of Adagrad that seeks to resolve this flaw. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \(w\).</p>

<p>Instead of inefficiently storing \(w\) previous squared gradients, they define the sum of gradients recursively as a decaying average of all past squared gradients. The running average \(E[g^2]_t\) at time step \(t\) then depends (as a fraction \(\gamma \) similarly to the Momentum term) only on the previous average and the current gradient:</p>

<p>\(E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t \).</p>

<p>We now simply replace the diagonal matrix \(G_{t}\) in Adagrad's update rule with the decaying average over past squared gradients \(E[g^2]_t\) to arrive at the update rule for Adadelta:</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>Note that the denominator essentially becomes the root mean squared (RMS) criterion while \(\epsilon\) is added to better condition it.</p>

<h2 id="adam">Adam</h2>

<p>show visualizations here: <a href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></p>

<h1 id="additionalconsiderations">Additional considerations</h1>

<h2 id="orderoftrainingexamples">Order of training examples</h2>

<h3 id="shuffling">Shuffling</h3>

<p>Providing the training examples in a meaningful order may bias the optimization algorithm. Consequently, it is a good idea to shuffle the training data after every epoch.</p>

<h3 id="curriculumlearning">Curriculum learning</h3>

<h3 id="batchnormalization">Batch normalization</h3>

<p><a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf">http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf</a> slide 62 for quick summary</p>

<p>An incomplete list of additional tips and tricks: <br>
- Early stopping
slide 63 <br>
Probably the best quote from NIPS 2015: "Early stopping (is) beautiful free lunch." <br>
- Gradient checking. See here: <a href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></p>

<h1 id="conclusion">Conclusion</h1>

<p>...</p>

<p>Are there any obvious algorithms to improve SGD that I've missed? What tricks are you using yourself to facilitate training with SGD? <strong>Let me know in the comments below.</strong></p>

<h1 id="references">References</h1>

<ol>
<li>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.  </li>
<li>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145â151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a>  </li>
<li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a>  </li>
<li>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, â¦ Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1â11. <a href="http://doi.org/10.1109/ICDAR.2011.95">http://doi.org/10.1109/ICDAR.2011.95</a>  </li>
<li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532â1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a>  </li>
<li>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a></li>
</ol>

<p><a name="fn:1">1</a> Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters \(d\).</p>]]></content:encoded></item></channel></rss>
