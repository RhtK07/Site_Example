<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.]]></description><link>http://sebastianruder.com/</link><generator>Ghost 0.7</generator><lastBuildDate>Fri, 16 Sep 2016 09:45:40 GMT</lastBuildDate><atom:link href="http://sebastianruder.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Lisbon Machine Learning Summer School Highlights]]></title><description><![CDATA[<p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#sequencemodels">Sequence Models</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#machinetranslation">Machine Translation</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#structuredprediction">Structured Prediction</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#crfsvsmemms">CRFs vs. MEMMs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#hmmsvscrfs">HMMs vs. CRFs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#structuredpredictioninnlpwithimitationlearning">Structured Prediction in NLP with Imitation Learning</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#demoday">Demo Day</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#syntaxandparsing">Syntax and Parsing</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#fromdependenciestoconstituents">From Dependencies to Constituents</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#deeplearning">Deep Learning</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#explodingvanishinggradients">Exploding / vanishing gradients</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#deeplstms">Deep LSTMs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#doesdepthmatter">Does Depth Matter?</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#minibatching">Mini-batching</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#characterbasedmodels">Character-based models</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#attention">Attention</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#memorynetworksforlanguageunderstanding">Memory Networks for Language Understanding</a></li></ul></li></ul>]]></description><link>http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/</link><guid isPermaLink="false">9e99b979-1000-4ae9-b85d-88cd01bc95c0</guid><category><![CDATA[deep learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[nlp]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Fri, 12 Aug 2016 09:52:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/08/group_picture-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/08/group_picture-1.jpg" alt="Lisbon Machine Learning Summer School Highlights"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#sequencemodels">Sequence Models</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#machinetranslation">Machine Translation</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#structuredprediction">Structured Prediction</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#crfsvsmemms">CRFs vs. MEMMs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#hmmsvscrfs">HMMs vs. CRFs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#structuredpredictioninnlpwithimitationlearning">Structured Prediction in NLP with Imitation Learning</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#demoday">Demo Day</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#syntaxandparsing">Syntax and Parsing</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#fromdependenciestoconstituents">From Dependencies to Constituents</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#deeplearning">Deep Learning</a>
<ul><li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#explodingvanishinggradients">Exploding / vanishing gradients</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#deeplstms">Deep LSTMs</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#doesdepthmatter">Does Depth Matter?</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#minibatching">Mini-batching</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#characterbasedmodels">Character-based models</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#attention">Attention</a></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#memorynetworksforlanguageunderstanding">Memory Networks for Language Understanding</a></li></ul></li>
<li><a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#slides">Slides</a></li>
</ul>

<p>From July 20th to July 28th 2016, I had the opportunity of  attending the 6th Lisbon Machine Learning School. <a href="http://lxmls.it.pt/2016/">The Lisbon Machine Learning School (LxMLS)</a> is an annual event that brings together researchers and graduate students in the fields of NLP and Computational Linguistics, computer scientists with an interest in statistics and ML, and industry practitioners with a desire for a more in-depth understanding. <br>
Participants had a chance to join workshops and labs, where they got hands-on experience with building and exploring state-of-the-art deep learning models, as well as to attend talks and speeches by prominent deep learning and NLP researchers from a variety of academic and industrial organisations. You can find the entire programme <a href="http://lxmls.it.pt/2016/?page_id=64">here</a>.</p>

<p>In this blog post, I am going to share some of the highlights, key insights and takeaways of the summer school. I am going to skip the lectures of the first and second day as they introduce basic Python, Linear Algebra, and Probability Theory concepts and focus on the later lectures and talks. First, we are going to talk about sequence models. We will then turn to structured prediction, a type of supervised ML common to NLP. We will then summarize the lecture on Syntax and Parsing and finally provide insights with regard to Deep Learning. The accompanying slides can be found <a href="http://sebastianruder.com/lisbon-machine-learning-summer-school-highlights/#slides">here</a>.</p>

<p><strong>Disclaimer</strong>: This blog post is not meant to give a comprehensive introduction of each of the topics discussed; it should rather give you an overview of the week-long event and provide you with pointers if you want to delve deeper into any of the topics.</p>

<p><strong>Note</strong>: This blog post also appeared at the <a href="http://blog.aylien.com/">AYLIEN blog</a> <a href="http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/">here</a>.</p>

<h1 id="sequencemodels">Sequence Models</h1>

<p><a href="http://homes.cs.washington.edu/~nasmith/">Noah Smith</a> of the University Washington kicked off the third day of the summer school with a compelling lecture about sequence models.
To test your understanding of sequence models, try to answer -- without reading further -- the following question: What is the most basic sequence model depicted in Figure 1?</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/bag_of_words.png" style="width: 50%; height: 50%" title="Bag-of-words model" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 1: The most basic sequence model</figcaption>  
</figure>

<p>Correct! It is the bag-of-words (notice which words have "fallen" out of the bag-of-words). The bag-of-words model makes the strongest independence assumption of all sequence models: It supposes that each word is entirely independent of its predecessors. It is obvious why models that rely on this assumption do only a poor job at modelling language: every word naturally depends on the words that have preceded it.</p>

<p>Somewhat more sophisticated models thus relax this naive assumption to reduce the entropy: A 1st Order Markov model makes each word dependent on the word that immediately preceded it. This way, it is already able to capture some context of the context that can help to disambiguate a new word. More generally, \(m^{\text{th}}\) Order Markov Models make each word depend on its previous \(m\) words.</p>

<p>In mathematical terms, in \(m^{\text{th}}\) Order Markov Models, the probability of a text sequence (we assume here that such a sequence is delimited by start and stop symbols) can be calculated using the chain rule as the product of the probabilities of the individual words:</p>

<p>\(p(\text{start}, w_1, w_2, ..., w_n, \text{stop}) = \prod\limits_{i=1}^{n+1} \gamma (w_i \: | \: w_{i-m}, ..., w_{i-1}) \)</p>

<p>where \(\gamma\) is the probability of the current word \(w_i\) given its \(m\) previous words, i.e. the probability to transition from the previous words to the current word.</p>

<p>We can view bag-of-words and \(m^{\text{th}}\) Order Markov Models as occupying the following spectrum:</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/mth_order_markov_spectrum.png" style="width: 80%; height: 80%" title="From bag-of-words to history-based models" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 2: From bag-of-words to history-based models</figcaption>  
</figure>

<p>As we go right in Figure 2, we make weaker independence assumption and in exchange gain richer expressive power, while requiring more parameters -- until we eventually obtain the most expressive -- and most rigid -- model, a history-based model where each word depends on its entire history, i.e. all preceding words.</p>

<p>As a side-note, state-of-the-art sequence modelling models such as recurrent neural networks and LSTMs can be thought of as being located on the right side of this spectrum, as they don't require an explicit specification of context words but are -- theoretically -- able to take into account the entire history.</p>

<p>In many cases, we would not only like to model just the observed sequence of symbols, but take some additional information into account. Hidden Markov Models (HMMs) allow us to associate with each symbol \(w_i\) some missing information, its "state" \(s_i\). The probability of a word sequence in an HMM then not only depends on the transition probability \(\gamma\) but also on the so-called emission probability \(\eta\):</p>

<p>\(p(\text{start}, w_1, w_2, ..., w_n, \text{stop}) = \prod\limits_{i=1}^{n+1} \eta (w_i \: | \: s_i) \: \gamma (s_i \: | \: s_{i-1}) \)</p>

<p>Consequently, the HMM is a joint model over observable symbols and hidden/latent/unknown classes. HMMs have traditionally been used in part-of-speech tagging or named entity recognition where the hidden states are POS and NER tags respectively.</p>

<p>If we want to determine the most probable sequence of hidden states, we face a space of potential sequences that grows exponentially with the sequence length. The classic dynamic algorithm to cope with this problem is the Viterbi algorithm, which is used in HMMs, CRFs, and other sequence models to calculate the most probable sequence of hidden states: It lays out the symbol sequence and all possible states in a grid and proceeds left-to-right to compute the maximum probability to transition in every new state given the previous states. The most probable sequence can then be found by back-tracking as in Figure 3.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/viterbi.png" style="width: 70%; height: 70%" title="Viterbi algorithm" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 3: The Viterbi algorithm</figcaption>  
</figure>

<p>A close relative is the forward-backward algorithm, which is used to calculate the probability of a word sequence and the probabilities of each word's states, e.g. for language modelling. Indeed, the only difference between Viterbi and the forward-backward algorithm is that Viterbi takes the maximum of the probabilities of the previous state, while forward-backward takes the sum. In this sense, they correspond to the same abstract algorithm, which is instantiated in two different semirings where a semiring informally is a set of values and some operations that obey certain properties.</p>

<p>Finally, if we want to learn HMMs in an unsupervised way, we use the well-known Expectation Maximisation (EM) algorithm, which consists of two steps: During the E step, we calculate the probability of each possible transition and emission at every position with forward-backward (or Viterbi for "hard" EM); for the M step, we re-estimate the parameters with MLE.</p>

<h2 id="machinetranslation">Machine Translation</h2>

<p>On the evening of the third day, Philipp Koehn, one of the pioneers of MT and inventor of phrase-based machine translation gave a talk on Machine Translation as Sequence Modelling, including a detailed review of different MT and alignment approaches. If you are interested in a comprehensive history of MT that takes you from IBM Model 1 all the way to phrase-based, syntax-based and eventually neural MT, while delving into the details of alignment, translation, and decoding, definitely check out the slides <a href="http://lxmls.it.pt/2016/talk.pdf">here</a>. </p>

<h1 id="structuredprediction"> Structured Prediction</h1>

<p>HMMs can model sequences, but as their weights are tied to the generative process, strong independence assumptions need to be made to make their computation tractable. We will now turn to a category of models that are more expressive and can be used to predict more complex structures: Structured prediction -- which was introduced by Xavier Carreras of Xerox Research on the morning of the fourth day -- is used to refer to ML algorithms that don’t just predict scalar or real values, but more complex structures. As complex structures are common in language, so is structured prediction; example tasks of structured prediction in NLP include POS tagging, named entity recognition, machine translation, parsing, and many others.</p>

<p>A successful category of structured prediction models are log-linear models, which are so-called because they model log-probabilities using a linear predictor. Such models try to estimate the parameters \(w\) by calculating the following probability:</p>

<p>\(\text{log} \: \text{Pr}(\mathbf{y} \: | \: \mathbf{x}; \mathbf{w}) = \text{log} \: \dfrac{\text{exp}\{\mathbf{w} \cdot \mathbf{f}(\mathbf{x},\mathbf{y})\}}{Z(\mathbf{x};\mathbf{w})}\)</p>

<p>where \(\mathbf{x} = x_1, x_2, ..., x_n \in \mathcal{X}\) is the sequence of symbols, \(\mathbf{y} = y_1, y_2, ..., y_n \in \mathcal{Y}\) is the corresponding sequence of labels, \(\mathbf{f}(\mathbf{x},\mathbf{y})\) is a feature representation of \(\mathbf{x}\) and \(\mathbf{y}\), and \(Z(\mathbf{x};\mathbf{w}) = \sum\limits_{\mathbf{y}' \in \mathcal{Y}} \text{exp}(\mathbf{w} \cdot \mathbf{f}(\mathbf{x},\mathbf{y}')) \) is also referred to as the partition function.</p>

<p>Two approaches that can be used to estimate the model parameters \(w\) are:</p>

<ol>
<li>Maximum Entropy Markov Models (MEMMs), which assume that \(\text{Pr}(\mathbf{y} \: | \: \mathbf{x}; \mathbf{w})\) decomposes, i.e. that we can express it as a product of the individual label probabilities that only depend on the previous label (similar to HMMs).  </li>
<li>Conditional Random Fields (CRFs), which make a weaker assumption by only assuming that \(\mathbf{f}(\mathbf{x},\mathbf{y})\) decomposes.</li>
</ol>

<p>In MEMMs, we assume -- similarly to Markov Models -- that the label \(y_i\) at does not depend on <em>all</em> past labels, but only on the previous label \(y_{i-1}\). In contrast to Markov Models, MEMMs allow us to condition the label \(y_i\) on the entire symbol sequence \(x_{1:n}\). Both assumptions combined lead to the following probability of label \(y_i\) in MEMMs:</p>

<p>\(\text{Pr}(y_i \: | \: x_{1:n}, y_{1:i-1}) = \text{Pr}(y_i \: | \: x_{1:n}, y_{i-1})\)</p>

<p>By this formulation, the objective of MEMMs reduces sequence modelling to multi-class logistic regression.</p>

<p>In CRFs, we factorize on label bigrams. Instead of greedily predicting the most probable label \(y_i\) at every position \(i\), we instead aim to find the sequence of labels with the maximum probability:</p>

<p>\(\underset{y \in \mathcal{Y}}{\text{argmax}} \sum_i \mathbf{w} \cdot \mathbf{f}(\mathbf{x}, i, y_{i-1}, y_i)\)</p>

<p>We then estimate the parameters \(w\) of our model using gradient-based methods where we can use forward-backward to compute the gradient.</p>

<h2 id="crfsvsmemms">CRFs vs. MEMMs</h2>

<p>By choosing between MEMMs and CRFs, we make the choice between local and global normalisation. While MEMMs aim to predict the most probable label at every position, CRFs aim to find the most probable label sequence. This, however, leads to the so-called "Label Bias Problem" in MEMMs: As MEMMs choose the label with the highest probability, the model is biased towards more frequent labels, often irrespective of the local context.</p>

<p>As MEMMs reduce to multi-class classification, they are cheaper to train. On the other hand, CRFs are more flexible and thus easier to extend to more complex structures.</p>

<p>This distinction between local and global normalisation has been a recurring topic in sequence modelling and a key criterion when choosing an algorithm. For text generation tasks, global normalisation is still too expensive, however. Many state-of-the-art approaches thus employ beam search as a compromise between local and global normalisation. In most sequence modelling tasks, local normalisation is very popular due to its ease of use, but might fall out of favour as more advanced models and implementations for global normalisation become available. To this effect, a recent outstanding paper at ACL <a href="https://arxiv.org/abs/1603.06042">(Andor et al., 2016)</a> shows that globally normalised models are strictly more expressive than locally normalised ones.</p>

<h2 id="hmmsvscrfs"> HMMs vs. CRFs</h2>

<p>Another distinction that is worth investigating is the difference between generative and discriminative models: HMMs are generative models, while CRFs are discriminative. HMMs only take into account the previous word as its features are tied to the generative process. In contrast, CRF features are very flexible. They can look at the whole input \(x\) paired with a label bigram \((y_i , y_{i+1})\). In practice, for prediction tasks, such “good” discriminative features can improve accuracy a lot.</p>

<p>Regarding the parameter estimation, the distinction between generative and discriminative becomes apparent: HMMs focus on explaining the data, both \(x\) and \(y\), while CRFs focus on the mapping from \(x\) to \(y\). Which model is more appropriate depends on the task: CRFs are commonly used in tasks such as POS tagging and NER, while HMMs have traditionally lain at the heart of speech recognition.</p>

<h2 id="structuredpredictioninnlpwithimitationlearning">Structured Prediction in NLP with Imitation Learning</h2>

<p><a href="http://andreasvlachos.github.io/">Andreas Vlachos</a> of the University of Sheffield gave a talk on using imitation learning for structured prediction in NLP, which followed the same distinction between local normalisation (aka incremental modelling), i.e. greedily predicting one label at a time and global normalisation (aka joint modelling), i.e. scoring the complete outputs with a CRF that we discussed above. Andreas talked about how imitation learning can be used to improve incremental modelling as it allows to a) explore the search space, b) to address error-propagation, and c) to train with regard to the task-level loss function.</p>

<p>There are many popular imitation learning algorithms in the literature such as <a href="http://www.umiacs.umd.edu/~hal/docs/daume09searn.pdf">SEARN (Daumé III et al., 2009)</a>, <a href="https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf">Dagger (Ross et al. 2011)</a>, or <a href="http://www.aclweb.org/anthology/Q14-1042">V-DAgger (Vlachos and Clark, 2014)</a>. Recently, <a href="http://www.aclweb.org/anthology/Q14-1042">MIXER (Ranzato et al., 2016)</a> has been proposed to directly optimise metrics for text generation, such as BLEU or ROUGE.</p>

<p>An interesting perspective is that imitation learning can be seen as inverse reinforcement learning: Whereas we want to learn the best policy in reinforcement learning, we know the optimal policy in imitation learning, i.e. the labels in the training data; we then infer the per-action reward function and learn a policy, i.e. a classifier that can generalise to unseen data.</p>

<h2 id="demoday">Demo Day</h2>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/demo_day.jpg" style="width: 70%; height: 70%" title="Aylien stand at Demo Day" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 4: Aylien stand at Demo Day</figcaption>  
</figure>

<p>In the evening of the fourth day, we presented -- along with other NLP companies and research labs -- Aylien at the LxMLS Demo Day.</p>

<p>We presented an overview of our research directions at Aylien, as well as a 1D generative adversarial network <a href="http://notebooks.aylien.com/research/gan/gan_simple.html">demo and visualization</a>.</p>

<h1 id="syntaxandparsing">Syntax and Parsing</h1>

<p>Having looked at generic models that are able to cope with sequences and more complex structures, we now briefly mention some of the techniques that are commonly used to deal with one of language’s unique characteristics: syntax. To this end, <a href="http://research.google.com/pubs/author38945.html">Slav Petrov</a> of Google Research gave an in-depth lecture about syntax and parsing on the fifth day of the summer school, which discussed, among others, successful parsers such as the Charniak and the Berkeley parser, context-free grammars and phrase-based parsing, projective and non-projective dependency parsing, as well as more recent transition and graph-based parsers.</p>

<p>To tie this to what we've already discussed, Figure 5 demonstrates how the distinction between generative and discriminative models applies to parsers.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/generative_discriminative.png" style="width: 80%; height: 80%" title="Generative vs. discriminative parsers" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 5: Generative vs. discriminative parsing models</figcaption>  
</figure>

<h2 id="fromdependenciestoconstituents"> From Dependencies to Constituents</h2>

<p>In the evening of the fifth day, <a href="https://www.cs.cmu.edu/~afm/Home.html">André Martins</a> of Unbabel gave talk a on an <a href="https://www.cs.cmu.edu/~afm/Home_files/acl2015_reduction.pdf">ACL 2015 paper</a> of his, in which he shows that constituent parsing can be reduced to dependency parsing to get the best of both worlds: the informativeness of constituent parser output and the speed of dependency parsers.</p>

<p>Their approach works for any out-of-the-box dependency parser, is competitive for English and morphologically rich languages, and achieves results above the state of the art for discontinuous parsing (where edges are allowed to intersect).</p>

<h1 id="deeplearning">Deep Learning</h1>

<p>Finally, the two last days were dedicated to Deep Learning and featured prolific researchers from academia and industry labs as speakers. On the morning of the sixth day, <a href="http://www.cs.cmu.edu/~lingwang/">Wang Ling</a> of Google DeepMind gave one of the most gentle, family-friendly intro to Deep Learning I've seen -- titled <em>Deep Neural Networks Are Our Friends</em> with a theme inspired by the Muppets.</p>

<p>The evening talk by <a href="http://research.google.com/pubs/OriolVinyals.html">Oriol Vinyals</a> of Google DeepMind detailed some of his lessons learned when working on sequence-to-sequence models at Google and gave glimpses of interesting future challenges, among them, one-shot learning for NLP <a href="http://arxiv.org/abs/1606.04080">(Vinyals et al., 2016)</a> and enabling neural networks to ponder decisions <a href="https://arxiv.org/abs/1603.08983">(Graves, 2016)</a>.</p>

<p>For the lecture on the last day, <a href="http://www.cs.cmu.edu/~cdyer/">Chris Dyer</a> of CMU and Google DeepMind discussed modelling sequential data with recurrent neural networks (RNNs) and shared some insights and intuitions with regard to working with RNNs and LSTMs.</p>

<h2 id="explodingvanishinggradients">Exploding / vanishing gradients</h2>

<p>If you've worked with RNNs before, then you're most likely familiar with the exploding/vanishing gradients problem: As the length of the sequence increases, computations of the model get amplified, which leads to either exploding or vanishing gradients and thus renders the model incapable to learn. The intuition why advanced models such as LSTMs and GRUs mitigate this problem is that they use summations instead of multiplications (which lead to exponential growth).</p>

<h2 id="deeplstms">Deep LSTMs</h2>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/deep_lstms.png" style="width: 80%; height: 80%" title="Deep LSTMs" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 6: Deep LSTMs</figcaption>  
</figure>

<p>Deep or stacked LSTMs are by now a very common sight in the literature and state-of-the-art for many sequence modelling problems. Still, descriptions of implementations often omit details, which might be perceived as self-evident. This, however, means that it is not always clear how a model looks like exactly or how it differs from similar architectures. The same applies to Deep LSTMs. The most standard convention feeds the input not only to the first but (via skip connections) also to subsequent layers as in Figure 6. Additionally, dropout is generally applied only between layers and not on the recurrent connections as this would drop out more and more value over time.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/dropout_and_deep_lstms.png" style="width: 80%; height: 80%" title="Dropout in Deep LSTMs" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 7: Dropout in Deep LSTMs</figcaption>  
</figure>

<h2 id="doesdepthmatter"> Does Depth Matter?</h2>

<p>Generally, depth helps. However, in comparison to other applications such as audio/visual processing, depth plays a less significant role in NLP. Hypotheses for this observation are: a) More transformation is required for speech processing, image recognition etc. than for common text applications; b) Less effort has been made to find good architectures (RNNs are expensive to train; have been widely used for less long); c) Backpropagation through time and depth is hard and we need better optimisers.</p>

<p>Generally, 2-8 layers are standard across text applications. Input skip connections are used often but by no means universally.</p>

<p>Only recently have also very deep architectures been proposed for NLP <a href="http://arxiv.org/abs/1606.01781">(Conneau et al., 2016)</a>.</p>

<h2 id="minibatching"> Mini-batching</h2>

<p>Mini-batching is generally necessary to make use of optimised matrix-matrix multiplication. In practice, however, this usually requires bucketing training instances based on similar lengths and padding with \(0\)'s, which can be a nuisance. Because of this, this is -- according to Chris Dyer -- "the era of assembly language programming for neural networks. Make the future an easier place to program!"</p>

<h2 id="characterbasedmodels"> Character-based models</h2>

<p>Character-based models have gained more popularity recently and for some tasks such as language modelling, using character-based LSTMs blows the results of word-based models out of the water, achieving a significantly lower perplexity with a lot fewer parameters particularly for morphologically rich languages.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/08/char_language_modeling.png" style="width: 80%; height: 80%" title="CharLSTM > Word Lookup" alt="Lisbon Machine Learning Summer School Highlights">
<figcaption>Figure 8: CharLSTM > Word Lookup</figcaption>  
</figure>

<h2 id="attention"> Attention</h2>

<p>Finally, no overview of recurrent neural networks is complete without the mention of attention, one of the most influential, recently proposed notions with regard to LSTMs. Attention is closely related to “pooling” operations in convolutional neural networks (and other architectures) as it also allows to selectively focus on particular elements of the input. <br>
The most popular attention architecture pioneered by Bahdanau et al. <a href="https://arxiv.org/abs/1409.0473">(2015)</a> seems to only care about “content” in that it relies on computing the dot product, i.e. the cosine similarity between vectors. It contains no obvious bias in favor of diagonals, short jumps, fertility, or other structures that might guide actual attention from a psycho-linguistic perspective. <br>
Some work has begun to add other “structural” biases (<a href="http://arxiv.org/abs/1508.04025">Luong et al., 2015</a>; <a href="https://arxiv.org/abs/1601.01085">Cohn et al., 2016</a>), but there are many more opportunities for research.</p>

<p>Attention is similar to alignment, but there are important differences: a) alignment makes stochastic but hard decisions. Even if the alignment probability distribution is “flat”, the model picks one word or phrase at a time; b) <br>
in contrast, attention is “soft” (all words are interpolated based on their attention weights). Finally, there is a big difference between “flat” and “peaked” attention weights.</p>

<h2 id="memorynetworksforlanguageunderstanding">Memory Networks for Language Understanding</h2>

<p><a href="https://research.facebook.com/antoine-bordes">Antoine Bordes</a> of Facebook AI Research gave the last talk of the summer school, in which he discussed Facebook AI Research's two main research directions:</p>

<p>On the one hand, they are working on (Key-Value) Memory Networks, which can be used to jointly model symbolic and continuous systems. They can be trained end-to-end through back propagation and with SGD and provide great flexibility on how to design memories.</p>

<p>On the other hand, they are working on new tools for developing learning algorithms. They have created <a href="https://research.facebook.com/research/babi/">several datasets</a> of reasonable sizes, such as bAbI, CBT, and MovieQA that are designed to ease interpretation of a model's capabilities and to foster research.</p>

<p>That was the Lisbon Machine Learning Summer School 2016! I had a blast and hope to be back next year!</p>

<h1 id="slides">Slides</h1>

<ul>
<li><a href="http://lxmls.it.pt/2016/sequencemodels.smith.7-23-16.pdf">Sequence Models (Noah Smith)</a></li>
<li><a href="http://lxmls.it.pt/2016/talk.pdf">Machine Translation as Sequence Modelling (Philipp Koehn)</a></li>
<li><a href="http://lxmls.it.pt/2016/strlearn.pdf">Learning Structured Predictors (Xavier Carreras)</a></li>
<li><a href="http://andreasvlachos.github.io/assets/lectures_reveal_js/LxMLS22July2016/ImitationLearningTutorial.html#/">Structured Prediction in NLP with Imitation Learning (Andreas Vlachos)</a></li>
<li>Syntax and Parsing - <a href="http://lxmls.it.pt/2016/Part_1_Constituency_Parsing_2016.pdf">I</a>, <a href="http://lxmls.it.pt/2016/Part2_Dependency_Parsing_2016.pdf">II</a> (Slav Petrov)</li>
<li><a href="http://lxmls.it.pt/2016/lxmls2016_slides.pdf">Turbo Parser Redux: From Dependencies to Constituents (André Martins)</a></li>
<li><a href="http://lxmls.it.pt/2016/Deep-Neural-Networks-Are-Our-Friends.pdf">Deep Neural Networks Are Our Friends (Wang Ling)</a></li>
<li><a href="http://lxmls.it.pt/2016/lxmls-dl2.pdf">Modeling Sequential Data with Recurrent Networks (Chris Dyer)</a></li>
<li><a href="http://lxmls.it.pt/2016/abordes-lxmlss.pdf">Memory Networks for Language Understanding (Antoine Bordes)</a></li>
</ul>]]></content:encoded></item><item><title><![CDATA[On word embeddings - Part 2: Approximating the Softmax]]></title><description><![CDATA[This blog post gives an overview of softmax-based and sampling-based approaches that approximate the softmax layer for learning word embeddings.]]></description><link>http://sebastianruder.com/word-embeddings-softmax/</link><guid isPermaLink="false">23b4d38f-dfda-4cbd-949b-7978ab56090e</guid><category><![CDATA[deep learning]]></category><category><![CDATA[word embeddings]]></category><category><![CDATA[softmax]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 13 Jun 2016 10:00:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/06/softmax_classifier.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/06/softmax_classifier.png" alt="On word embeddings - Part 2: Approximating the Softmax"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#softmaxarchitecturemodifications">Softmax-based Approaches</a>
<ul><li><a href="http://sebastianruder.com/word-embeddings-softmax/#hierarchicalsoftmax">Hierarchical Softmax</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#differentiatedsoftmax">Differentiated Softmax</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#cnnsoftmax">CNN-Softmax</a></li></ul></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#samplingbasedapproaches">Sampling-based Approaches</a>
<ul><li><a href="http://sebastianruder.com/word-embeddings-softmax/#importancesampling">Importance Sampling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#adaptiveimportancesampling">Adaptive Importance Sampling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#targetsampling">Target Sampling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#noisecontrastiveestimation">Noise Contrastive Estimation</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#negativesampling">Negative Sampling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#selfnormalisation">Self-Normalisation</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#infrequentnormalisation">Infrequent Normalisation</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#otherapproaches">Other Approaches</a></li></ul></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#whichapproachtochoose">Which Approach to Choose?</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-softmax/#conclusion">Conclusion</a></li>
</ul>

<p>This is the second post in a series on word embeddings and representation learning. In the <a href="http://sebastianruder.com/word-embeddings-1/index.html">previous post</a>, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [<sup id="fnref:10"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:10" rel="footnote">10</a></sup>]) and language modelling (Jozefowicz et al. [<sup id="fnref:6"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:6" rel="footnote">6</a></sup>]).</p>

<p>In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.</p>

<p>Let us know partially re-introduce the previous post's notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of \(T\) training words \(w_1, w_2, w_3, \cdots, w_T\) that belong to a vocabulary \(V\) whose size is \(|V|\). Our models generally consider a context \(c\) of \( n \) words. We associate every word with an input embedding \( v_w \) (the eponymous word embedding in the Embedding Layer) with \(d\) dimensions and an output embedding \( v'_w \) (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function \(J_\theta\) with regard to our model parameters \(\theta\).</p>

<p>Recall that the softmax calculates the probability of a word \(w\) given its context \(c\) and can be computed using the following equation:</p>

<p>\(p(w \: | \: c) = \dfrac{\text{exp}({h^\top v'_w})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \)</p>

<p>where \(h\) is the output vector of the penultimate network layer. Note that we use \(c\) for the context as mentioned above and drop the index \(t\) of the target word \(w_t\) for simplicity. Computing the softmax is expensive as the inner product between \(h\) and the output embedding of every word \(w_i\) in the vocabulary \(V\) needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word \(w\) given its context \(c\).</p>

<p>In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.</p>

<h1 id="softmaxbasedapproaches">Softmax-based Approaches</h1>

<h2 id="hierarchicalsoftmax"> Hierarchical Softmax</h2>

<p>Hierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [<sup id="fnref:3"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:3" rel="footnote">3</a></sup>]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1. <br>
This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least \(50 \times\) and is thus critical for low-latency tasks such as real-time communication in <a href="http://googleresearch.blogspot.ie/2016/05/chat-smarter-with-allo.html">Google's new messenger app Allo</a>.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/06/hierarchical_softmax_example.png" style="width: 70%; height: 70%" title="Hierarchical softmax" alt="On word embeddings - Part 2: Approximating the Softmax">
<figcaption>Figure 1: Hierarchical softmax (<a href="https://www.quora.com/Word2vec-How-can-hierarchical-soft-max-training-method-of-CBOW-guarantee-its-self-consistence">Quora</a>)</figcaption>  
</figure>

<p>We can think of the regular softmax as a tree of depth \(1\), with each word in \(V\) as a leaf node.  Computing the softmax probability of one word then requires normalizing over the probabilities of all \(|V|\) leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes. <br>
Since a balanced binary tree has a depth of \(\text{log}_2 (\:|V|\:)\), we only need to evaluate at most \(\text{log}_2 (\:|V|\:)\) nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to \( 1 \) and thus form a probability distribution. To informally verify this, we can reason that at a tree's root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to \(1\). At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to \(1\) and hence the hierarchical softmax defines a normalized probability distribution over all words in \(V\).</p>

<p>To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings \(v'_w\) for every word \(w\) -- instead, we have embeddings \(v'_n\) for every node \(n\). As we have \(|V|-1\) nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node \(n\) given the context \(c\) the following way:</p>

<p>\( p(\: \text{right}\: | \: n, c) = \sigma (h^\top v'_{n})\).</p>

<p>This is almost the same as the computations in the regular softmax; now instead of computing the dot product between \(h\) and the output word embedding \(v'_{w}\), we compute the dot product between \(h\) and the embedding \(v'_{w}\) of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node \(n\) in this case, with the sigmoid function. Conversely, the probability of turning left is simply \( 1 - p(\:\text{right}\: | \: n,c)\).</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/05/hierarchical_softmax.png" style="width: 70%; height: 70%" title="Hierarchical softmax" alt="On word embeddings - Part 2: Approximating the Softmax">
<figcaption>Figure 2: Hierarchical softmax computations (<a href="https://www.youtube.com/watch?v=B95LTf2rVWM">Hugo Lachorelle's Youtube lectures</a>)</figcaption>  
</figure>

<p>The probability of a word \(w\) given its context \(c\) is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context "the", "dog", "and", "the", the probability of the word "cat" in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent <a href="https://www.youtube.com/watch?v=B95LTf2rVWM">lecture video</a>. Rong [<sup id="fnref:7"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:7" rel="footnote">7</a></sup>] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.</p>

<p>Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [<sup id="fnref:8"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:8" rel="footnote">8</a></sup>] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.</p>

<p>Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.</p>

<p>In practice, instead of using "right" and "left" in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a <code class="language-python">0</code> bit for turning left and a <code class="language-python">1</code> bit for turning right, we can thus represent the path to "cat" as <code class="language-python">011</code>. <br>
Recall that the path length in a balanced binary tree is \(\text{log}_2 |V|\). If we set \(|V| = 10000\), this amounts to an average path length of about \( 13.3 \). Analogously, we can represent every word by the bit vector of its path that is on average \(13.3\) bits long. In information theory, this is referred to as an  information content of \(13.3\) bits per word.</p>

<h3 id="anoteontheinformationcontentofwords">A note on the information content of words</h3>

<p>Recall that the information content \(I(w)\) of a word \(w\) is the negative logarithm of its probability \(p(w)\):</p>

<p>\(I(w) = -\log_2 p(w) \).</p>

<p>The entropy \( H \) of all words in a corpus is then the expectation of the information content of all words in the vocabulary:  </p>

<p>\( H = \sum_{i\in V} p(w_i) \: I(w_i) \).</p>

<p>We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need \(1\) bit per flip, whereas we need \(0\) bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy \(H\) equals the information content \(I(w)\) of every word \(w\), as each word has the same probability. The average word entropy \(H\) in a balanced binary tree with \(|V| = 10000\) thus coincides with its average path length:</p>

<p>\( H = - \sum_{i\in V} \dfrac{1}{10000} \log_2  \dfrac{1}{10000} = 13.3\).</p>

<p>We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with \(|V| = 10,000\)) is about \( 9.16 \). </p>

<p>Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from \( 13.3 \) to \( 9.16 \) in this case, which amounts to a speed-up of 31%. A <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman tree</a>, which is used by Mikolov et al. [<sup id="fnref:1"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:1" rel="footnote">1</a></sup>] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, "the", the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.</p>

<p>A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [<sup id="fnref:5"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:5" rel="footnote">5</a></sup>] establishes in his experiments that the lower bound on the information rate in English is between \(0.6\) to \(1.3\) bits per character; given an average word length of \(4.5\), this amounts to \(2.7\) - \(5.85\) bits per word.</p>

<p>To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is \(2^{H}\) where \(H\) is the entropy. A unigram entropy of \( 9.16 \) thus entails a still very high perplexity of \( 2^{9.16} = 572.0\). We can render this value more tangible by observing that a model with a perplexity of \(572\) is as confused by the data as if it had to choose among \(572\) possibilities for each word uniformly and independently.</p>

<p>To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of \(24.2\) per word on the 1B Word Benchmark. Such a model would thus require an average of around \(4.60\) bits to encode each word, as \( 2^{4.60} = 24.2 \), which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.</p>

<h2 id="differentiatedsoftmax"> Differentiated Softmax</h2>

<p>Chen et al. [<sup id="fnref:9"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:9" rel="footnote">9</a></sup>] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.</p>

<p>In order to do this, instead of the dense matrix of the regular softmax layer of size \(d \: \times \: |V| \) containing the output word embeddings \( v'_w \in \mathbb{R}^d \), they use a sparse matrix. They then arrange \( v'_w\) in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality \(d_k \). The number of blocks and their embedding sizes are hyperparameters that can be tuned.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/05/differentiated_softmax_1.png" style="width: 30%; height: 30%" title="Differentiated softmax" alt="On word embeddings - Part 2: Approximating the Softmax">
<figcaption>Figure 3: Differentiated softmax (Chen et al. (2015))</figcaption>  
</figure>

<p>In Figure 3, embeddings in partition \(A\) are of dimensionality \(d_A\) (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions \(B\) and \(C\) have \(d_B\) and \(d_C\) dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to \(0\).</p>

<p>The output of the previous hidden layer \(h\) is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. \(h\) in Figure 3 is made up of partitions of size \(d_A\), \(d_B\), and \(d_B\) respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and \(h\) as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in \(h\).</p>

<p>As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.</p>

<h2 id="cnnsoftmax">CNN-Softmax</h2>

<p>Another modification to the traditional softmax layer is inspired by recent work by Kim et al. [<sup id="fnref:13"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:13" rel="footnote">13</a></sup>] who produce input word embeddings \(v_w\) via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings \(v'_w\) via a character-level CNN -- and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings \(v'_w\) is necessarily different from the CNN generating the input word embeddings \(v_w\), just as the input and output word embedding matrices would be different.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/05/cnn-softmax_1.png" style="width: 30%; height: 30%" title="CNN-Softmax" alt="On word embeddings - Part 2: Approximating the Softmax">
<figcaption>Figure 4: CNN-Softmax (Jozefowicz et al. (2016))</figcaption>  
</figure>

<p>While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of \(d \: \times \: |V| \), we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings \(v'_w\) can be pre-computed, so that there is no loss in performance. </p>

<p>However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.</p>

<p>The authors also note that instead of using a CNN-softmax, the output of the previous layer \(h\) can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [<sup id="fnref:14"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:14" rel="footnote">14</a></sup>] use a similar layer for machine translation and achieve competitive results.</p>

<h1 id="samplingbasedapproaches">Sampling-based Approaches</h1>

<p>While the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time -- during inference, the full softmax still needs to be computed to obtain a normalised probability.</p>

<p>In order to gain some intuitions about the softmax denominator's impact on the loss, we will derive the gradient of our loss function \(J_\theta\) w.r.t. the parameters of our model \(\theta\). <br>
During training, we aim to minimize the cross-entropy loss of our model for every word \(w\) in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at <a href="http://cs231n.github.io/linear-classify/#softmax-classifier">Karpathy's explanation</a> to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:</p>

<p>\(J_\theta = - \: \text{log} \: \dfrac{\text{exp}({h^\top v'_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>

<p>Note that in practice \(J_\theta\) would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose \(J_\theta \) into a sum as \(\text{log} \: \dfrac{x}{y} = \text{log} \: x - \text{log} \: y \): </p>

<p>\(J_\theta = - \: h^\top v'_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top v'_{w_i}) \)</p>

<p>For brevity and to conform with the notation of  Bengio and Senécal [<sup id="fnref:4"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:4" rel="footnote">4</a></sup>, <sup id="fnref:15"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:15" rel="footnote">15</a></sup>] (note that in the first paper, they compute the gradient of the <em>positive</em> logarithm), we replace the dot product \( h^\top v'_{w} \) with \( - \mathcal{E}(w) \). Our loss then looks like the following:</p>

<p>\(J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i)) \)</p>

<p>For back-propagation, we can now compute the gradient \(\nabla \) of \(J_\theta \) w.r.t. our model's parameters \(\theta\):</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w)  + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \)</p>

<p>As the gradient of \( \text{log} \: x \) is \(\dfrac{1}{x} \), an application of the chain rule yields:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i) \)</p>

<p>We can now move the gradient inside the sum:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w)  + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i)) \)</p>

<p>As the gradient of \(\text{exp}(x)\) is just \(\text{exp}(x)\), another application of the chain rule yields:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i)) \)</p>

<p>We can rewrite this as:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i)) \)</p>

<p>Note that \( \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \) is just the softmax probability \(P(w_i) \) of \(w_i\) (we omit the dependence on the context \(c\) here for brevity). Replacing it yields:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i)) \)</p>

<p>Finally, repositioning the negative coefficient in front of the sum yields:</p>

<p>\(\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) \)</p>

<p>Bengio and Senécal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word \(w\) (the first term in the above equation) and a negative reinforcement for all other words \(w_i\), which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation \(\mathbb{E}_{w_i \sim P}\) of the gradient of \(\mathcal{E} \) for all words \(w_i\) in \(V\):</p>

<p>\(\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) =  \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]\).</p>

<p>The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we don't want to sum over the probabilities for all words in \(V\).</p>

<h2 id="importancesampling">Importance Sampling</h2>

<p>We can approximate the expected value \(\mathbb{E}\) of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the network's distribution, i.e. \(P(w)\), we could thus directly sample \(m\) words \( w_1 , \cdots , w_m \) from it and approximate the above expectation with:</p>

<p>\( \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i) \).</p>

<p>However, in order to sample from the probability distribution \( P \), we need to compute \( P \), which is just what we wanted to avoid in the first place. We therefore have find some other distribution \( Q \) (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, \(Q\) should also be similar to \(P\), since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for \( Q \).</p>

<p>This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution \(P\) via a proposal distribution \(Q\). However, this still requires computing \(P(w)\) for every word \(w\) that is sampled. To avoid this, Bengio and Senécal (2003) use a biased estimator that was first proposed by Liu [<sup id="fnref:16"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:16" rel="footnote">16</a></sup>]. This estimator can be used when \( P(w) \) is computed as a product, which is the case here, since every division can be transformed into a multiplication. <br>
Essentially, instead of weighting the gradient \(\nabla_\theta \mathcal{E}(w_i)\) with the expensive to compute probability \(P_{w_i}\), we weight it with a factor that leverages the proposal distribution \(Q\). For biased IS, this factor is \(\dfrac{1}{R}r(w_i)\) where \( r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)} \) and \( R = \sum^m_{j=1} r(w_j) \). <br>
Note that we use \( r \) and \( R \) instead of \( w\) and \(W\) as in Bengio and Senécal (2003, 2008) to avoid name clashes. As we can see, we still compute the nominator of the softmax, but replace the normalisation in the denominator with the proposal distribution \(Q\). Our biased estimator that approximates the expectation thus looks like the following:</p>

<p>\( \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx  \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)\)</p>

<p>Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the network's distribution \(P\) might diverge from the unigram distribution \(Q \) during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and Senécal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of \(19 \) over the regular softmax for this method.</p>

<h2 id="adaptiveimportancesampling">Adaptive Importance Sampling</h2>

<p>Bengio and Senécal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution \(Q\) from the model's true distribution \(P\) does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution \(P\) more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution \( Q \) and the target distribution \( P\). For experiments, they report a speed-up factor of about \(100\).</p>

<h2 id="targetsampling"> Target Sampling</h2>

<p>Jean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset \(V'\) of the vocabulary.</p>

<p>This essentially means that a separate proposal distribution \(Q_i \) can be used for every partition \(i\) of the training set, which assigns equal probability to all words included in the vocabulary subset \(V'_i\) and zero probability to all other words.</p>

<h2 id="noisecontrastiveestimation">Noise Contrastive Estimation</h2>

<p>Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen) [<sup id="fnref:17"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:17" rel="footnote">17</a></sup>] is proposed by Mnih and Teh [<sup id="fnref:18"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:18" rel="footnote">18</a></sup>] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution \(Q\) diverge from the distribution \(P\) that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.</p>

<p>Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than "corrupted" windows, which we discussed in the <a href="http://sebastianruder.com/word-embeddings-1/index.html">previous post</a>. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/06/negative_sampling.png" style="width: 70%; height: 70%" title="Noise Contrastive Estimation" alt="On word embeddings - Part 2: Approximating the Softmax">
<figcaption>Figure 4: Noise Contrastive Estimation (<a href="https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html">TensorFlow</a>)</figcaption>  
</figure>

<p>For every word \(w_i\) given its context \(c_i \) of \(n\) previous words \(w_{t-1} , \cdots , w_{t-n+1}\) in the training set, we thus generate \(k\) noise samples \(\tilde{w}_{ik}\) from a noise distribution \(Q\). As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words \(w_i\) given their context \(c_i\) as true (\(y=1\)) and all noise samples \(\tilde{w}_{ik}\) as false (\(y=0\)).</p>

<p>We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the <em>positive</em> log-likelihood as some papers do):</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]] \).</p>

<p>Instead of computing the expectation \(\mathbb{E}_{\tilde{w}_{ik} \sim Q}\) of our noise samples, which would still require summing over all words in \(V\) to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)] \),</p>

<p>which reduces to:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)] \),</p>

<p>By generating \(k\) noise samples for every genuine word \(w_i\) given its context \(c\), we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set \(P_{\text{train}}\) and depend on their context \(c\), whereas noise samples come from the noise distribution \(Q\). We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:</p>

<p>\(P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w) \).</p>

<p>Given this mixture, we can now calculate the probability that a sample came from the training \(P_{\text{train}}\) distribution as a conditional probability of \(y\) given \(w\) and \(c\):</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)} \),</p>

<p>which can be simplified to:</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)} \).</p>

<p>As we don't know \(P_{\text{train}}\) (which is what we would like to calculate), we replace \(P_{\text{train}}\) with the probability of our model \(P\):</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)} \).</p>

<p>The probability of predicting a noise sample (\(y=0\)) is then simply \(P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)\). Note that computing \( P(w \: | \: c) \), i.e. the probability of a word \(w\) given its context \(c\) is essentially the definition of our softmax:</p>

<p>\(P(w \: | \: c) = \dfrac{\text{exp}({h^\top v'_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>

<p>For notational brevity and unambiguity, let us designate the denominator of the softmax with \(Z(c)\), since the denominator only depends on \(h\), which is generated from \(c\) (assuming a fixed \(V\)). The softmax then looks like this:</p>

<p>\(P(w \: | \: c) = \dfrac{\text{exp}({h^\top v'_{w}})}{Z(c)} \). </p>

<p>Having to compute \(P(w \: | \: c)\) means that -- again -- we need to compute \(Z(c)\), which requires us to sum over the probabilities of all words in \(V\). In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator \(Z(c)\) as a parameter that the model can learn. <br>
Mnih and Teh (2012) and Vaswani et al. [<sup id="fnref:20"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:20" rel="footnote">20</a></sup>] actually keep \(Z(c)\) fixed at \(1\), which they report does not affect the model's performance. This assumption has the nice side-effect of reducing the model's parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in \(Z(c)\). Indeed, Zoph et al. [<sup id="fnref:19"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:19" rel="footnote">19</a></sup>] find that even when learned, \(Z(c)\) is close to \(1\) and has low variance.</p>

<p>If we thus set \(Z(c)\) to \(1\) in the above softmax equation, we are left with the following probability of word \(w\) given a context \(c\):</p>

<p>\(P(w \: | \: c) = \text{exp}({h^\top v'_{w}})\).</p>

<p>We can now insert this term in the above equation to compute \(P(y=1\:|\:w,c)\):</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v'_{w}})}{\text{exp}({h^\top v'_{w}}) + k \: Q(w)} \).</p>

<p>Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top v'_{w_i}})}{\text{exp}({h^\top v'_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top v'_{\tilde{w}_{ij}}})}{\text{exp}({h^\top v'_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})] \).</p>

<p>Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples \(k\), the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that \(25\) noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about \(45\). For more information on NCE, Chris Dyer has published some excellent notes [<sup id="fnref:21"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:21" rel="footnote">21</a></sup>].</p>

<p>One caveat of NCE is that as typically different noise samples are sampled for every training word \(w\), the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.</p>

<h3 id="similaritybetweennceandis">Similarity between NCE and IS</h3>

<p>Jozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.</p>

<h2 id="negativesampling">Negative Sampling</h2>

<p>Negative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples \(k\) increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.</p>

<p>NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word \(w\) comes from the empirical training distribution \(P_{\text{train}}\) given a context \(c\) as follows:</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v'_{w}})}{\text{exp}({h^\top v'_{w}}) + k \: Q(w)} \).</p>

<p>The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, \(k \: Q(w)\) to \(1\), which leaves us with:</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v'_{w}})}{\text{exp}({h^\top v'_{w}}) + 1} \).</p>

<p>\(k \: Q(w) = 1\) is exactly then true, when \(k=\: |V|\) and \(Q\) is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set \(k \: Q(w) = 1\) and not to some other constant can be seen by rewriting the equation, as \(P(y=1\:|\:w,c)\) can be transformed into the sigmoid function:</p>

<p>\( P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top v'_{w}})} \).</p>

<p>If we now insert this back into the logistic regression loss from before, we get:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v'_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top v'_{\tilde{w}_{ij}}})}] \).</p>

<p>By simplifying slightly, we obtain:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v'_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top v'_{\tilde{w}_{ij}}})}] \).</p>

<p>Setting \(\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}\) finally yields the NEG loss:</p>

<p>\( J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top v'_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top v'_{\tilde{w}_{ij}})] \).</p>

<p>To conform with the notation of Mikolov et al. (2013), \(h\) must be replaced with \(v_{w_I}\), \(v'_{w_i}\) with \(v'_{w_O}\) and \(v_{\tilde{w}_{ij}}\) with \(v'_{w_i}\). Also, in contrast to Mikolov's NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation \(\mathbb{E}_{\tilde{w}_{ik} \sim Q}\) with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levy's notes [<sup id="fnref:22"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:22" rel="footnote">22</a></sup>].</p>

<p>We have seen that NEG is only equivalent to NCE when \(k=\: |V|\) and \(Q\) is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.</p>

<h2 id="selfnormalisation">Self-Normalisation</h2>

<p>Even though the self-normalisation technique proposed by Devlin et al. <sup id="fnref:23"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:23" rel="footnote">23</a></sup> is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator \(Z(c)\) of the NCE loss to \(1\), the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in \(Z(c)\).</p>

<p>Recall that our loss function \(J_\theta\) minimises the negative log-likelihood of all words \(w_i\) in our training data:</p>

<p>\(J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top v'_{w_i}})}{Z(c)}] \). </p>

<p>We can decompose the softmax into a sum as we did before:</p>

<p>\(J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top v'_{w_i} + \text{log} \: Z(c)] \).</p>

<p>If we are able to constrain our model so that it sets \(Z(c) = 1\) or similarly \(\text{log} \: Z(c) = 0\), then we can avoid computing the normalisation in \(Z(c)\) altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep \( Z(c)\) as close as possible to \(0\): </p>

<p>\(J_\theta = - \sum\limits_i [h^\top v'_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2] \),</p>

<p>which can be rewritten as:</p>

<p>\(J_\theta = - \sum\limits_i [h^\top v'_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)] \)</p>

<p>where \(\alpha\) allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that \(Z(c)\) will be as close to \(1\) as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to \(1\) and only use the nominator for computing \(P(w \: | \: c)\) together with their penalty term:</p>

<p>\(J_\theta = - \sum\limits_i [h^\top v'_{w_i} - \alpha \: \text{log}^2 Z(c)] \)</p>

<p>They report that self-normalisation achieves a speed-up factor of about \(15\), while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.</p>

<h2 id="infrequentnormalisation">Infrequent Normalisation</h2>

<p>Andreas and Klein [<sup id="fnref:11"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:11" rel="footnote">11</a></sup>] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.</p>

<p>Let us first decompose the sum of the previous loss \(J_\theta\) into two separate sums:</p>

<p>\(J_\theta = - \sum\limits_i h^\top v'_{w_i} +  \alpha \sum\limits_i \text{log}^2 Z(c) \).</p>

<p>We can now down-sample the second term by only computing the normalisation for a subset \(C\) of words \(w_j\) and thus of contexts \(c_j\) (as \(Z(c)\) only depends on the context \(c\)) in the training data:</p>

<p>\(J_\theta = - \sum\limits_i h^\top v'_{w_i} +  \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j) \)</p>

<p>where \(\gamma\) controls the size of the subset \(C\). Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of \(10\) when normalising only a tenth of the training set, with no noticeable performance penalty.</p>

<h3 id="otherapproaches">Other Approaches</h3>

<p>So far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator \(Z(c)\), as it is the most expensive term in the computation. We have thus not paid particular attention to \(h^\top v'_{w}\), i.e. the dot-product between the penultimate layer representation \(h\) and output word embedding \(v'_{w}\). Vijayanarasimhan et al. [<sup id="fnref:12"><a href="http://sebastianruder.com/word-embeddings-softmax/#fn:12" rel="footnote">12</a></sup>] propose fast locality-sensitive hashing to approximate \(h^\top v'_{w}\). However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.</p>

<h1 id="whichapproachtochoose">Which Approach to Choose?</h1>

<p>Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good ol' softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.</p>

<figure>  
<style type="text/css">  
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-e3zv{font-weight:bold}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
</style>  

<table class="tg">  
  <tr>
    <th class="tg-e3zv">Approach</th>
    <th class="tg-e3zv">Speed-up<br>factor</th>
    <th class="tg-e3zv">During<br>training?</th>
    <th class="tg-9hbo">During<br>testing?</th>
    <th class="tg-9hbo">Performance<br>(small vocab)</th>
    <th class="tg-9hbo">Performance<br>(large vocab)</th>
    <th class="tg-9hbo">Proportion of<br>parameters</th>
  </tr>
  <tr>
    <td class="tg-031e">Softmax</td>
    <td class="tg-031e">1x</td>
    <td class="tg-031e">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">very good</td>
    <td class="tg-yw4l">very poor</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Hierarchical Softmax</td>
    <td class="tg-yw4l">25x (50-100x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">very poor</td>
    <td class="tg-yw4l">very good</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Differentiated Softmax</td>
    <td class="tg-yw4l">2x</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">very good</td>
    <td class="tg-yw4l">very good</td>
    <td class="tg-yw4l">&lt; 100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">CNN-Softmax</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">bad - good</td>
    <td class="tg-yw4l">30%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Importance Sampling</td>
    <td class="tg-yw4l">(19x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Adaptive<br>Importance Sampling</td>
    <td class="tg-yw4l">(100x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Target Sampling</td>
    <td class="tg-yw4l">2x</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">good</td>
    <td class="tg-yw4l">bad</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Noise Contrastive<br>Estimation</td>
    <td class="tg-yw4l">8x (45x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">very bad</td>
    <td class="tg-yw4l">very bad</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Negative Sampling</td>
    <td class="tg-yw4l">(50-100x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Self-Normalisation</td>
    <td class="tg-yw4l">(15x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">100%</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Infrequent<br>Normalisation</td>
    <td class="tg-yw4l">6x (10x)</td>
    <td class="tg-yw4l">X</td>
    <td class="tg-yw4l">-</td>
    <td class="tg-yw4l">very good</td>
    <td class="tg-yw4l">good</td>
    <td class="tg-yw4l">100%</td>
  </tr>
</table>  

<figcaption>Table 1: Comparison of approaches to approximate the softmax for language modelling.</figcaption>  
</figure>

<p>We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.</p>

<p>As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies. <br>
Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vec's success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.</p>

<p>Finally, if you are looking to actually use the described methods, TensorFlow has <a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling">implementations</a> for a few sampling-based approaches and also explains the differences between some of them <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">here</a>.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.</p>

<p>As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.</p>

<p><strong>As always, let me know about any mistakes I made and approaches I missed in the comments below.</strong></p>

<h1 id="references">References</h1>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:1" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:2"><p>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:2" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:3"><p>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:3" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:4"><p>Bengio, Y., &amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. <a href="http://doi.org/10.1017/CBO9781107415324.004">http://doi.org/10.1017/CBO9781107415324.004</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:4" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:5"><p>Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. <a href="http://doi.org/10.1002/j.1538-7305.1951.tb01366.x">http://doi.org/10.1002/j.1538-7305.1951.tb01366.x</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:5" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:6"><p>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410">http://arxiv.org/abs/1602.02410</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:6" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:7"><p>Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:7" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:8"><p>Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from <a href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf">http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:8" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:9"><p>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from <a href="http://arxiv.org/abs/1512.04906">http://arxiv.org/abs/1512.04906</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:9" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:10"><p>Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from <a href="http://www.aclweb.org/anthology/P15-1001">http://www.aclweb.org/anthology/P15-1001</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:10" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:11"><p>Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:11" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:12"><p>Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from <a href="http://arxiv.org/abs/1412.7479">http://arxiv.org/abs/1412.7479</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:12" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:13"><p>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615">http://arxiv.org/abs/1508.06615</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:13" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:14"><p>Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.04586">http://arxiv.org/abs/1511.04586</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:14" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:15"><p>Bengio, Y., &amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. <a href="http://doi.org/10.1109/TNN.2007.912312">http://doi.org/10.1109/TNN.2007.912312</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:15" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:16"><p>Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. <a href="http://doi.org/10.1017/CBO9781107415324.004">http://doi.org/10.1017/CBO9781107415324.004</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:16" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:17"><p>Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf">http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:17" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:18"><p>Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:18" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:19"><p>Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:19" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:20"><p>Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:20" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:21"><p>Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from <a href="http://arxiv.org/abs/1410.8251">http://arxiv.org/abs/1410.8251</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:21" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:22"><p>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a> <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:22" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:23"><p>Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL’2014, 1370–1380. <a href="http://sebastianruder.com/word-embeddings-softmax/#fnref:23" title="return to article">↩</a></p></li></ol></div>

<p>Credit for the cover image goes to the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html">Tensorflow</a> team.</p>]]></content:encoded></item><item><title><![CDATA[On word embeddings - Part 1]]></title><description><![CDATA[The first post in a series about word embeddings. This post presents word embedding models in the context of language modeling and past research.]]></description><link>http://sebastianruder.com/word-embeddings-1/</link><guid isPermaLink="false">4c791259-51e7-4e35-a5b4-a588296a9de3</guid><category><![CDATA[deep learning]]></category><category><![CDATA[word embeddings]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 11 Apr 2016 14:00:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png" alt="On word embeddings - Part 1"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/word-embeddings-1/#abriefhistoryofwordembeddings">A brief history of word embeddings</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#wordembeddingmodels">Word embedding models</a>
<ul><li><a href="http://sebastianruder.com/word-embeddings-1/#anoteonlanguagemodeling">A note on language modeling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#classicneurallanguagemodel">Classic neural language model</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#cwmodel">C&amp;W model</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#word2vec">Word2Vec</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#continuousbagofwordscbow">CBOW</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#skipgram">Skip-gram</a></li></ul></li>
</ul>

<p>Unsupervisedly learned word embeddings have been exceptionally successful in many NLP tasks and are frequently seen as something akin to a <em>silver bullet</em>. In fact, in many NLP architectures, they have almost completely replaced traditional distributional features such as Brown clusters and LSA features. </p>

<p>Proceedings of last year's <a href="https://aclweb.org/anthology/P/P15/">ACL</a> and <a href="https://aclweb.org/anthology/D/D15/">EMNLP</a> conferences have been dominated by word embeddings, with some people musing that <em>Embedding Methods in Natural Language Processing</em> was a more fitting name for EMNLP. This year's ACL features not <a href="https://sites.google.com/site/repl4nlp2016/">one</a> but <a href="https://sites.google.com/site/repevalacl16/">two</a> workshops on word embeddings.</p>

<p>Semantic relations between word embeddings seem nothing short of magical to the uninitiated and Deep Learning NLP talks frequently prelude with the notorious \(king - man + woman \approx queen \) slide, while <a href="http://cacm.acm.org/magazines/2016/3/198856-deep-or-shallow-nlp-is-breaking-out/fulltext">a recent article</a> in <em>Communications of the ACM</em> hails word embeddings as the primary reason for NLP's breakout.</p>

<p>This post will be the first in a series that aims to give an extensive overview of word embeddings showcasing why this hype may or may not be warranted. In the course of this review, we will try to connect the disperse literature on word embedding models, highlighting many models, applications and interesting features of word embeddings, with a focus on multilingual embedding models and word embedding evaluation tasks in later posts. <br>
This first post lays the foundations by presenting current word embeddings based on language modelling. While many of these models have been discussed at length, we hope that investigating and discussing their merits in the context of past and current research will provide new insights.</p>

<p>A brief note on nomenclature: In the following we will use the currently prevalent term <em>word embeddings</em> to refer to dense representations of words in a low-dimensional vector space. Interchangeable terms are <em>word vectors</em> and <em>distributed representations</em>. We will particularly focus on <em>neural word embeddings</em>, i.e. word embeddings learned by a neural network.</p>

<h1 id="abriefhistoryofwordembeddings">A brief history of word embeddings</h1>

<p>Since the 1990s, vector space models have been used in distributional semantics. During this time, many models for estimating continuous representations of words have been developed, including Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Have a look at <a href="https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/">this blog post</a> for a more detailed overview of distributional semantics history in the context of word embeddings.</p>

<p>Bengio et al. coin the term word embeddings in 2003 and train them in a neural language model jointly with the model's parameters. First to show the utility of pre-trained word embeddings were arguably Collobert and Weston in 2008. Their landmark paper <em>A unified architecture for natural language processing</em> not only establishes word embeddings as a useful tool for downstream tasks, but also introduces a neural network architecture that forms the foundation for many current approaches. However, the eventual popularization of word embeddings can be attributed to Mikolov et al. in 2013 who created word2vec, a toolkit that allows the seamless training and use of pre-trained embeddings. In 2014, Pennington et al. released GloVe, a competitive set of pre-trained word embeddings, signalling that word embeddings had reached the main stream.</p>

<p>Word embeddings are one of the few currently successful applications of unsupervised learning. Their main benefit arguably is that they don't require expensive annotation, but can be derived from large unannotated corpora that are readily available. Pre-trained embeddings can then be used in downstream tasks that use small amounts of labeled data.</p>

<h1 id="wordembeddingmodels">Word embedding models</h1>

<p>Naturally, every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as <em>Embedding Layer</em>.</p>

<p>The main difference between such a network that produces word embeddings as a by-product and a method such as word2vec whose explicit goal is the generation of word embeddings is its computational complexity. Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary. This is the main reason why it took until 2013 for word embeddings to explode onto the NLP stage; computational complexity is a key trade-off for word embedding models and will be a recurring theme in our review.</p>

<p>Another difference is the training objective: word2vec and GloVe are geared towards producing word embeddings that encode general semantic relationships, which are beneficial to many downstream tasks; notably, word embeddings trained this way won't be helpful in tasks that do not rely on these kind of relationships. In contrast, regular neural networks typically produce task-specific embeddings that are only of limited use elsewhere. Note that a task that relies on semantically coherent representations such as language modelling will produce similar embeddings to word embedding models, which we will investigate in the next chapter. <br>
As a side-note, word2vec and Glove might be said to be to NLP what <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">VGGNet</a> is to vision, i.e. a common weight initialisation that provides generally helpful features without the need for lengthy training.</p>

<p>To facilitate comparison between models, we assume the following notational standards: We assume a training corpus containing a sequence of \(T\) training words \(w_1, w_2, w_3, \cdots, w_T\) that belong to a vocabulary \(V\) whose size is \(|V|\). Our models generally consider a context of \( n \) words. We associate every word with an input embedding \( v_w \) (the eponymous word embedding in the Embedding Layer) with \(d\) dimensions and an output embedding \( v'_w \) (another word representation whose role will soon become clearer). We finally optimize an objective function \(J_\theta\) with regard to our model parameters \(\theta\) and our model outputs some score \(f_\theta(x)\) for every input \( x \).</p>

<h2 id="anoteonlanguagemodelling">A note on language modelling</h2>

<p>Word embedding models are quite closely intertwined with language models. The quality of language models is measured based on their ability to learn a probability distribution over words in \( V \). In fact, many state-of-the-art word embedding models try to predict the next word in a sequence to some extent. Additionally, word embedding models are often evaluated using <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a>, a cross-entropy based measure borrowed from language modelling.</p>

<p>Before we get into the gritty details of word embedding models, let us briefly talk about some language modelling fundamentals.</p>

<p>Language models generally try to compute the probability of a word \(w_t\) given its \(n\) previous words, i.e. \(p(w_t \: | \: w_{t-1} , \cdots w_{t-n+1})\). By applying the chain rule together with the Markov assumption, we can approximate the product of a whole sentence or document by the product of the probabilities of each word given its \(n\) previous words:</p>

<p>\(p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) \).</p>

<p>In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams: <br>
\( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{count(w_{t-n+1}, \cdots , w_{t-1},w_t)}{count({w_{t-n+1}, \cdots , w_{t-1}})}\).</p>

<p>Setting \(n = 2\) yields bigram probabilities, while \(n = 5\) together with Kneser-Ney smoothing leads to smoothed 5-gram models that have been found to be a strong baseline for language modelling. For more details, you can refer to <a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf">these slides</a> from Stanford.</p>

<p>In neural networks, we achieve the same objective using the well-known softmax layer:</p>

<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v'_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>

<p>The inner product \( h^\top v'_{w_t} \) computes the (unnormalized) log-probability of word \( w_t \), which we normalize by the sum of the log-probabilities of all words in \( V \). \(h\) is the output vector of the penultimate network layer (the hidden layer in the feed-forward network in Figure 1), while \(v'_w\) is the output embedding of word \(w \), i.e. its representation in the weight matrix of the softmax layer. Note that even though \(v'_w\) represents the word \(w\), it is learned separately from the input word embedding \(v_w\), as the multiplications both vectors are involved in differ (\(v_w\) is multiplied with an index vector, \(v'_w\) with \(h\)).</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/04/nn_language_model-1.jpg" style="width: 80%; height: 80%" title="SGD without momentum" alt="On word embeddings - Part 1">
<figcaption>Figure 1: A neural language model (Bengio et al., 2006)</figcaption>  
</figure>

<p>Note that we need to calculate the probability of every word \( w \) at the output layer of the neural network. To do this efficiently, we perform a matrix multiplication between \(h\) and a weight matrix whose rows consist of \(v'_w\) of all words \(w\) in \(V\). We then feed the resulting vector, which is often referred to as a logit, i.e. the output of a previous layer that is not a probability, with \(d = |V|\) into the softmax, while the softmax layer "squashes" the vector to a probability distribution over the words in \(V\).</p>

<p>Note that the softmax layer (in contrast to the previous n-gram calculations) only implicitly takes into account \(n\) previous words: LSTMs, which are typically used for neural language models, encode these in their state \(h\), while Bengio's neural language model, which we will see in the next chapter, feeds the previous \(n\) words through a feed-forward layer.</p>

<p>Keep this softmax layer in mind, as many of the subsequent word embedding models will use it in some fashion.</p>

<p>Using this softmax layer, the model tries to maximize the probability of predicting the correct word at every timestep \( t \). The whole model thus tries to maximize the averaged log probability of the whole corpus:</p>

<p>\(J_\theta = \frac{1}{T} \text{log} \space p(w_1 , \cdots , w_T)\).</p>

<p>Analogously, through application of the chain rule, it is usually trained to maximize the average of the log probabilities of all words in the corpus given their previous \( n \) words:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})\).</p>

<p>To sample words from the language model at test time, we can either greedily choose the word with the highest probability \(p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})\) at every time step \( t \) or use beam search. We can do this for instance to generate arbitrary text sequences as in <a href="https://github.com/karpathy/char-rnn">Karpathy's Char-RNN</a> or as part of a sequence prediction task, where an LSTM is used as the decoder.</p>

<h2 id="classicneurallanguagemodel"> Classic neural language model</h2>

<p>The classic neural language model proposed by  Bengio et al. [<sup id="fnref:1"><a href="http://sebastianruder.com/word-embeddings-1/#fn:1" rel="footnote">1</a></sup>] in 2003 consists of a one-hidden layer feed-forward neural network that predicts the next word in a sequence as in Figure 2.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/04/bengio_language_model.png" style="width: 70%; height: 70%" title="Language model by Bengio et al." alt="On word embeddings - Part 1">
<figcaption>Figure 2: Classic neural language model (Bengio et al., 2003)</figcaption>  
</figure>

<p>Their model maximizes what we've described above as the prototypical neural language model objective (we omit the regularization term for simplicity):</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space f(w_t , w_{t-1} , \cdots , w_{t-n+1})\).</p>

<p>\( f(w_t , w_{t-1} , \cdots , w_{t-n+1}) \) is the output of the model, i.e. the probability \( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) \) as computed by the softmax, where \(n \) is the number of previous words fed into the model.</p>

<p>Bengio et al. are one of the first to introduce what we now refer to as a word embedding, a real-valued word feature vector in \(\mathbb{R}\). Their architecture forms very much the prototype upon which current approaches have gradually improved. The general building blocks of their model, however, are still found in all current neural language and word embedding models. These are:</p>

<ol>
<li><strong>Embedding Layer</strong>: a layer that generates word embeddings by multiplying an index vector with a word embedding matrix;  </li>
<li><strong>Intermediate Layer(s)</strong>: one or more layers that produce an intermediate representation of the input, e.g. a fully-connected layer that applies a non-linearity to the concatenation of word embeddings of \(n\) previous words;  </li>
<li><strong>Softmax Layer</strong>: the final layer that produces a probability distribution over words in \(V\).</li>
</ol>

<p>Additionally, Bengio et al. identify two issues that lie at the heart of current state-of-the-art-models:</p>

<ul>
<li>They remark that <strong>2.</strong> can be replaced by an LSTM, which is used by state-of-the-art neural language models [<sup id="fnref:6"><a href="http://sebastianruder.com/word-embeddings-1/#fn:6" rel="footnote">6</a></sup>, <sup id="fnref:7"><a href="http://sebastianruder.com/word-embeddings-1/#fn:7" rel="footnote">7</a></sup>].</li>
<li>They identify the final softmax layer (more precisely: the normalization term) as the network's main bottleneck, as the cost of computing the softmax is proportional to the number of words in \(V\), which is typically on the order of hundreds of thousands or millions.</li>
</ul>

<p>Finding ways to mitigate the computational cost associated with computing the softmax over a large vocabulary [<sup id="fnref:9"><a href="http://sebastianruder.com/word-embeddings-1/#fn:9" rel="footnote">9</a></sup>] is thus one of the key challenges both in neural language models as well as in word embedding models.</p>

<h2 id="cwmodel"> C&amp;W model</h2>

<p>After Bengio et al.'s first steps in neural language models, research in word embeddings stagnated as computing power and algorithms did not yet allow the training of a large vocabulary.</p>

<p>Collobert and Weston [<sup id="fnref:4"><a href="http://sebastianruder.com/word-embeddings-1/#fn:4" rel="footnote">4</a></sup>] (thus C&amp;W) showcase in 2008 that word embeddings trained on a sufficiently large dataset carry syntactic and semantic meaning and improve performance on downstream tasks. They elaborate upon this in their 2011 paper [<sup id="fnref:8"><a href="http://sebastianruder.com/word-embeddings-1/#fn:8" rel="footnote">8</a></sup>].</p>

<p>Their solution to avoid computing the expensive softmax is to use a different objective function: Instead of the cross-entropy criterion of Bengio et al., which maximizes the probability of the next word given the previous words, Collobert and Weston train a network to output a higher score \(f_\theta\) for a correct word sequence (a probable word sequence in Bengio's model) than for an incorrect one. For this purpose, they use a pairwise ranking criterion, which looks like this:</p>

<p>\(J_\theta\ = \sum\limits_{x \in X} \sum\limits_{w \in V} \text{max} \lbrace 0, 1 - f_\theta(x) + f_\theta(x^{(w)}) \rbrace \).</p>

<p>They sample correct windows \(x\) containing \(n\) words from the set of all possible windows \(X\) in their corpus. For each window \(x\), they then produce a corrupted, incorrect version \(x^{(w)}\) by replacing \(x\)'s centre word with another word \(w\) from \(V\). Their objective now maximises the distance between the scores output by the model for the correct and the incorrect window with a margin of \(1\). Their model architecture, depicted in Figure 3 without the ranking objective, is analogous to Bengio et al.'s model.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/04/nlp_almost_from_scratch_window_approach.png" style="width: 50%; height: 50%" title="C&W model" alt="On word embeddings - Part 1">
<figcaption>Figure 3: The C&W model without ranking objective (Collobert et al., 2011)</figcaption>  
</figure>

<p>The resulting language model produces embeddings that already possess many of the relations word embeddings have become known for, e.g. countries are clustered close together and syntactically similar words occupy similar locations in the vector space. While their ranking objective eliminates the complexity of the softmax, they keep the intermediate fully-connected hidden layer (<strong>2.</strong>) of Bengio et al. around (the <strong>HardTanh</strong> layer in Figure 3), which constitutes another source of expensive computation. Partially due to this, their full model trains for seven weeks in total with \(|V| = 130000\).</p>

<h2 id="word2vec"> Word2Vec</h2>

<p>Let us now introduce arguably the most popular word embedding model, the model that launched a thousand word embedding papers: word2vec, the subject of two papers by Mikolov et al. in 2013. As word embeddings are a key building block of deep learning models for NLP, word2vec is often assumed to belong to the same group. Technically however, word2vec is not be considered to be part of deep learning, as its architecture is neither deep nor uses non-linearities (in contrast to Bengio's model and the C&amp;W model). </p>

<p>In their first paper [<sup id="fnref:2"><a href="http://sebastianruder.com/word-embeddings-1/#fn:2" rel="footnote">2</a></sup>], Mikolov et al. propose two architectures for learning word embeddings that are computationally less expensive than previous models. In their second paper [<sup id="fnref:3"><a href="http://sebastianruder.com/word-embeddings-1/#fn:3" rel="footnote">3</a></sup>], they improve upon these models by employing additional strategies to enhance training speed and accuracy. <br>
These architectures offer two main benefits over the C&amp;W model and Bengio's language model:</p>

<ul>
<li>They do away with the expensive hidden layer.</li>
<li>They enable the language model to take additional context into account.</li>
</ul>

<p>As we will later show, the success of their model is not only due to these changes, but especially due to certain training strategies.</p>

<p>In the following, we will look at both of these architectures:</p>

<h3 id="continuousbagofwordscbow">Continuous bag-of-words (CBOW)</h3>

<p>While a language model is only able to look at the past words for its predictions, as it is evaluated on its ability to predict each next word in the corpus, a model that just aims to generate accurate word embeddings does not suffer from this restriction. Mikolov et al. thus use both the \(n\) words before and after the target word \( w_t \) to predict it as depicted in Figure 4. They call this continuous bag-of-words (CBOW), as it uses continuous representations whose order is of no importance.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/02/cbow.png" style="width: 50%; height: 50%" title="Continuous bag-of-words" alt="On word embeddings - Part 1">
<figcaption>Figure 4: Continuous bag-of-words (Mikolov et al., 2013)</figcaption>  
</figure>

<p>The objective function of CBOW in turn is only slightly different than the language model one:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})\).</p>

<p>Instead of feeding \( n \) previous words into the model, the model receives a window of \( n \) words around the target word \( w_t \) at each time step \( t \).</p>

<h3 id="skipgram">Skip-gram</h3>

<p>While CBOW can be seen as a precognitive language model, skip-gram turns the language model objective on its head: Instead of using the surrounding words to predict the centre word as with CBOW, skip-gram uses the centre word to predict the surrounding words as can be seen in Figure 5.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/02/skip-gram.png" style="width: 50%; height: 50%" title="Skip-gram" alt="On word embeddings - Part 1">
<figcaption>Figure 5: Skip-gram (Mikolov et al., 2013)</figcaption>  
</figure>

<p>The skip-gram objective thus sums the log probabilities of the surrounding \( n \) words  to the left and to the right of the target word \( w_t \) to produce the following objective:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)\).</p>

<p>To gain a better intuition of how the skip-gram model computes \( p(w_{t+j} \: | \: w_t) \), let's recall the definition of our softmax:</p>

<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v'_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>

<p>Instead of computing the probability of the target word \( w_t \) given its previous words, we calculate the probability of the surrounding word \( w_{t+j} \) given \( w_t \). We can thus simply replace these variables in the equation:</p>

<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top v'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>

<p>As the skip-gram architecture does not contain a hidden layer that produces an intermediate state vector \(h\), \(h\) is simply the word embedding \(v_{w_t}\) of the input word \(w_t\). This also makes it clearer why we want to have different representations for input embeddings \(v_w\) and output embeddings \(v'_w\), as we would otherwise multiply the word embedding by itself. Replacing \(h \) with \(v_{w_t}\) yields:</p>

<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({v^\top_{w_t} v'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({v^\top_{w_t} v'_{w_i}})} \).</p>

<p>Note that the notation in Mikolov's paper differs slightly from ours, as they denote the centre word with \( w_I \) and the surrounding words with \( w_O \). If we replace \( w_t \) with \( w_I \), \( w_{t+j} \) with \( w_O \), and swap the vectors in the inner product due to its commutativity, we arrive at the softmax notation in their paper:</p>

<p>\(p(w_O|w_I) = \dfrac{\text{exp}(v'^\top_{w_O} v_{w_I})}{\sum^V_{w=1}\text{exp}(v'^\top_{w} v_{w_I})}\).</p>

<p>In the next post, we will discuss different ways to approximate the expensive softmax as well as key training decisions that account for much of skip-gram's success. We will also introduce GloVe [<sup id="fnref:5"><a href="http://sebastianruder.com/word-embeddings-1/#fn:5" rel="footnote">5</a></sup>], a word embedding model based on matrix factorisation and discuss the link between word embeddings and methods from distributional semantics.</p>

<p>Did I miss anything? <strong>Let me know in the comments below.</strong></p>

<p>Update 21.06.16: This post was posted to Hacker News. <a href="https://news.ycombinator.com/item?id=11943685">The discussion</a> provides some interesting pointers to related work and other techniques.</p>

<h1 id="references">References</h1>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. <a href="http://doi.org/10.1162/153244303322533223">http://doi.org/10.1162/153244303322533223</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:1" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:2"><p>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://sebastianruder.com/word-embeddings-1/#fnref:2" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:3"><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://sebastianruder.com/word-embeddings-1/#fnref:3" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:4"><p>Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. <a href="http://doi.org/10.1145/1390156.1390177">http://doi.org/10.1145/1390156.1390177</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:4" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:5"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:5" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:6"><p>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615">http://arxiv.org/abs/1508.06615</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:6" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:7"><p>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410">http://arxiv.org/abs/1602.02410</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:7" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:8"><p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from <a href="http://arxiv.org/abs/1103.0398">http://arxiv.org/abs/1103.0398</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:8" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:9"><p>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models, 12. Retrieved from <a href="http://arxiv.org/abs/1512.04906">http://arxiv.org/abs/1512.04906</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:9" title="return to article">↩</a></p></li></ol></div>

<p>Credit for the post image goes to <a href="http://colah.github.io/">Christopher Olah</a>.</p>]]></content:encoded></item><item><title><![CDATA[An overview of gradient descent optimization algorithms]]></title><description><![CDATA[This blog post looks at variants of gradient descent and the algorithms that are commonly used to optimize them.]]></description><link>http://sebastianruder.com/optimizing-gradient-descent/</link><guid isPermaLink="false">f991a148-274f-497e-97b9-598455211f57</guid><category><![CDATA[optimization]]></category><category><![CDATA[deep learning]]></category><category><![CDATA[sgd]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 19 Jan 2016 14:20:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" alt="An overview of gradient descent optimization algorithms"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentvariants">Gradient descent variants</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchgradientdescent">Batch gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#stochasticgradientdescent">Stochastic gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#minibatchgradientdescent">Mini-batch gradient descent</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#challenges">Challenges</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#momentum">Momentum</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#nesterovacceleratedgradient">Nesterov accelerated gradient</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adagrad">Adagrad</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adadelta">Adadelta</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#rmsprop">RMSprop</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adam">Adam</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#visualizationofalgorithms">Visualization of algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#whichoptimizertochoose">Which optimizer to choose?</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#parallelizinganddistributingsgd">Parallelizing and distributing SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#hogwild">Hogwild!</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#downpoursgd">Downpour SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#tensorflow">TensorFlow</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#elasticaveragingsgd">Elastic Averaging SGD</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#shufflingandcurriculumlearning">Shuffling and Curriculum Learning</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchnormalization">Batch normalization</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#earlystopping">Early Stopping</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientnoise">Gradient noise</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#conclusion">Conclusion</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#references">References</a></li>
</ul>

<p>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. <a href="http://lasagne.readthedocs.org/en/latest/modules/updates.html">lasagne's</a>, <a href="http://caffe.berkeleyvision.org/tutorial/solver.html">caffe's</a>, and <a href="http://keras.io/optimizers/">keras'</a> documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.</p>

<p>This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training. Subsequently, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent.</p>

<p>Gradient descent is a way to minimize an objective function \(J(\theta)\) parameterized by a model's parameters \(\theta \in \mathbb{R}^d \) by updating the parameters in the opposite direction of the gradient of the objective function \(\nabla_\theta J(\theta)\) w.r.t. to the parameters. The learning rate \(\eta\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley. If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks <a href="http://cs231n.github.io/optimization-1/">here</a>.</p>

<h1 id="gradientdescentvariants"> Gradient descent variants</h1>

<p>There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.</p>

<h2 id="batchgradientdescent">Batch gradient descent</h2>

<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \(\theta\) for the entire training dataset:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta)\).</p>

<p>As we need to calculate the gradients for the whole dataset to perform just <em>one</em> update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model <em>online</em>, i.e. with new examples on-the-fly.</p>

<p>In code, batch gradient descent looks something like this:</p>

<pre><code class="language-python">for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad</code></pre>

<p>For a pre-defined number of epochs,  we first compute the gradient vector <code class="language-python">params_grad</code> of the loss function for the whole dataset w.r.t. our parameter vector <code class="language-python">params</code>.  Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. If you derive the gradients yourself, then gradient checking is a good idea. (See <a href="http://cs231n.github.io/neural-networks-3/">here</a> for some great tips on how to check gradients properly.)</p>

<p>We then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform. Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.</p>

<h2 id="stochasticgradientdescent">Stochastic gradient descent</h2>

<p>Stochastic gradient descent (SGD) in contrast performs a parameter update for <em>each</em> training example \(x^{(i)}\) and label \(y^{(i)}\):</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})\).</p>

<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. <br>
SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in Image 1.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/sgd_fluctuation.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 1: SGD fluctuation (Source: <a href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png">Wikipedia</a>)</figcaption>  
</figure>

<p>While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. <br>
Its code fragment simply adds a loop over the training examples and evaluates the gradient w.r.t. each example. Note that we shuffle the training data at every epoch as explained in <a href="http://sebastianruder.com/optimizing-gradient-descent/#shufflingandcurriculumlearning">this section</a>.</p>

<pre><code class="language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad</code></pre>

<h2 id="minibatchgradientdescent">Mini-batch gradient descent</h2>

<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \(n\) training examples:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\).</p>

<p>This way, it <em>a)</em> reduces the variance of the parameter updates, which can lead to more stable convergence; and <em>b)</em> can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. Note: In modifications of SGD in the rest of this post, we leave out the parameters \(x^{(i:i+n)}; y^{(i:i+n)}\) for simplicity.</p>

<p>In code, instead of iterating over examples, we now iterate over mini-batches of size 50:</p>

<pre><code class="language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad</code></pre>

<h1 id="challenges">Challenges</h1>

<p>Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:</p>

<ul>
<li><p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p></li>
<li><p>Learning rate schedules [<sup id="fnref:11"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:11" rel="footnote">11</a></sup>] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [<sup id="fnref:10"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:10" rel="footnote">10</a></sup>].</p></li>
<li><p>Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p></li>
<li><p>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [<sup id="fnref:19"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:19" rel="footnote">19</a></sup>] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p></li>
</ul>

<h1 id="gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</h1>

<p>In the following, we will outline some algorithms that are widely used by the deep learning community to deal with the aforementioned challenges. We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton's method</a>.</p>

<h2 id="momentum">Momentum</h2>

<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [<sup id="fnref:1"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:1" rel="footnote">1</a></sup>], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image 2.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/without_momentum.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 2: SGD without momentum</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/with_momentum.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 3: SGD with momentum</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>Momentum [<sup id="fnref:2"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:2" rel="footnote">2</a></sup>] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Image 3. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)\).</p>

<p>\(\theta = \theta - v_t\).</p>

<p>Note: Some implementations exchange the signs in the equations. The momentum term \(\gamma\) is usually set to 0.9 or a similar value.</p>

<p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \(\gamma &lt; 1\)). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>

<h1 id="nesterovacceleratedgradient">Nesterov accelerated gradient</h1>

<p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>

<p>Nesterov accelerated gradient (NAG) [<sup id="fnref:7"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:7" rel="footnote">7</a></sup>] is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \(\gamma v_{t-1}\) to move the parameters \(\theta\). Computing \( \theta - \gamma v_{t-1} \) thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters \(\theta\) but w.r.t. the approximate future position of our parameters:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} )\).</p>

<p>\(\theta = \theta - v_t\).</p>

<p>Again, we set the momentum term \(\gamma\) to a value of around 0.9. While Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks [<sup id="fnref:8"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:8" rel="footnote">8</a></sup>].</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/nesterov_update_vector.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 4: Nesterov update (Source: <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">G. Hinton's lecture 6c</a>)</figcaption>  
</figure>

<p>Refer to <a href="http://cs231n.github.io/neural-networks-3/">here</a> for another explanation about the intuitions behind NAG, while Ilya Sutskever gives a more detailed overview in his PhD thesis [<sup id="fnref:9"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:9" rel="footnote">9</a></sup>].</p>

<p>Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.</p>

<h2 id="adagrad">Adagrad</h2>

<p>Adagrad [<sup id="fnref:3"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:3" rel="footnote">3</a></sup>] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. Dean et al. [<sup id="fnref:4"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:4" rel="footnote">4</a></sup>] have found that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, which -- among other things -- learned to <a href="http://www.wired.com/2012/06/google-x-neural-network/">recognize cats in Youtube videos</a>. Moreover, Pennington et al. [<sup id="fnref:5"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:5" rel="footnote">5</a></sup>] used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.</p>

<p>Previously, we performed an update for all parameters \(\theta\) at once as every parameter \(\theta_i\) used the same learning rate \(\eta\). As Adagrad uses a different learning rate for every parameter \(\theta_i\) at every time step \(t\), we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we set \(g_{t, i}\) to be the gradient of the objective function w.r.t. to the parameter \(\theta_i\) at time step \(t\):</p>

<p>\(g_{t, i} = \nabla_\theta J( \theta_i )\).</p>

<p>The SGD update for every parameter \(\theta_i\) at each time step \(t\) then becomes:</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}\).</p>

<p>In its update rule, Adagrad modifies the general learning rate \(\eta\) at each time step \(t\) for every parameter \(\theta_i\) based on the past gradients that have been computed for \(\theta_i\):</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}\).</p>

<p>\(G_{t} \in \mathbb{R}^{d \times d} \) here is a diagonal matrix where each diagonal element \(i, i\) is the sum of the squares of the gradients w.r.t. \(\theta_i\) up to time step \(t\) <sup id="fnref:24"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:24" rel="footnote">24</a></sup>, while \(\epsilon\) is a smoothing term that avoids division by zero (usually on the order of \(1e-8\)). Interestingly, without the square root operation, the algorithm performs much worse.</p>

<p>As \(G_{t}\) contains the sum of the squares of the past gradients w.r.t. to all parameters \(\theta\) along its diagonal, we can now vectorize our implementation by performing an element-wise matrix-vector multiplication \(\odot\) between \(G_{t}\) and \(g_{t}\):</p>

<p>\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.</p>

<p>Adagrad's main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>

<h2 id="adadelta">Adadelta</h2>

<p>Adadelta [<sup id="fnref:6"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:6" rel="footnote">6</a></sup>] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \(w\).</p>

<p>Instead of inefficiently storing \(w\) previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average \(E[g^2]_t\) at time step \(t\) then depends (as a fraction \(\gamma \) similarly to the Momentum term) only on the previous average and the current gradient:</p>

<p>\(E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t \).</p>

<p>We set \(\gamma\) to a similar value as the momentum term, around 0.9. For clarity, we now rewrite our vanilla SGD update in terms of the parameter update vector \( \Delta \theta_t \):</p>

<p>\(\Delta \theta_t = - \eta \cdot g_{t, i}\).</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<p>The parameter update vector of Adagrad that we derived previously thus takes the form:</p>

<p>\( \Delta \theta_t = - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>We now simply replace the diagonal matrix \(G_{t}\) with the decaying average over past squared gradients \(E[g^2]_t\):</p>

<p>\( \Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>As the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:</p>

<p>\( \Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t\).</p>

<p>The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:</p>

<p>\(E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t \).</p>

<p>The root mean squared error of parameter updates is thus: </p>

<p>\(RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon} \).</p>

<p>Since \(RMS[\Delta \theta]_{t}\) is unknown, we approximate it with the RMS of parameter updates until the previous time step. Replacing the learning rate \(\eta \) in the previous update rule with \(RMS[\Delta \theta]_{t-1}\) finally yields the Adadelta update rule:</p>

<p>\( \Delta \theta_t = - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t}\).</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<p>With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.</p>

<h2 id="rmsprop">RMSprop</h2>

<p>RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</p>

<p>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:</p>

<p>\(E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t \).</p>

<p>\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \(\gamma\) to be set to 0.9, while a good default value for the learning rate \(\eta\) is 0.001.</p>

<h2 id="adam">Adam</h2>

<p>Adaptive Moment Estimation (Adam) [<sup id="fnref:15"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:15" rel="footnote">15</a></sup>] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients \(m_t\), similar to momentum:</p>

<p>\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \).</p>

<p>\(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \).</p>

<p>\(m_t\) and \(v_t\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \(m_t\) and \(v_t\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \(\beta_1\) and \(\beta_2\) are close to 1). </p>

<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>

<p>\(\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \).</p>

<p>\(\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2} \).</p>

<p>They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:</p>

<p>\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\).</p>

<p>The authors propose default values of 0.9 for \(\beta_1\), 0.999 for \(\beta_2\), and \(10^{-8}\) for \(\epsilon\). They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.</p>

<h2 id="visualizationofalgorithms">Visualization of algorithms</h2>

<p>The following two animations (Image credit: <a href="https://twitter.com/alecrad">Alec Radford</a>) provide some intuitions towards the optimization behaviour of the presented optimization algorithms. Also have a look <a href="http://cs231n.github.io/neural-networks-3/">here</a> for a description of the same images by Karpathy and another concise overview of the algorithms discussed.</p>

<p>In Image 5, we see their behaviour on the contours of a loss surface over time. Note that Adagrad, Adadelta, and RMSprop almost immediately head off in the right direction and converge similarly fast, while Momentum and NAG are led off-track, evoking the image of a ball rolling down the hill. NAG, however, is quickly able to correct its course due to its increased responsiveness by looking ahead and heads to the minimum.</p>

<p>Image 6 shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the two latter eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 5: SGD optimization on loss surface contours</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 6: SGD optimization on saddle point</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>As we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam are most suitable and provide the best convergence for these scenarios.</p>

<h2 id="whichoptimizertouse">Which optimizer to use?</h2>

<p>So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.</p>

<p>In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [<sup id="fnref:15"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:15" rel="footnote">15</a></sup>] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.</p>

<p>Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.</p>

<h1 id="parallelizinganddistributingsgd">Parallelizing and distributing SGD</h1>

<p>Given the ubiquity of large-scale data solutions and the availability of low-commodity clusters, distributing SGD to speed it up further is an obvious choice. <br>
SGD by itself is inherently sequential: Step-by-step, we progress further towards the minimum. Running it provides good convergence but can be slow particularly on large datasets. In contrast, running SGD asynchronously is faster, but suboptimal communication between workers can lead to poor convergence. Additionally, we can also parallelize SGD on one machine without the need for a large computing cluster. The following are algorithms and architectures that have been proposed to optimize parallelized and distributed SGD.</p>

<h2 id="hogwild">Hogwild!</h2>

<p>Niu et al. [<sup id="fnref:23"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:23" rel="footnote">23</a></sup>] introduce an update scheme called Hogwild! that allows performing SGD updates in parallel on CPUs. Processors are allowed to access shared memory without locking the parameters. This only works if the input data is sparse, as each update will only modify a fraction of all parameters. They show that in this case, the update scheme achieves almost an optimal rate of convergence, as it is unlikely that processors will overwrite useful information.</p>

<h2 id="downpoursgd">Downpour SGD</h2>

<p>Downpour SGD is an asynchronous variant of SGD that was used by Dean et al. [<sup id="fnref:4"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:4" rel="footnote">4</a></sup>] in their DistBelief framework (predecessor to TensorFlow) at Google. It runs multiple replicas of a model in parallel on subsets of the training data. These models send their updates to a parameter server, which is split across many machines. Each machine is responsible for storing and updating a fraction of the model's parameters. However, as replicas don't communicate with each other e.g. by sharing weights or updates, their parameters are continuously at risk of diverging, hindering convergence.</p>

<h2 id="delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</h2>

<p>McMahan and Streeter [<sup id="fnref:12"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:12" rel="footnote">12</a></sup>] extend AdaGrad to the parallel setting by developing delay-tolerant algorithms that not only adapt to past gradients, but also to the update delays. This has been shown to work well in practice.</p>

<h2 id="tensorflow">TensorFlow</h2>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> [<sup id="fnref:13"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:13" rel="footnote">13</a></sup>] is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It is based on their experience with DistBelief and is already used internally to perform computations on a large range of mobile devices as well as on large-scale distributed systems. For distributed execution, a computation graph is split into a subgraph for every device and communication takes place using Send/Receive node pairs. However, the open source version of TensorFlow currently does not support distributed functionality (see <a href="https://github.com/tensorflow/tensorflow/issues/23">here</a>).
Update 13.04.16: A distributed version of TensorFlow has <a href="http://googleresearch.blogspot.ie/2016/04/announcing-tensorflow-08-now-with.html">been released</a>.</p>

<h2 id="elasticaveragingsgd"> Elastic Averaging SGD</h2>

<p>Zhang et al. [<sup id="fnref:14"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:14" rel="footnote">14</a></sup>] propose Elastic Averaging SGD (EASGD), which links the parameters of the workers of asynchronous SGD with an elastic force, i.e. a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. They show empirically that this increased capacity for exploration leads to improved performance by finding new local optima.</p>

<h1 id="additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</h1>

<p>Finally, we introduce additional strategies that can be used alongside any of the previously mentioned algorithms to further improve the performance of SGD. For a great overview of some other common tricks, refer to [<sup id="fnref:22"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:22" rel="footnote">22</a></sup>].</p>

<h2 id="shufflingandcurriculumlearning">Shuffling and Curriculum Learning</h2>

<p>Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch. </p>

<p>On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning [<sup id="fnref:16"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:16" rel="footnote">16</a></sup>]. </p>

<p>Zaremba and Sutskever [<sup id="fnref:17"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:17" rel="footnote">17</a></sup>] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which sorts examples by increasing difficulty.</p>

<h2 id="batchnormalization">Batch normalization</h2>

<p>To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.</p>

<p>Batch normalization [<sup id="fnref:18"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:18" rel="footnote">18</a></sup>] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.</p>

<h2 id="earlystopping">Early stopping</h2>

<p>According to Geoff Hinton: "<em>Early stopping (is) beautiful free lunch</em>" (<a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf">NIPS 2015 Tutorial slides</a>, slide 63). You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.</p>

<h2 id="gradientnoise">Gradient noise</h2>

<p>Neelakantan et al. [<sup id="fnref:21"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:21" rel="footnote">21</a></sup>] add noise that follows a Gaussian distribution \(N(0, \sigma^2_t)\) to each gradient update:</p>

<p>\(g_{t, i} = g_{t, i} + N(0, \sigma^2_t)\).</p>

<p>They anneal the variance according to the following schedule:</p>

<p>\( \sigma^2_t = \dfrac{\eta}{(1 + t)^\gamma} \).</p>

<p>They show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima, which are more frequent for deeper models.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we have initially looked at the three variants of gradient descent, among which mini-batch gradient descent is the most popular. We have then investigated algorithms that are most commonly used for optimizing SGD: Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, as well as different algorithms to optimize asynchronous SGD. Finally, we've considered other strategies to improve SGD such as shuffling and curriculum learning, batch normalization, and early stopping.</p>

<p>I hope that this blog post was able to provide you with some intuitions towards the motivation and the behaviour of the different optimization algorithms. Are there any obvious algorithms to improve SGD that I've missed? What tricks are you using yourself to facilitate training with SGD? <strong>Let me know in the comments below.</strong></p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>Thanks to <a href="https://twitter.com/dennybritz">Denny Britz</a> and <a href="https://twitter.com/cesarsvs">Cesar Salgado</a> for reading drafts of this post and providing suggestions.</p>

<h1 id="printableversionandcitation">Printable version and citation</h1>

<p>This blog post is also available as an <a href="http://arxiv.org/abs/1609.04747">article on arXiv</a>, in case you want to refer to it later.</p>

<p>In case you found it helpful, consider citing the corresponding arXiv article as: <br>
<em>Sebastian Ruder (2016). An overview of gradient descent optimisation algorithms. arXiv preprint arXiv:1609.04747.</em></p>

<h1 id="references">References</h1>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:1" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:2"><p>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:2" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:3"><p>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:3" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:4"><p>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. <a href="http://doi.org/10.1109/ICDAR.2011.95">http://doi.org/10.1109/ICDAR.2011.95</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:4" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:5"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:5" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:6"><p>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:6" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:7"><p>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543– 547. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:7" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:8"><p>Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="http://arxiv.org/abs/1212.0901">http://arxiv.org/abs/1212.0901</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:8" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:9"><p>Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:9" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:10"><p>Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1–11. <a href="http://doi.org/10.1109/NNSP.1992.253713">http://doi.org/10.1109/NNSP.1992.253713</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:10" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:11"><p>H. Robinds and S. Monro, “A stochastic approximation method,” Annals of Mathematical Statistics, vol. 22, pp. 400–407, 1951. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:11" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:12"><p>Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1–9. Retrieved from <a href="http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf">http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:12" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:13"><p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:13" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:14"><p>Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1–24. Retrieved from <a href="http://arxiv.org/abs/1412.6651">http://arxiv.org/abs/1412.6651</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:14" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:15"><p>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:15" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:16"><p>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41–48. <a href="http://doi.org/10.1145/1553374.1553380">http://doi.org/10.1145/1553374.1553380</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:16" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:17"><p>Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1–25. Retrieved from <a href="http://arxiv.org/abs/1410.4615">http://arxiv.org/abs/1410.4615</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:17" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:18"><p>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:18" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:19"><p>Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1–14. Retrieved from <a href="http://arxiv.org/abs/1406.2572">http://arxiv.org/abs/1406.2572</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:19" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:20"><p>Sutskever, I., &amp; Martens, J. (2013). On the importance of initialization and momentum in deep learning. <a href="http://doi.org/10.1109/ICASSP.2013.6639346">http://doi.org/10.1109/ICASSP.2013.6639346</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:20" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:21"><p>Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.06807">http://arxiv.org/abs/1511.06807</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:21" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:22"><p>LeCun, Y., Bottou, L., Orr, G. B., &amp; Müller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9–50. <a href="http://doi.org/10.1007/3-540-49430-8_2">http://doi.org/10.1007/3-540-49430-8_2</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:22" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:23"><p>Niu, F., Recht, B., Christopher, R., &amp; Wright, S. J. (2011). Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1–22. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:23" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:24"><p>Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters \(d\). <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:24" title="return to article">↩</a></p></li></ol></div>

<p>Image credit for cover photo: <a href="http://lossfunctions.tumblr.com/">Karpathy's beautiful loss functions tumblr</a></p>]]></content:encoded></item></channel></rss>
