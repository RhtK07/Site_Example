<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a PhD student in Natural Language Processing and a researcher at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.]]></description><link>http://sebastianruder.com/</link><generator>Ghost 0.7</generator><lastBuildDate>Sat, 09 Jan 2016 19:33:16 GMT</lastBuildDate><atom:link href="http://sebastianruder.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[An overview of gradient descent optimization algorithms]]></title><description><![CDATA[This blog post looks at variants of gradient descent and the algorithms that are commonly used to optimize them.]]></description><link>http://sebastianruder.com/optimizing-gradient-descent/</link><guid isPermaLink="false">f991a148-274f-497e-97b9-598455211f57</guid><category><![CDATA[optimization]]></category><category><![CDATA[deep learning]]></category><category><![CDATA[sgd]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Fri, 08 Jan 2016 11:10:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" alt="An overview of gradient descent optimization algorithms"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentvariants">Gradient descent variants</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchgradientdescent">Batch gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#stochasticgradientdescent">Stochastic gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#minibatchgradientdescent">Mini-batch gradient descent</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#challenges">Challenges</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#momentum">Momentum</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#nesterovacceleratedgradient">Nesterov accelerated gradient</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adagrad">Adagrad</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adadelta">Adadelta</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#rmsprop">RMSprop</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adam">Adam</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#visualizationofalgorithms">Visualization of algorithms</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#distributedsgd">Distributed SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#downpoursgd">Downpour SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#tensorflow">TensorFlow</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#elasticaveragingsgd">Elasic Averaging SGD</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#shufflingandcurriculumlearning">Shuffling and Curriculum Learning</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchnormalization">Batch normalization</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#earlystopping">Early Stopping</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#conclusion">Conclusion</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#references">References</a></li>
</ul>

<p>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (see <a href="http://lasagne.readthedocs.org/en/latest/modules/updates.html">here</a> for lasagne's documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.</p>

<p>This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges when training gradient descent. Subsequently, we will introduce the most common optimization algorithms by showing their motivation and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a distributed setting. Finally, we will briefly consider additional strategies that are helpful for optimizing gradient descent.</p>

<p>Gradient descent is a way to minimize an objective function \(J(\theta)\) parameterized by a model's parameters \(\theta \in \mathbb{R}^d \) by updating the parameters in direction of the gradient of the objective function \(\nabla_\theta J(\theta)\) w.r.t. to the parameters. The learning rate \(\eta\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.</p>

<p>If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks <a href="http://cs231n.github.io/optimization-1/">here</a>.</p>

<h1 id="gradientdescentvariants">Â Gradient descent variants</h1>

<p>There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.  </p>

<h2 id="batchgradientdescent">Batch gradient descent</h2>

<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \(\theta\) for the entire training dataset:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta)\)</p>

<p>As we need to calculate the gradients for the whole dataset to perform just <em>one</em> update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model <em>online</em>, i.e. with new examples on-the-fly.</p>

<p>In code, batch gradient descent looks something like this:</p>

<pre><code class="language-python">while True:
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad</code></pre>

<p>We first compute the gradient vector <code>weights_grad</code> for the whole dataset w.r.t. our parameter vector <code>params</code>.  Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. If you derive the gradients yourself, then gradient checking is a good idea. (See <a href="http://cs231n.github.io/neural-networks-3/">here</a> for some great tips on how to check gradients properly.)</p>

<p>We then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform.</p>

<h2 id="stochasticgradientdescent">Stochastic gradient descent</h2>

<p>Stochastic gradient descent (SGD) in contrast performs a parameter update for <em>each</em> training example \(x^{(i)}\) and label \(y^{(i)}\):</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})\)</p>

<p>It can thus also be used to learn online, one example at a time. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in image 1 but can still lead to fast convergence.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/sgd_fluctuation.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 1: SGD fluctuation (Source: <a href="https://upload.wikimedia.org/wikipedia/en/f/f3/Stogra.png">Wikipedia</a>)</figcaption>  
</figure>

<p>Its code fragment simply adds a loop over the training examples and evaluates the gradient w.r.t. each example. Note that we shuffle the training data at every epoch as we explain in <a href="http://sebastianruder.com/optimizing-gradient-descent/#shuffling">this section</a>.</p>

<pre><code class="language-python">while True:
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad</code></pre>

<h2 id="minibatchgradientdescent">Mini-batch gradient descent</h2>

<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \(n\) training examples:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\)</p>

<p>This way, it a) reduces the variance of the parameter updates, which can lead to more stable convergence; and b) can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. Note: In modifications of SGD in the rest of this post, we leave out the parameters \(x^{(i:i+n)}; y^{(i:i+n)}\) for simplicity.</p>

<p>In code, instead of iterating over examples, we now iterate over mini-batches of size 50:</p>

<pre><code class="language-python">while True:
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad</code></pre>

<h1 id="challenges">Challenges</h1>

<p>Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:</p>

<ul>
<li><p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum.</p></li>
<li><p>Learning rate schedules [11] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [10].</p></li>
<li><p>Additionally, the same learning rate applies for all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p></li>
</ul>

<h1 id="gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</h1>

<p>In the following, we will outline some algorithms that are widely used by the deep learning community to deal with the aforementioned challenges. We will not discuss algorithms that are theoretically optimal but infeasible to compute in practice, e.g. second-order methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton's method</a>.</p>

<h2 id="momentum">Momentum</h2>

<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [1], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in image 1.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/without_momentum.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 2: SGD without momentum</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/with_momentum.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 3: SGD with momentum</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>Momentum [2] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as in image 2. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)\)</p>

<p>\(\theta = \theta - v_t\)</p>

<p>Note: Some implementations exchange the signs in the equations. Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way. The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. In the consequence, we gain faster convergence and reduced oscillation.</p>

<h1 id="nesterovacceleratedgradient">Nesterov accelerated gradient</h1>

<p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that knows where it's going so it knows to slow down before the hill slopes up again.</p>

<p>Nesterov accelerated gradient (NAG) [7] is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \(\gamma v_{t-1}\) to move the parameters \(\theta\). Computing \( \theta - \gamma v_{t-1} \) thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters \(\theta\) but the approximate future position of our parameters:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} )\)</p>

<p>\(\theta = \theta - v_t\)</p>

<p>This anticipatory update prevents us from going too fast, as we will realize sooner that we are hitting a slope, letting us slow down more timely. </p>

<p>While Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks [8].</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/nesterov_update_vector.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 4: Nesterov update (Source: <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">G. Hinton's lecture 6c</a>)</figcaption>  
</figure>

<p>Refer to <a href="http://cs231n.github.io/neural-networks-3/">here</a> for another explanation about the intuitions behind NAG, while Ilya Sutskever gives a more detailed overview in his PhD thesis. [9]</p>

<h2 id="adagrad">Adagrad</h2>

<p>Adagrad [3] is an algorithm for gradient-based optimization such as SGD. It adapts the learning rate to the features, performing larger updates for infrequent and smaller updates for frequent features. For this reason, it is well-suited for dealing with sparse data. Dean et al. [4] have found that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, which -- among other things -- learned to <a href="http://www.wired.com/2012/06/google-x-neural-network/">recognize cats in Youtube videos</a>, while Pennington et al. [5] used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.</p>

<p>Before, we performed an update for all parameters \(\theta\) at once as every parameter \(\theta_i\) used the same learning rate \(\eta\). As Adagrad uses a different learning rate for every parameter \(\theta_i\) at every time step \(t\), we first show Adagrad's per-parameter update, which we then vectorize.</p>

<p>For brevity, we set \(g_{t, i}\) to be the gradient of the objective function w.r.t. to the parameter \(\theta_i\) at time step \(t\):</p>

<p>\(g_{t, i} = \nabla_\theta J( \theta_i )\).</p>

<p>The SGD update for every parameter \(\theta_i\) at each time step \(t\) then becomes:</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}\).</p>

<p>In its update rule, Adagrad modifies the general learning rate \(\eta\) at each time step \(t\) for every parameter \(\theta_i\) based on the past gradients that have been computed for \(\theta_i\):</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}\).</p>

<p>\(G_{t} \in \mathbb{R}^{d \times d} \) here is a diagonal matrix where each diagonal element \(i, i\) is the sum of the squares of the gradients w.r.t. \(\theta_i\) up to time step \(t\) <sup id="fnref:1"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:1" rel="footnote">1</a></sup>, while \(\epsilon\) is a smoothing term that avoids division by zero (usually on the order of \(1e-8\). Interestingly, without the square root operation, the algorithm performs much worse.</p>

<p>As \(G_{t}\) contains the sum of the squares of the past gradients w.r.t. to all parameters \(\theta\) along its diagonal, we can now vectorize our implementation by performing an element-wise matrix-vector multiplication \(\odot\) between \(G_{t}\) and \(g_{t}\):</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>However, as Adagrad accumulates the squared gradients in the denominator and since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>

<h2 id="adadelta">Adadelta</h2>

<p>Adadelta [6] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \(w\).</p>

<p>Instead of inefficiently storing \(w\) previous squared gradients, they define the sum of gradients recursively as a decaying average of all past squared gradients. The running average \(E[g^2]_t\) at time step \(t\) then depends (as a fraction \(\gamma \) similarly to the Momentum term) only on the previous average and the current gradient:</p>

<p>\(E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t \).</p>

<p>Note that we can write an SGD update in terms of the parameter update vector \( \Delta \theta_t \):</p>

<p>\(\Delta \theta_t = - \eta \cdot g_{t, i}\)</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<p>Adagrad's parameter update vector is thus:</p>

<p>\( \Delta \theta_t = - \frac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>We now simply replace the diagonal matrix \(G_{t}\) with the decaying average over past squared gradients \(E[g^2]_t\):</p>

<p>\( \Delta \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>This can also be written in terms of the root mean squared (RMS) error criterion:</p>

<p>\( \Delta \theta_t = - \frac{\eta}{RMS[g]_{t}}\).</p>

<p>The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define an exponentially decaying average of squared parameter updates:</p>

<p>\(E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t \).</p>

<p>The root mean squared error of parameter updates is thus: </p>

<p>\(RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon} \).</p>

<p>Replacing the learning rate \(\eta \) in the previous update rule with the RMS of parameter updates finally yields the Adadelta update rule:</p>

<p>\( \Delta \theta_t = - \frac{RMS[\Delta \theta]_{t}}{RMS[g]_{t}} g_{t}\).</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<h2 id="rmsprop">RMSprop</h2>

<p>RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</p>

<p>RMSprop is identical to the first update vector of Adadelta, as it also divides the learning rate by an exponentially decaying average of squared gradients, while Hinton suggests \(\gamma\) to be set to 0.9:</p>

<p>\(E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t \).</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<h2 id="adam">Adam</h2>

<p>Adaptive Moment Estimation (Adam) [15] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients \(m_t\):</p>

<p>\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \).</p>

<p>\(v_t = \beta_2 v_{t-1} + (1 - \beta_1) g_t^2 \).</p>

<p>\(m_t\) and \(v_t\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \(m_t\) and \(v_t\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \(\beta_1\) and \(\beta_2\) are close to 1). </p>

<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>

<p>\(\hat{m}_t = \frac{m_t}{1 - \beta^t_1} \).</p>

<p>\(\hat{v}_t = \frac{v_t}{1 - \beta^t_2} \).</p>

<p>They then use these to update the parameters just as we have seen in Adadelta and RMSprop:</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \hat{m}_t\).</p>

<h2 id="visualizationofalgorithms">Visualization of algorithms</h2>

<p>The following two animations (Image credit: <a href="https://twitter.com/alecrad">Alec Radford</a>) provide some intuitions towards the optimization behaviour of the different optimization algorithms.</p>

<p>In Image 5, we see their behaviour on the contours of a loss surface over time. Note that Adagrad, Adadelta, and RMSprop almost immediately head off in the right direction and converge similarly fast, while Momentum and NAG are led off-track, evoking the image of a ball rolling down the hill. NAG, however, is quickly able to correct its course and head to the minimum.</p>

<p>Image 6 shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope. Notice that SGD, Momentum, and NAG have a hard time breaking symmetry, although the two latter eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 5: SGD optimization on loss surface contours</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 6: SGD optimization on saddle point</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>As we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam (as it extends RMSprop) are suitable and provide the best convergence for most applications.</p>

<h2 id="distributedsgd">Distributed SGD</h2>

<p>Given the ubiquity of large-scale data solutions and the availability of low-commodity clusters, parallelizing SGD to speed it up further is an obvious choice. SGD by itself is inherently sequential: Step-by-step, we progress further towards the minimum. Running it provides good convergence but can be slow particularly on large datasets. In contrast, running SGD asynchronously is faster, but suboptimal communication between workers can lead to poor convergence. The following are algorithms and architectures that have been proposed to optimize distributed SGD.</p>

<h3 id="downpoursgd">Downpour SGD</h3>

<p>Downpour SGD is an asynchronous variant of SGD that was used by Dean et al. [4] in their DistBelief framework (predecessor to TensorFlow) at Google. It runs multiple replicas of a model in parallel on subsets of the training data. These models send their updates to a parameter server, which is split across many machines. Each machine is responsible for storing and updating a fraction of the model's parameters. However, as replicas don't communicate with each other e.g. by sharing weights or updates, their parameters are continuously at risk of diverging, hindering convergence.</p>

<h3 id="delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</h3>

<p>McMahan and Streeter [13] extend AdaGrad to the parallel setting by developing delay-tolerant algorithms that not only adapt to past gradients, but also to the update delays. This has been shown to work well in practice.</p>

<h3 id="tensorflow">TensorFlow</h3>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> [14] is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It is based on their experience with DistBelief and is already used internally to perform computations on a large range of mobile devices as well as on large-scale distributed systems. For distributed execution, a computation graph is split into a subgraph for every device and communication takes place using Send/Receive node pairs. However, the open source version of TensorFlow currently does not support distributed functionality (see <a href="https://github.com/tensorflow/tensorflow/issues/23">here</a>).</p>

<h3 id="elasticaveragingsgd">Â Elastic Averaging SGD</h3>

<p>Zhang et al. [14] propose Elastic Averaging SGD (EASGD), which links the parameters of the workers of asynchronous SGD with an elastic force, i.e. a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. They show empirically that this increased capacity for exploration leads to improved performance by finding new local optima.</p>

<h1 id="additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</h1>

<p>Finally, we introduce some additional strategies that can be used alongside any of the previously   mentioned algorithms to further improve the performance of SGD.</p>

<h2 id="shufflingandcurriculumlearning">Shuffling and Curriculum Learning</h2>

<p>Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.</p>

<p>On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning [16]. </p>

<p>Zaremba and Sutskever [17] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which shorts examples by increasing difficulty.</p>

<h2 id="batchnormalization">Batch normalization</h2>

<p>To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.</p>

<p>Batch normalization [18] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By normalization being a part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.</p>

<h2 id="earlystopping">Early stopping</h2>

<p>According to Geoff Hinton: "<em>Early stopping (is) beautiful free lunch</em>" (<a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf">NIPS 2015 Tutorial slides</a>, slide 63). You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we have initially looked at the three variants of gradient descent, among which mini-batch gradient descent is the most popular. We have then investigated algorithms that are most commonly used for optimizing SGD: Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, as well as different algorithms to optimize asynchronous SGD. Finally, we've considered other strategies to improve SGD such as shuffling and Curriculum Learning, Batch normalization, and Early Stopping. </p>

<p>I hoped that this blog post was able to provide you with some intuitions towards the behaviour of the different optimization algorithms. Are there any obvious algorithms to improve SGD that I've missed? What tricks are you using yourself to facilitate training with SGD? <strong>Let me know in the comments below.</strong></p>

<h1 id="references">References</h1>

<ol>
<li>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.  </li>
<li>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145â151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a>  </li>
<li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a>  </li>
<li>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, â¦ Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1â11. <a href="http://doi.org/10.1109/ICDAR.2011.95">http://doi.org/10.1109/ICDAR.2011.95</a>  </li>
<li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532â1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a>  </li>
<li>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a>  </li>
<li>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543â 547.  </li>
<li>Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="http://arxiv.org/abs/1212.0901">http://arxiv.org/abs/1212.0901</a>  </li>
<li>Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis.  </li>
<li>Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1â11. <a href="http://doi.org/10.1109/NNSP.1992.253713">http://doi.org/10.1109/NNSP.1992.253713</a>  </li>
<li>H. Robinds and S. Monro, âA stochastic approximation method,â Annals of Mathematical Statistics, vol. 22, pp. 400â407, 1951.  </li>
<li>Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1â9.  </li>
<li>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., â¦ Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems.  </li>
<li>Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1â24. Retrieved from <a href="http://arxiv.org/abs/1412.6651">http://arxiv.org/abs/1412.6651</a>  </li>
<li>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1â13.  </li>
<li>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41â48. <a href="http://doi.org/10.1145/1553374.1553380">http://doi.org/10.1145/1553374.1553380</a>  </li>
<li>Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1â25. Retrieved from <a href="http://arxiv.org/abs/1410.4615">http://arxiv.org/abs/1410.4615</a>  </li>
<li>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3.</li>
</ol>

<p>Image credit for cover photo: <a href="http://lossfunctions.tumblr.com/">Karpathy's beautiful loss functions tumblr</a></p>

<p><a name="fn:1">1</a> Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters \(d\).</p>]]></content:encoded></item></channel></rss>
