<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a PhD student in Natural Language Processing and a researcher at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.]]></description><link>http://sebastianruder.com/</link><generator>Ghost 0.7</generator><lastBuildDate>Sun, 10 Apr 2016 13:49:04 GMT</lastBuildDate><atom:link href="http://sebastianruder.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[On word embeddings - Part 1]]></title><description><![CDATA[The first post in a series about word embeddings. This post presents word embedding models in the context of language modeling and past research.]]></description><link>http://sebastianruder.com/word-embeddings-1/</link><guid isPermaLink="false">4c791259-51e7-4e35-a5b4-a588296a9de3</guid><category><![CDATA[deep learning]]></category><category><![CDATA[word embeddings]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 09 Apr 2016 10:09:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png" alt="On word embeddings - Part 1"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/word-embeddings-1/#abriefhistoryofwordembeddings">A brief history of word embeddings</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#wordembeddingmodels">Word embedding models</a>
<ul><li><a href="http://sebastianruder.com/word-embeddings-1/#anoteonlanguagemodeling">A note on language modeling</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#classicneurallanguagemodel">Classic neural language model</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#cwmodel">C&amp;W model</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#word2vec">Word2Vec</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#continuousbagofwordscbow">CBOW</a></li>
<li><a href="http://sebastianruder.com/word-embeddings-1/#skipgram">Skip-gram</a></li></ul></li>
</ul>

<p>Unsupervisedly learned word embeddings have been exceptionally successful in many NLP tasks and are frequently seen as something akin to a <em>silver bullet</em>. In fact, in many NLP architectures, they have almost completely replaced traditional distributional features such as Brown clusters and LSA features. </p>

<p>Proceedings of last year's <a href="https://aclweb.org/anthology/P/P15/">ACL</a> and <a href="https://aclweb.org/anthology/D/D15/">EMNLP</a> conferences have been dominated by word embeddings, with some people musing that <em>Embedding Methods in Natural Language Processing</em> was a more appropriate name for EMNLP due to the plethora of papers on word embeddings. This year's ACL features not <a href="https://sites.google.com/site/repl4nlp2016/">one</a> but <a href="https://sites.google.com/site/repevalacl16/">two</a> workshops on word embeddings.</p>

<p>Semantic relations between word embeddings seem nothing short of magical to the uninitiated and Deep Learning NLP talks frequently prelude with the notorious \(king - man + woman \approx queen \) slide, while <a href="http://cacm.acm.org/magazines/2016/3/198856-deep-or-shallow-nlp-is-breaking-out/fulltext">a recent article</a> in <em>Communications of the ACM</em> hails word embeddings as the primary reason for NLP's breakout.</p>

<p>This post will be the first in a series that aims to give an extensive overview of word embeddings showcasing why this hype may or may not be warranted. In the course of this review, we will try to connect the disperse literature on word embedding models, highlighting many models, applications and interesting features of word embeddings, with a focus on multilingual embedding models and word embedding evaluation tasks in later posts. <br>
This first post lays the foundations by presenting current word embeddings based on language modelling. While many of these models have been discussed at length, we hope that investigating and discussing their merits in the context of past and current research will provide new insights.</p>

<p>A brief note on nomenclature: In the following we will use the currently prevalent term <em>word embeddings</em> to refer to dense representations of words in a low-dimensional vector space. Interchangeable terms are <em>word vectors</em> and <em>distributed representations</em>. We will particularly focus on <em>neural word embeddings</em>, i.e. word embeddings learned by a neural network.</p>

<h1 id="abriefhistoryofwordembeddings">A brief history of word embeddings</h1>

<p>Since the 1990s, vector space models have been used in distributional semantics. During this time, many models for estimating continuous representations of words have been developed, including Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Have a look at <a href="https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/">this blog post</a> for a more detailed overview of distributional semantics history in the context of word embeddings.</p>

<p>Bengio et al. are the first ones to introduce the concept of word embeddings in 2003, by training them in a neural language model jointly with the model's parameters. First to show the utility of pre-trained word embeddings were arguably Collobert and Weston in 2008. Their landmark paper <em>A unified architecture for natural language processing</em> not only establishes word embeddings as a useful tool for downstream tasks, but also introduces a neural network architecture that forms the foundation for many current approaches. However, the eventual popularization of word embeddings can be attributed to Mikolov et al. in 2013 who made word2vec available, a toolkit that allows the seamless training and use of pre-trained embeddings. In 2014, Pennington et al. released GloVe, a competitive set of pre-trained word embeddings, signalling that word embeddings had reached the main stream.</p>

<p>Word embeddings are one of the few currently successful applications of unsupervised learning. Their main benefit arguably is that they don't require expensive annotation, but can be derived from large unannotated corpora that are readily available. Pre-trained embeddings can then be used in downstream tasks that use small amounts of labeled data.</p>

<h1 id="wordembeddingmodels">Word embedding models</h1>

<p>Naturally, every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as <em>Embedding Layer</em>.</p>

<p>The main difference between such a network that produces word embeddings as a by-product and a method such as word2vec whose explicit goal is the generation of word embeddings is its computational complexity. Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary. This is the main reason why it took until 2013 for word embeddings to explode onto the NLP stage; computational complexity is a key trade-off for word embedding models and will be a recurring theme in our review.</p>

<p>Another difference is that methods such as word2vec and GloVe aim to produce general-purpose word embeddings that can be used in many downstream tasks and that encode general semantic relationships, while word embeddings produced by a regular neural network are specific to the task it is applied to. Note that a task that relies on semantically coherent representations such as language modelling will produce similar embeddings to word embedding models, which we will investigate in the next chapter.</p>

<p>To facilitate comparison between models, we enforce the following notational standards: We assume a training corpus containing a sequence of \(T\) training words \(w_1, w_2, w_3, \cdots, w_T\) that belong to a vocabulary \(V\) whose size is \(|V|\). Our models generally consider a context of \( n \) words. We optimize an objective function \(J_\theta\) with regard to our model parameters \(\theta\) and our model outputs some score \(f_\theta(x)\) for every input \( x \). Finally, our vectors have some dimensionality \(d\). </p>

<h2 id="anoteonlanguagemodelling">A note on language modelling</h2>

<p>Word embedding models are quite closely intertwined with language models. The quality of language models is measured based on their ability to learn a probability distribution over a vocabulary. In fact, many state-of-the-art word embedding models try to predict the next word in a sequence to some extent. We will talk about this in more details in the following sections.</p>

<p>Before we thus get into the gritty details of word embedding models, we will briefly introduce some language modelling fundamentals.</p>

<p>Language models generally try to compute the probability of a word \(w_t\) given its \(n\) previous words, i.e. \(p(w_t \: | \: w_{t-1} , \cdots w_{t-n+1})\). By applying the chain rule together with the Markov assumption, we can approximate the product of a whole sentence or document by the product of the probabilities of each word given its \(n\) previous words:</p>

<p>\(p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) \).</p>

<p>In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams: <br>
\( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{count(w_{t-n+1}, \cdots , w_{t-1},w_t)}{count({w_{t-n+1}, \cdots , w_{t-1}})}\).</p>

<p>Setting \(n = 2\) yields bigram probabilities, while \(n = 5\) together with Kneser-Ney smoothing leads to smoothed 5-gram models that have been found to be a strong baseline for language modelling. For more details, you can refer to <a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf">these slides</a> from Stanford.</p>

<p>In neural networks, we achieve the same objective using the well-known softmax layer:</p>

<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top e_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top e_{w_i}})} \).</p>

<p>The inner product \( h^\top e_{w_t} \) computes the (unnormalized) log-probability of word \( w_t \), which we normalize by the sum of the log-probabilities of all words in \( V \). \(h\) is the output vector of the penultimate network layer, i.e. a representation of what the network has seen, e.g. an LSTM's state, while \(e_w\) is a representation of the word \(w\). Note that even though \(e_w\) represents the word \(w\) in some way, it is generally learned separately from the word embedding of \(w\), as the multiplications both vectors are involved in differ.</p>

<p>Note that as the output layer of the neural network, we need to calculate the probability of every word \( w \). To do this efficiently, we perform a matrix multiplication between \(h\) and a weight matrix whose rows consist of \(e_w\) of all words \(w\) in \(V\). We then feed the resulting vector, which is often referred to as a logit, i.e. the output of a previous layer that is not a probability, with \(d = |V|\) into the softmax. The softmax layer then "squashes" the vector to a probability distribution over the words in \(V\).</p>

<p>Note that the softmax layer (in contrast to the previous n-gram calculations) only implicitly takes into account \(n\) previous words: LSTMs, which are typically used for neural language models, encode these in their state \(h\), while Bengio's neural language model, which we will see in the next chapter, feeds the previous \(n\) words through a feed-forward layer.</p>

<p>Keep this softmax layer in mind, as many of the subsequent word embedding models will use it in some fashion.</p>

<p>Using this softmax layer, the model then tries to maximize the probability of predicting the correct word at every timestep \( t \). The whole model thus tries to maximize the averaged log probability of the whole corpus:</p>

<p>\(J_\theta = \frac{1}{T} \text{log} \space p(w_1 , \cdots , w_T)\).</p>

<p>Analogously, through application of the chain rule, it is usually trained to maximize the average of the log probabilities of all words in the corpus given their previous \( n \) words:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})\).</p>

<p>In order to predict the next word, we simply choose the word with the highest probability \(p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})\) at every time step \( t \). We can do this for instance to sample arbitrary text sequences as in <a href="https://github.com/karpathy/char-rnn">Karpathy's Char-RNN</a> or as part of a sequence prediction task, where an LSTM is used as the decoder.</p>

<h2 id="classicneurallanguagemodel"> Classic neural language model</h2>

<p>The classic neural language model proposed by  Bengio et al. [<sup id="fnref:1"><a href="http://sebastianruder.com/word-embeddings-1/#fn:1" rel="footnote">1</a></sup>] in 2003 consists of a one-hidden layer feed-forward neural network that predicts the next word in a sequence.</p>

<p>Their model maximizes what we've described above as the prototypical neural language model objective (we omit the regularization term for simplicity):</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space f(w_t , w_{t-1} , \cdots , w_{t-n+1})\).</p>

<p>\( f(w_t , w_{t-1} , \cdots , w_{t-n+1}) \) is the output of the model, i.e. the probability \( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) \) as computed by the softmax, where \(n \) is the number of previous words fed into the model.</p>

<p>Bengio et al. are one of the first to introduce what we now refer to as a word embedding, a real-valued word feature vector in \(\mathbb{R}\). Their architecture forms very much the prototype upon which current approaches have gradually improved. The general building blocks of their model, however, are still found in all current neural language and word embedding models. These are:</p>

<ol>
<li><strong>Embedding Layer</strong>: a layer that generates word embeddings by multiplying an index vector with a word embedding matrix;  </li>
<li><strong>Intermediate Layer(s)</strong>: one or more layers that produce an intermediate representation of the input, e.g. a fully-connected layer that applies a non-linearity to the concatenation of word embeddings of \(n\) previous words;  </li>
<li><strong>Softmax Layer</strong>: the final layer that produces a probability distribution over words in \(V\).</li>
</ol>

<p>Additionally, Bengio et al. identify two issues that lie at the heart of current state-of-the-art-models:</p>

<ul>
<li>They remark that <strong>2.</strong> can be replaced by an LSTM, which is used by state-of-the-art neural language models [<sup id="fnref:6"><a href="http://sebastianruder.com/word-embeddings-1/#fn:6" rel="footnote">6</a></sup>, <sup id="fnref:7"><a href="http://sebastianruder.com/word-embeddings-1/#fn:7" rel="footnote">7</a></sup>].</li>
<li>They identify the final softmax layer (more precisely: the normalization term) as the network's main bottleneck, as the cost of computing the softmax is proportional to the number of words in \(V\), which is typically on the order of hundreds of thousands or millions.</li>
</ul>

<p>Finding ways to mitigate the computational cost associated with computing the softmax over a large vocabulary is thus one of the key challenges both in neural language models as well as in word embedding models.</p>

<h2 id="cwmodel"> C&amp;W model</h2>

<p>After Bengio et al.'s first steps in neural language models, research in word embeddings stagnated as computing power and algorithms did not yet allow the training of a large vocabulary.</p>

<p>Collobert and Weston [<sup id="fnref:4"><a href="http://sebastianruder.com/word-embeddings-1/#fn:4" rel="footnote">4</a></sup>] (thus C&amp;W) showcase in 2008 that word embeddings trained on a sufficiently large dataset carry syntactic and semantic meaning and improve performance on downstream tasks. They elaborate upon this in their 2011 paper [<sup id="fnref:8"><a href="http://sebastianruder.com/word-embeddings-1/#fn:8" rel="footnote">8</a></sup>].</p>

<p>Their solution to avoid computing the expensive softmax is to use a different objective function: Instead of the cross-entropy criterion of Bengio et al., which maximizes the probability of the next word given the previous words, Collobert and Weston train a network to output a higher score \(f_\theta\) for a correct word sequence (a probable word sequence in Bengio's model) than for an incorrect one. For this purpose, they use a pairwise ranking criterion, which looks like this:</p>

<p>\(J_\theta\ = \sum\limits_{x \in X} \sum\limits_{w \in V} \text{max} \lbrace 0, 1 - f_\theta(x) + f_\theta(x^{(w)}) \rbrace \).</p>

<p>They sample correct windows \(x\) containing \(n\) words from the set of all possible windows \(X\) in their corpus. For each window \(x\), they then produce a corrupted, incorrect version \(x^{(w)}\) by replacing \(x\)'s centre word with another word \(w\) from \(V\). Their objective now maximises the distance between the scores output by the model for the correct and the incorrect window with a margin of \(1\).</p>

<p>The resulting language model produces embeddings that already possess many of the relations word embeddings have become known for, e.g. countries are clustered close together and syntactically similar words occupy similar locations in the vector space. While their ranking objective eliminates the complexity of the softmax, they keep the intermediate fully-connected hidden layer (<strong>2.</strong>) of Bengio et al. around, which constitutes another source of expensive computation. Partially due to this, their full model trains for seven weeks in total with \(|V| = 130000\).</p>

<h2 id="word2vec"> Word2Vec</h2>

<p>Let us now introduce arguably the most popular word embedding model, the model that launch'd a thousand word embedding papers: word2vec, the subject of two papers by Mikolov et al. in 2013. In their first paper [<sup id="fnref:2"><a href="http://sebastianruder.com/word-embeddings-1/#fn:2" rel="footnote">2</a></sup>], they propose two architectures for learning word embeddings that are computationally less expensive than previous models. In their second paper [<sup id="fnref:3"><a href="http://sebastianruder.com/word-embeddings-1/#fn:3" rel="footnote">3</a></sup>], they improve upon these models by employing additional strategies to enhance training speed and accuracy.</p>

<p>These architectures offer two main benefits over the C&amp;W model and Bengio's language model:</p>

<ul>
<li>They do away with the expensive hidden layer.</li>
<li>They enable the language model to take additional context into account.</li>
</ul>

<p>As we will later show, the success of their model is not only due to these changes, but especially due to certain training strategies.</p>

<p>In the following, we will look at both of these architectures:</p>

<h3 id="continuousbagofwordscbow">Continuous bag-of-words (CBOW)</h3>

<p>While a language model is only able to look at the past words for its predictions, as it is evaluated on its ability to predict each next word in the corpus, a model that just aims to generate accurate word embeddings does not suffer from this restriction. Mikolov et al. thus use both the \(n\) words before and after the target word \( w_t \) to predict it. They call this continuous bag-of-words (CBOW), as it uses continuous representations whose order is of no importance.</p>

<p>The objective function of CBOW in turn is only slightly different than the language model one:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})\).</p>

<p>Instead of feeding \( n \) previous words into the model, the model receives a window of \( n \) words around the target word \( w_t \) at each time step \( t \).</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/02/cbow.png" style="width: 90%; height: 90%" title="SGD without momentum" alt="On word embeddings - Part 1">
<figcaption>Image 1: Continuous bag-of-words</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/02/skip-gram.png" style="width: 90%; height: 90%" title="SGD with momentum" alt="On word embeddings - Part 1">
<figcaption>Image 2: Skip-gram</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<h3 id="skipgram">Skip-gram</h3>

<p>While CBOW can be seen as a precognitive language model, skip-gram turns the language model objective on its head: Instead of using the surrounding words to predict the centre word as with CBOW, skip-gram uses the centre word to predict the surrounding words.</p>

<p>The skip-gram objective thus sums the log probabilities of the surrounding \( n \) words  to the left and to the right of the target word \( w_t \) to produce the following objective:</p>

<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)\).</p>

<p>To gain a better intuition of how the skip-gram model computes \( p(w_{t+j} \: | \: w_t) \), let's recall the definition of our softmax:</p>

<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top e_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top e_{w_i}})} \).</p>

<p>Instead of computing the probability of the target word \( w_t \) given its previous words, we calculate the probability of the surrounding word \( w_{t+j} \) given \( w_t \). We can thus simply replace these variables in the equation:</p>

<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top e_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top e_{w_i}})} \).</p>

<p>As the skip-gram architecture does not contain a hidden layer that produces an intermediate state vector \(h\), \(h\) is simply the word embedding of the input word \(w_t\), which we will refer to as \(v_{w_t}\). This also makes it clearer why we want to have different representations for input embeddings \(v_w\) and output embeddings \(e_w\), as we would otherwise multiply the word embedding by itself. Replacing \(h \) with \(v_{w_t}\) yields:</p>

<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({v^\top_{w_t} e_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({v^\top_{w_t} e_{w_i}})} \).</p>

<p>Note that the notation in Mikolov's paper differs slightly from ours, as they denote the centre word with \( w_I \), the surrounding words with \( w_O \), and the output word embeddings with \(v_{w}'\). If we replace \( w_t \) with \( w_I \), \( w_{t+j} \) with \( w_O \), \( e_w \) with \(v_{w}\), and swap the vectors in the inner product due to its commutativity, we arrive at the softmax notation in their paper:</p>

<p>\(p(w_O|w_I) = \dfrac{\text{exp}(v'^\top_{w_O} v_{w_I})}{\sum^V_{w=1}\text{exp}(v'^\top_{w} v_{w_I})}\).</p>

<p>In the next post, we will discuss different ways to approximate the expensive softmax as well as key training decisions that account for much of skip-gram's success. We will also introduce GloVe [<sup id="fnref:5"><a href="http://sebastianruder.com/word-embeddings-1/#fn:5" rel="footnote">5</a></sup>], a word embedding model based on matrix factorisation and discuss the link between word embeddings and methods from distributional semantics.</p>

<p>Credit for the post image goes to <a href="http://colah.github.io/">Christopher Olah</a>.</p>

<p>Did I miss anything? <strong>Let me know in the comments below.</strong></p>

<h1 id="references">References</h1>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. <a href="http://doi.org/10.1162/153244303322533223">http://doi.org/10.1162/153244303322533223</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:1" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:2"><p>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://sebastianruder.com/word-embeddings-1/#fnref:2" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:3"><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://sebastianruder.com/word-embeddings-1/#fnref:3" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:4"><p>Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. <a href="http://doi.org/10.1145/1390156.1390177">http://doi.org/10.1145/1390156.1390177</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:4" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:5"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:5" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:6"><p>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615">http://arxiv.org/abs/1508.06615</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:6" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:7"><p>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410">http://arxiv.org/abs/1602.02410</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:7" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:8"><p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from <a href="http://arxiv.org/abs/1103.0398">http://arxiv.org/abs/1103.0398</a> <a href="http://sebastianruder.com/word-embeddings-1/#fnref:8" title="return to article">↩</a></p></li></ol></div>]]></content:encoded></item><item><title><![CDATA[An overview of gradient descent optimization algorithms]]></title><description><![CDATA[This blog post looks at variants of gradient descent and the algorithms that are commonly used to optimize them.]]></description><link>http://sebastianruder.com/optimizing-gradient-descent/</link><guid isPermaLink="false">f991a148-274f-497e-97b9-598455211f57</guid><category><![CDATA[optimization]]></category><category><![CDATA[deep learning]]></category><category><![CDATA[sgd]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 19 Jan 2016 14:20:00 GMT</pubDate><media:content url="u=http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" medium="image"/><content:encoded><![CDATA[<img src="http://sebastianruder.com/content/images/2016/01/loss_function_image_tumblr.png" alt="An overview of gradient descent optimization algorithms"><p>Table of contents:</p>

<ul>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentvariants">Gradient descent variants</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchgradientdescent">Batch gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#stochasticgradientdescent">Stochastic gradient descent</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#minibatchgradientdescent">Mini-batch gradient descent</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#challenges">Challenges</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#momentum">Momentum</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#nesterovacceleratedgradient">Nesterov accelerated gradient</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adagrad">Adagrad</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adadelta">Adadelta</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#rmsprop">RMSprop</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#adam">Adam</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#visualizationofalgorithms">Visualization of algorithms</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#whichoptimizertochoose">Which optimizer to choose?</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#parallelizinganddistributingsgd">Parallelizing and distributing SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#hogwild">Hogwild!</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#downpoursgd">Downpour SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#tensorflow">TensorFlow</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#elasticaveragingsgd">Elastic Averaging SGD</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</a>
<ul><li><a href="http://sebastianruder.com/optimizing-gradient-descent/#shufflingandcurriculumlearning">Shuffling and Curriculum Learning</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#batchnormalization">Batch normalization</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#earlystopping">Early Stopping</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#gradientnoise">Gradient noise</a></li></ul></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#conclusion">Conclusion</a></li>
<li><a href="http://sebastianruder.com/optimizing-gradient-descent/#references">References</a></li>
</ul>

<p>Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. <a href="http://lasagne.readthedocs.org/en/latest/modules/updates.html">lasagne's</a>, <a href="http://caffe.berkeleyvision.org/tutorial/solver.html">caffe's</a>, and <a href="http://keras.io/optimizers/">keras'</a> documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.</p>

<p>This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training. Subsequently, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent.</p>

<p>Gradient descent is a way to minimize an objective function \(J(\theta)\) parameterized by a model's parameters \(\theta \in \mathbb{R}^d \) by updating the parameters in the opposite direction of the gradient of the objective function \(\nabla_\theta J(\theta)\) w.r.t. to the parameters. The learning rate \(\eta\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley. If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks <a href="http://cs231n.github.io/optimization-1/">here</a>.</p>

<h1 id="gradientdescentvariants"> Gradient descent variants</h1>

<p>There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.</p>

<h2 id="batchgradientdescent">Batch gradient descent</h2>

<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \(\theta\) for the entire training dataset:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta)\).</p>

<p>As we need to calculate the gradients for the whole dataset to perform just <em>one</em> update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model <em>online</em>, i.e. with new examples on-the-fly.</p>

<p>In code, batch gradient descent looks something like this:</p>

<pre><code class="language-python">for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad</code></pre>

<p>For a pre-defined number of epochs,  we first compute the gradient vector <code class="language-python">weights_grad</code> of the loss function for the whole dataset w.r.t. our parameter vector <code class="language-python">params</code>.  Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. If you derive the gradients yourself, then gradient checking is a good idea. (See <a href="http://cs231n.github.io/neural-networks-3/">here</a> for some great tips on how to check gradients properly.)</p>

<p>We then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform. Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.</p>

<h2 id="stochasticgradientdescent">Stochastic gradient descent</h2>

<p>Stochastic gradient descent (SGD) in contrast performs a parameter update for <em>each</em> training example \(x^{(i)}\) and label \(y^{(i)}\):</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})\).</p>

<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. <br>
SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in Image 1.</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/sgd_fluctuation.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 1: SGD fluctuation (Source: <a href="https://upload.wikimedia.org/wikipedia/en/f/f3/Stogra.png">Wikipedia</a>)</figcaption>  
</figure>

<p>While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. <br>
Its code fragment simply adds a loop over the training examples and evaluates the gradient w.r.t. each example. Note that we shuffle the training data at every epoch as explained in <a href="http://sebastianruder.com/optimizing-gradient-descent/#shufflingandcurriculumlearning">this section</a>.</p>

<pre><code class="language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad</code></pre>

<h2 id="minibatchgradientdescent">Mini-batch gradient descent</h2>

<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \(n\) training examples:</p>

<p>\(\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\).</p>

<p>This way, it <em>a)</em> reduces the variance of the parameter updates, which can lead to more stable convergence; and <em>b)</em> can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. Note: In modifications of SGD in the rest of this post, we leave out the parameters \(x^{(i:i+n)}; y^{(i:i+n)}\) for simplicity.</p>

<p>In code, instead of iterating over examples, we now iterate over mini-batches of size 50:</p>

<pre><code class="language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad</code></pre>

<h1 id="challenges">Challenges</h1>

<p>Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:</p>

<ul>
<li><p>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p></li>
<li><p>Learning rate schedules [<sup id="fnref:11"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:11" rel="footnote">11</a></sup>] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [<sup id="fnref:10"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:10" rel="footnote">10</a></sup>].</p></li>
<li><p>Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</p></li>
<li><p>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [<sup id="fnref:19"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:19" rel="footnote">19</a></sup>] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p></li>
</ul>

<h1 id="gradientdescentoptimizationalgorithms">Gradient descent optimization algorithms</h1>

<p>In the following, we will outline some algorithms that are widely used by the deep learning community to deal with the aforementioned challenges. We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton's method</a>.</p>

<h2 id="momentum">Momentum</h2>

<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [<sup id="fnref:1"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:1" rel="footnote">1</a></sup>], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image 2.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/without_momentum.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 2: SGD without momentum</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2015/12/with_momentum.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 3: SGD with momentum</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>Momentum [<sup id="fnref:2"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:2" rel="footnote">2</a></sup>] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Image 3. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)\).</p>

<p>\(\theta = \theta - v_t\).</p>

<p>Note: Some implementations exchange the signs in the equations. The momentum term \(\gamma\) is usually set to 0.9 or a similar value.</p>

<p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \(\gamma &lt; 1\)). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>

<h1 id="nesterovacceleratedgradient">Nesterov accelerated gradient</h1>

<p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>

<p>Nesterov accelerated gradient (NAG) [<sup id="fnref:7"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:7" rel="footnote">7</a></sup>] is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \(\gamma v_{t-1}\) to move the parameters \(\theta\). Computing \( \theta - \gamma v_{t-1} \) thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters \(\theta\) but w.r.t. the approximate future position of our parameters:</p>

<p>\(v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} )\).</p>

<p>\(\theta = \theta - v_t\).</p>

<p>Again, we set the momentum term \(\gamma\) to a value of around 0.9. While Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks [<sup id="fnref:8"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:8" rel="footnote">8</a></sup>].</p>

<figure>  
      <img src="http://sebastianruder.com/content/images/2016/01/nesterov_update_vector.png" style="width: 50%; height: 50%" title="SGD fluctuation" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 4: Nesterov update (Source: <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">G. Hinton's lecture 6c</a>)</figcaption>  
</figure>

<p>Refer to <a href="http://cs231n.github.io/neural-networks-3/">here</a> for another explanation about the intuitions behind NAG, while Ilya Sutskever gives a more detailed overview in his PhD thesis [<sup id="fnref:9"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:9" rel="footnote">9</a></sup>].</p>

<p>Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.</p>

<h2 id="adagrad">Adagrad</h2>

<p>Adagrad [<sup id="fnref:3"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:3" rel="footnote">3</a></sup>] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. Dean et al. [<sup id="fnref:4"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:4" rel="footnote">4</a></sup>] have found that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, which -- among other things -- learned to <a href="http://www.wired.com/2012/06/google-x-neural-network/">recognize cats in Youtube videos</a>. Moreover, Pennington et al. [<sup id="fnref:5"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:5" rel="footnote">5</a></sup>] used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.</p>

<p>Previously, we performed an update for all parameters \(\theta\) at once as every parameter \(\theta_i\) used the same learning rate \(\eta\). As Adagrad uses a different learning rate for every parameter \(\theta_i\) at every time step \(t\), we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we set \(g_{t, i}\) to be the gradient of the objective function w.r.t. to the parameter \(\theta_i\) at time step \(t\):</p>

<p>\(g_{t, i} = \nabla_\theta J( \theta_i )\).</p>

<p>The SGD update for every parameter \(\theta_i\) at each time step \(t\) then becomes:</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}\).</p>

<p>In its update rule, Adagrad modifies the general learning rate \(\eta\) at each time step \(t\) for every parameter \(\theta_i\) based on the past gradients that have been computed for \(\theta_i\):</p>

<p>\(\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}\).</p>

<p>\(G_{t} \in \mathbb{R}^{d \times d} \) here is a diagonal matrix where each diagonal element \(i, i\) is the sum of the squares of the gradients w.r.t. \(\theta_i\) up to time step \(t\) <sup id="fnref:24"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:24" rel="footnote">24</a></sup>, while \(\epsilon\) is a smoothing term that avoids division by zero (usually on the order of \(1e-8\)). Interestingly, without the square root operation, the algorithm performs much worse.</p>

<p>As \(G_{t}\) contains the sum of the squares of the past gradients w.r.t. to all parameters \(\theta\) along its diagonal, we can now vectorize our implementation by performing an element-wise matrix-vector multiplication \(\odot\) between \(G_{t}\) and \(g_{t}\):</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.</p>

<p>Adagrad's main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>

<h2 id="adadelta">Adadelta</h2>

<p>Adadelta [<sup id="fnref:6"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:6" rel="footnote">6</a></sup>] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \(w\).</p>

<p>Instead of inefficiently storing \(w\) previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average \(E[g^2]_t\) at time step \(t\) then depends (as a fraction \(\gamma \) similarly to the Momentum term) only on the previous average and the current gradient:</p>

<p>\(E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t \).</p>

<p>We set \(\gamma\) to a similar value as the momentum term, around 0.9. For clarity, we now rewrite our vanilla SGD update in terms of the parameter update vector \( \Delta \theta_t \):</p>

<p>\(\Delta \theta_t = - \eta \cdot g_{t, i}\).</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<p>The parameter update vector of Adagrad that we derived previously thus takes the form:</p>

<p>\( \Delta \theta_t = - \frac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\).</p>

<p>We now simply replace the diagonal matrix \(G_{t}\) with the decaying average over past squared gradients \(E[g^2]_t\):</p>

<p>\( \Delta \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>As the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:</p>

<p>\( \Delta \theta_t = - \frac{\eta}{RMS[g]_{t}}\).</p>

<p>The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:</p>

<p>\(E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t \).</p>

<p>The root mean squared error of parameter updates is thus: </p>

<p>\(RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon} \).</p>

<p>Replacing the learning rate \(\eta \) in the previous update rule with the RMS of parameter updates finally yields the Adadelta update rule:</p>

<p>\( \Delta \theta_t = - \frac{RMS[\Delta \theta]_{t}}{RMS[g]_{t}} g_{t}\).</p>

<p>\(\theta_{t+1} = \theta_t + \Delta \theta_t \).</p>

<p>With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.</p>

<h2 id="rmsprop">RMSprop</h2>

<p>RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</p>

<p>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:</p>

<p>\(E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t \).</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\).</p>

<p>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \(\gamma\) to be set to 0.9, while a good default value for the learning rate \(\eta\) is 0.001.</p>

<h2 id="adam">Adam</h2>

<p>Adaptive Moment Estimation (Adam) [<sup id="fnref:15"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:15" rel="footnote">15</a></sup>] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients \(m_t\), similar to momentum:</p>

<p>\(m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \).</p>

<p>\(v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \).</p>

<p>\(m_t\) and \(v_t\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \(m_t\) and \(v_t\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \(\beta_1\) and \(\beta_2\) are close to 1). </p>

<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>

<p>\(\hat{m}_t = \frac{m_t}{1 - \beta^t_1} \).</p>

<p>\(\hat{v}_t = \frac{v_t}{1 - \beta^t_2} \).</p>

<p>They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:</p>

<p>\(\theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t + \epsilon}} \hat{m}_t\).</p>

<p>They propose default values of 0.9 for \(\beta_1\), 0.999 for \(\beta_2\), and \(10^{-8}\) for \(\epsilon\). They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.</p>

<h2 id="visualizationofalgorithms">Visualization of algorithms</h2>

<p>The following two animations (Image credit: <a href="https://twitter.com/alecrad">Alec Radford</a>) provide some intuitions towards the optimization behaviour of the presented optimization algorithms.</p>

<p>In Image 5, we see their behaviour on the contours of a loss surface over time. Note that Adagrad, Adadelta, and RMSprop almost immediately head off in the right direction and converge similarly fast, while Momentum and NAG are led off-track, evoking the image of a ball rolling down the hill. NAG, however, is quickly able to correct its course due to its increased responsiveness by looking ahead and heads to the minimum.</p>

<p>Image 6 shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG have a hard time breaking symmetry, although the two latter eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope.</p>

<table>  
  <tr>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD without momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 5: SGD optimization on loss surface contours</figcaption>  
</figure>  
    </td>
    <td style="padding:1px">
      <figure>
      <img src="http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif" style="width: 90%; height: 90%" title="SGD with momentum" alt="An overview of gradient descent optimization algorithms">
<figcaption>Image 6: SGD optimization on saddle point</figcaption>  
</figure>  
    </td>
  </tr>
</table>

<p>As we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam are most suitable and provide the best convergence for these scenarios.</p>

<h2 id="whichoptimizertouse">Which optimizer to use?</h2>

<p>So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.</p>

<p>In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [<sup id="fnref:15"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:15" rel="footnote">15</a></sup>] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.</p>

<p>Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.</p>

<h1 id="parallelizinganddistributingsgd">Parallelizing and distributing SGD</h1>

<p>Given the ubiquity of large-scale data solutions and the availability of low-commodity clusters, distributing SGD to speed it up further is an obvious choice. <br>
SGD by itself is inherently sequential: Step-by-step, we progress further towards the minimum. Running it provides good convergence but can be slow particularly on large datasets. In contrast, running SGD asynchronously is faster, but suboptimal communication between workers can lead to poor convergence. Additionally, we can also parallelize SGD on one machine without the need for a large computing cluster. The following are algorithms and architectures that have been proposed to optimize parallelized and distributed SGD.</p>

<h2 id="hogwild">Hogwild!</h2>

<p>Niu et al. [<sup id="fnref:23"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:23" rel="footnote">23</a></sup>] introduce an update scheme called Hogwild! that allows performing SGD updates in parallel on CPUs. Processors are allowed to access shared memory without locking the parameters. This only works if the input data is sparse, as each update will only modify a fraction of all parameters. They show that in this case, the update scheme achieves almost an optimal rate of convergence, as it is unlikely that processors will overwrite useful information.</p>

<h2 id="downpoursgd">Downpour SGD</h2>

<p>Downpour SGD is an asynchronous variant of SGD that was used by Dean et al. [<sup id="fnref:4"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:4" rel="footnote">4</a></sup>] in their DistBelief framework (predecessor to TensorFlow) at Google. It runs multiple replicas of a model in parallel on subsets of the training data. These models send their updates to a parameter server, which is split across many machines. Each machine is responsible for storing and updating a fraction of the model's parameters. However, as replicas don't communicate with each other e.g. by sharing weights or updates, their parameters are continuously at risk of diverging, hindering convergence.</p>

<h2 id="delaytolerantalgorithmsforsgd">Delay-tolerant Algorithms for SGD</h2>

<p>McMahan and Streeter [<sup id="fnref:12"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:12" rel="footnote">12</a></sup>] extend AdaGrad to the parallel setting by developing delay-tolerant algorithms that not only adapt to past gradients, but also to the update delays. This has been shown to work well in practice.</p>

<h2 id="tensorflow">TensorFlow</h2>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> [<sup id="fnref:13"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:13" rel="footnote">13</a></sup>] is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It is based on their experience with DistBelief and is already used internally to perform computations on a large range of mobile devices as well as on large-scale distributed systems. For distributed execution, a computation graph is split into a subgraph for every device and communication takes place using Send/Receive node pairs. However, the open source version of TensorFlow currently does not support distributed functionality (see <a href="https://github.com/tensorflow/tensorflow/issues/23">here</a>).</p>

<h2 id="elasticaveragingsgd"> Elastic Averaging SGD</h2>

<p>Zhang et al. [<sup id="fnref:14"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:14" rel="footnote">14</a></sup>] propose Elastic Averaging SGD (EASGD), which links the parameters of the workers of asynchronous SGD with an elastic force, i.e. a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. They show empirically that this increased capacity for exploration leads to improved performance by finding new local optima.</p>

<h1 id="additionalstrategiesforoptimizingsgd">Additional strategies for optimizing SGD</h1>

<p>Finally, we introduce additional strategies that can be used alongside any of the previously mentioned algorithms to further improve the performance of SGD. For a great overview of some of some other common tricks, refer to [<sup id="fnref:22"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:22" rel="footnote">22</a></sup>].</p>

<h2 id="shufflingandcurriculumlearning">Shuffling and Curriculum Learning</h2>

<p>Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch. </p>

<p>On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning [<sup id="fnref:16"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:16" rel="footnote">16</a></sup>]. </p>

<p>Zaremba and Sutskever [<sup id="fnref:17"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:17" rel="footnote">17</a></sup>] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which shorts examples by increasing difficulty.</p>

<h2 id="batchnormalization">Batch normalization</h2>

<p>To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.</p>

<p>Batch normalization [<sup id="fnref:18"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:18" rel="footnote">18</a></sup>] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.</p>

<h2 id="earlystopping">Early stopping</h2>

<p>According to Geoff Hinton: "<em>Early stopping (is) beautiful free lunch</em>" (<a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf">NIPS 2015 Tutorial slides</a>, slide 63). You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.</p>

<h2 id="gradientnoise">Gradient noise</h2>

<p>Neelakantan et al. [<sup id="fnref:21"><a href="http://sebastianruder.com/optimizing-gradient-descent/#fn:21" rel="footnote">21</a></sup>] add noise that follows a Gaussian distribution \(N(0, \sigma^2_t\) to each gradient update:</p>

<p>\(g_{t, i} = g_{t, i} + N(0, \sigma^2_t)\).</p>

<p>They anneal the variance according to the following schedule:</p>

<p>\( \sigma^2_t = \frac{\eta}{(1 + t)^\gamma} \).</p>

<p>They show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima, which are more frequent for deeper models.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we have initially looked at the three variants of gradient descent, among which mini-batch gradient descent is the most popular. We have then investigated algorithms that are most commonly used for optimizing SGD: Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, as well as different algorithms to optimize asynchronous SGD. Finally, we've considered other strategies to improve SGD such as shuffling and curriculum learning, batch normalization, and early stopping.</p>

<p>I hope that this blog post was able to provide you with some intuitions towards the motivation and the behaviour of the different optimization algorithms. Are there any obvious algorithms to improve SGD that I've missed? What tricks are you using yourself to facilitate training with SGD? <strong>Let me know in the comments below.</strong></p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>Thanks to <a href="https://twitter.com/dennybritz">Denny Britz</a> and <a href="https://twitter.com/cesarsvs">Cesar Salgado</a> for reading drafts of this post and providing suggestions.</p>

<h1 id="references">References</h1>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:1" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:2"><p>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:2" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:3"><p>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:3" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:4"><p>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. <a href="http://doi.org/10.1109/ICDAR.2011.95">http://doi.org/10.1109/ICDAR.2011.95</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:4" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:5"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:5" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:6"><p>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:6" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:7"><p>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543– 547. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:7" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:8"><p>Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="http://arxiv.org/abs/1212.0901">http://arxiv.org/abs/1212.0901</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:8" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:9"><p>Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:9" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:10"><p>Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1–11. <a href="http://doi.org/10.1109/NNSP.1992.253713">http://doi.org/10.1109/NNSP.1992.253713</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:10" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:11"><p>H. Robinds and S. Monro, “A stochastic approximation method,” Annals of Mathematical Statistics, vol. 22, pp. 400–407, 1951. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:11" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:12"><p>Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1–9. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:12" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:13"><p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:13" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:14"><p>Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1–24. Retrieved from <a href="http://arxiv.org/abs/1412.6651">http://arxiv.org/abs/1412.6651</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:14" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:15"><p>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:15" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:16"><p>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41–48. <a href="http://doi.org/10.1145/1553374.1553380">http://doi.org/10.1145/1553374.1553380</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:16" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:17"><p>Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1–25. Retrieved from <a href="http://arxiv.org/abs/1410.4615">http://arxiv.org/abs/1410.4615</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:17" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:18"><p>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:18" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:19"><p>Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1–14. Retrieved from <a href="http://arxiv.org/abs/1406.2572">http://arxiv.org/abs/1406.2572</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:19" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:20"><p>Sutskever, I., &amp; Martens, J. (2013). On the importance of initialization and momentum in deep learning. <a href="http://doi.org/10.1109/ICASSP.2013.6639346">http://doi.org/10.1109/ICASSP.2013.6639346</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:20" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:21"><p>Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.06807">http://arxiv.org/abs/1511.06807</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:21" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:22"><p>LeCun, Y., Bottou, L., Orr, G. B., &amp; Müller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9–50. <a href="http://doi.org/10.1007/3-540-49430-8_2">http://doi.org/10.1007/3-540-49430-8_2</a> <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:22" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:23"><p>Niu, F., Recht, B., Christopher, R., &amp; Wright, S. J. (2011). Hogwild ! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1–22. <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:23" title="return to article">↩</a></p></li>
<li class="footnote" id="fn:24"><p>Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters \(d\). <a href="http://sebastianruder.com/optimizing-gradient-descent/#fnref:24" title="return to article">↩</a></p></li></ol></div>

<p>Image credit for cover photo: <a href="http://lossfunctions.tumblr.com/">Karpathy's beautiful loss functions tumblr</a></p>]]></content:encoded></item></channel></rss>
