
  <head>
    <title>Highlights of EMNLP 2017</title>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link rel="apple-touch-icon" sizes="57x57" href="../assets/img/apple-touch-icon-57x57.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="60x60" href="../assets/img/apple-touch-icon-60x60.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="72x72" href="../assets/img/apple-touch-icon-72x72.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="76x76" href="../assets/img/apple-touch-icon-76x76.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="114x114" href="../assets/img/apple-touch-icon-114x114.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="120x120" href="../assets/img/apple-touch-icon-120x120.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="144x144" href="../assets/img/apple-touch-icon-144x144.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="152x152" href="../assets/img/apple-touch-icon-152x152.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="180x180" href="../assets/img/apple-touch-icon-180x180.png?v=wAAv6Wqe6l">
<link rel="icon" type="image/png" href="../assets/img/favicon-32x32.png?v=wAAv6Wqe6l" sizes="32x32">
<link rel="icon" type="image/png" href="../assets/img/favicon-194x194.png?v=wAAv6Wqe6l" sizes="194x194">
<link rel="icon" type="image/png" href="../assets/img/favicon-96x96.png?v=wAAv6Wqe6l" sizes="96x96">
<link rel="icon" type="image/png" href="../assets/img/android-chrome-192x192.png?v=wAAv6Wqe6l" sizes="192x192">
<link rel="icon" type="image/png" href="../assets/img/favicon-16x16.png?v=wAAv6Wqe6l" sizes="16x16">
<link rel="manifest" href="../assets/img/manifest.json?v=wAAv6Wqe6l">
<link rel="shortcut icon" href="../assets/img/favicon.ico?v=wAAv6Wqe6l">
<meta name="msapplication-TileColor" content="#e74c3c">
<meta name="msapplication-TileImage" content="/assets/img/mstile-144x144.png?v=wAAv6Wqe6l">
<meta name="msapplication-config" content="/assets/img/browserconfig.xml?v=wAAv6Wqe6l">
<meta name="theme-color" content="#e74c3c">
    <link rel="stylesheet" type="text/css" href="../assets/css/uno-zen.css?v=8de49cd7a6">
    <link rel="canonical" href="http://ruder.io/highlights-emnlp-2017/">
    <meta name="referrer" content="origin">
    
    <meta property="og:site_name" content="Sebastian Ruder">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Highlights of EMNLP 2017">
    <meta property="og:description" content="This post gives an overview of highlights of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen.">
    <meta property="og:url" content="u=http://ruder.io/highlights-emnlp-2017/">
    <meta property="og:image" content="u=http://ruder.io/content/images/2017/09/emnlp_landscape.jpg">
    <meta property="article:published_time" content="2017-09-22T16:06:14.177Z">
    <meta property="article:modified_time" content="2017-09-24T13:41:31.436Z">
    <meta property="article:tag" content="nlp">
    <meta property="article:tag" content="natural language processing">
    <meta property="article:tag" content="word embeddings">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Highlights of EMNLP 2017">
    <meta name="twitter:description" content="This post gives an overview of highlights of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen.">
    <meta name="twitter:url" content="u=http://ruder.io/highlights-emnlp-2017/">
    <meta name="twitter:image:src" content="u=http://ruder.io/content/images/2017/09/emnlp_landscape.jpg">
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Sebastian Ruder",
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "url": "u=http://ruder.io/author/sebastian",
        "sameAs": null,
        "description": null
    },
    "headline": "Highlights of EMNLP 2017",
    "url": "u=http://ruder.io/highlights-emnlp-2017/",
    "datePublished": "2017-09-22T16:06:14.177Z",
    "dateModified": "2017-09-24T13:41:31.436Z",
    "image": "u=http://ruder.io/content/images/2017/09/emnlp_landscape.jpg",
    "keywords": "nlp, natural language processing, word embeddings",
    "description": "This post gives an overview of highlights of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen."
}
    </script>

    <meta name="generator" content="Ghost 0.7">
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="http://ruder.io/rss/">
    <script>
var open_button = '.nav-blog > a'
</script>
<script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
  </head>
  <body class="post-template tag-nlp tag-natural-language-processing tag-word-embeddings">
    <header id="menu-button" class="expanded">
      <a><i class="icon icon-list"></i></a>
    </header>
    <aside class="cover" style="background: url(../content/images/2017/05/imageedit_8_8459453433.jpg) center/cover no-repeat fixed">
  <div class="cover container">
    <div class="profile">
      <a id="avatar-link" title="link to homepage for Sebastian Ruder" href="http://ruder.io/#open">
        <img src="../content/images/2015/12/Seb_LinkedIn_Profile-.png" alt="Sebastian Ruder avatar" class="profile avatar rounded hvr-buzz-out">
        <h1 id="profile-title">Sebastian Ruder</h1>
        <h3 id="profile-resume"></h3>
      </a>

      <hr class="divider long">
      <p>I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</p>
      <hr class="divider short">
      <div class="navigation">
        <div class="profile contact">
          <nav class="navigation left">
  <ul class="links">
      <li class="nav-blog ">
        <a href="http://ruder.io/">Blog</a>
      </li>
      <li class="nav-about ">
        <a href="http://ruder.io/about/">About</a>
      </li>
      <li class="nav-papers ">
        <a href="http://ruder.io/publications/">Papers</a>
      </li>
      <li class="nav-news ">
        <a href="http://ruder.io/news">News</a>
      </li>
      <li class="nav-newsletter ">
        <a href="http://newsletter.ruder.io">Newsletter</a>
      </li>
      <li class="nav-faq ">
        <a href="http://ruder.io/faq">FAQ</a>
      </li>
  </ul>
</nav>

          
<nav class="navigation right">
  <ul class="social expanded">

  <!-- Twitter -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="http://twitter.com/seb_ruder" title="@seb_ruder on Twitter">
      <i class="icon icon-social-twitter"></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  <!-- Linkedin -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://www.linkedin.com/in/sebastianruder" title="sebastianruder on LinkedIn">
      <i class="icon icon-social-linkedin"></i>
      <span class="label">Linkedin</span>
    </a>
  </li>

  <!-- Github -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://github.com/sebastianruder" title="sebastianruder on Github">
      <i class="icon icon-social-github"></i>
      <span class="label">Github</span>
    </a>
  </li>

  <!-- E-mail -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="mailto:sebastian@ruder.io" title="send me an email">
      <i class="icon icon-mail"></i>
      <span class="label">Email</span>
    </a>
  </li>

  <!-- RSS -->
  <li class="social item hvr-grow-rotate">
    <a href="../rss/index.rss" title="Subscribe to RSS">
      <i class="icon icon-rss"></i>
      <span class="label">RSS</span>
    </a>
  </li>

  </ul>
</nav>
          <section class="icon icon-search" id="search-container">
  <hr class="divider short">
  <form id="search-form" action="https://www.google.com/#q=site:u=http://ruder.io">
    <input type="text" name="search" placeholder="Deep Learning, NLP, ..." id="search-field">
  </form>
</section>
        </div>
      </div>
    </div>
  </div>
</aside>
    <main>
      <section id="search-results"></section>
      <section class="content">
        

  <article class="post tag-nlp tag-natural-language-processing tag-word-embeddings">
    <header>
      <div class="post meta">
        <time datetime="22 Sep 2017">22 Sep 2017</time>
        <span class="post tags">in <a href="../tag/nlp/">nlp</a> <a href="../tag/natural-language-processing/">natural language processing</a> <a href="../tag/word-embeddings/">word embeddings</a></span>


        <span class="post reading-time"> ~ <span></span> read.</span>
      </div>
      <a alt="Tweet 'Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more'" href="https://twitter.com/intent/tweet?text=Highlights%20of%20EMNLP%202017%3A%20Exciting%20datasets%2C%20return%20of%20the%20clusters%2C%20and%20more%20%C2%BB&amp;hashtags=nlp,natural%20language%20processing,word%20embeddings&amp;url=http://ruder.io/highlights-emnlp-2017/">
        <img id="post-image" src="../content/images/2017/09/emnlp_landscape.jpg" alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more">
        <h1 class="icon-reverse icon-social-twitter-post" id="post-title">Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more</h1>
      </a>
    </header>

    <div id="post-content" class="post tag-nlp tag-natural-language-processing tag-word-embeddings">
      <p><em>This post originally appeared at the <a href="http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/">AYLIEN blog</a>.</em></p>

<p>I spent the past week at the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 in Copenhagen, Denmark. The conference handbook can be found <a href="http://emnlp2017.net/downloads/handbook.pdf">here</a> and the proceedings can be found <a href="http://aclweb.org/anthology/D/D17/">here</a>. Videos of the conference talks and presentations can be found <a href="https://ku.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%229042b495-7b6b-4169-a5a1-d250cc0ee4ec%22">here</a>.</p>

<p>The program consisted of two days of workshops and tutorials and three days of main conference. The conference was superbly organized, had a great venue, and a social event with fireworks.</p>

<figure>  
      <img src="../content/images/2017/09/emnlp_fireworks-1.jpg" style="width: 60%; height: 60%" title="EMNLP 2017 fireworks">
<figcaption>Figure 1: Fireworks at the social event</figcaption>  
</figure>

<p>225 long papers, 107 papers, and 9 TACL papers had been accepted, with a clear uptick of submissions compared to last year. The number of long and short paper submissions to EMNLP this year was even higher than those at ACL for the first time within the last 13 years, as can be seen in Figure 2.</p>

<figure>  
      <img src="../content/images/2017/09/long_short_paper_submissions_emnlp_2017.jpg" style="width: 60%; height: 60%" title="Long and short paper submissions EMNLP 2017">
<figcaption>Figure 2: Long and short paper submissions at ACL and EMNLP from 2004-2017</figcaption>  
</figure>

<p>In the following, I will outline my highlights and list some research papers that caught my eye.</p>

<p><strong>Exciting datasets</strong>   Evaluating your approach on CoNLL-2003 or PTB is appropriate for comparing against previous state-of-the-art, but kind of boring. The two following papers introduce datasets that allow you to test your model in more exciting settings:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1274.pdf">Durrett et al.</a> release a new domain adaptation dataset. The dataset evaluates models on their ability to identify products being bought and sold in online cybercrime forums.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf">Kutuzov et al.</a> evaluate their word embedding model on a new dataset that focuses on predicting insurgent armed groups based on geographical locations.</li>
<li>While he did not introduce a new dataset, Nando de Freitas made the point during <a href="http://emnlp2017.net/invited-speakers.html">his keynote</a> that the best environment for learning and evaluating language is simulation.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/nando_de_freitas_vision.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research">
<figcaption>Figure 3: Nando de Freitas’ vision for AI research</figcaption>  
</figure>

<p><strong>Return of the clusters</strong>   Brown clusters, an agglomerative, hierarchical clustering of word types based on contexts that was introduced in 1992 seem to come in vogue again. They were found to be particularly helpful for cross-lingual applications, while clusters were key features in several approaches:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1268.pdf">Mayhew et al.</a> found that Brown cluster features were an important signal for cross-lingual NER.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1308.pdf">Botha et al.</a> use word clusters as a key feature in their small, efficient feed-forward neural networks.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1070.pdf">Mekala et al.</a>’s new document representations cluster word embeddings, which give it an edge for text classification.</li>
<li>In his talk at the <a href="https://sites.google.com/view/sclem2017/home">SCLeM workshop</a>, Noah Smith cites the benefits of using Brown clusters as features for tasks such as POS tagging and sentiment analysis.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/noah_smith_benefits_clustering.jpg" style="width: 60%; height: 60%" title="Nando de Freitas' vision for AI research">
<figcaption>Figure 4: Noah Smith on the benefits of clustering in his invited talk at the SCLeM workshop</figcaption>  
</figure>

<p><strong>Distant supervision</strong>   Distant supervision can be leveraged to collect large amounts of noisy training data, which can be useful in many applications. Some papers used novel forms of distant supervision to create new corpora or to train a model more effectively:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1127.pdf">Lan et al.</a> use urls in tweets to collect a large corpus of paraphrase data. Paraphrase data is usually hard to create, so this approach facilitates the process significantly and enables a continuously expanding collection of paraphrases.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1169.pdf">Felbo et al.</a> show that training on fine-grained emoji detection is more effective for pre-training sentiment and emotion models. Previous approaches primarily pre-trained on positive and negative emoticons or emotion hashtags.</li>
</ul>

<p><strong>Data selection</strong>   The current generation of deep learning models is excellent at learning from data. However, we often do not pay much attention to the actual data our model is using. In many settings, we can improve upon the model by selecting the most relevant data:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1064.pdf">Fang et al.</a> reframe active learning as reinforcement learning and explicitly learn a data selection policy. Active learning is one of the best ways to create a model with as few annotations as possible; any improvement to this process is beneficial.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1148.pdf">Van der Wees et al.</a> introduce dynamic data selection for NMT, which varies the selected subset of the training data between different training epochs. This approach has the potential to reduce the training time of NMT models at comparable or better performance.</li>
<li>In <a href="http://aclweb.org/anthology/D/D17/D17-1038.pdf">my paper with Barbara Plank</a>, we use Bayesian Optimization to learn data selection policies for transfer learning and investigate how well these transfer across models, domains, and tasks. This approach brings us a step closer towards gaining a better understanding of what constitutes similarity between different tasks and domains.</li>
</ul>

<p><strong>Character-level models</strong>   Characters are nowadays used as standard features in most sequence models. The <a href="https://sites.google.com/view/sclem2017/home">Subword and Character-level Models in NLP workshop</a> discussed approaches in more detail, with invited talks on subword language models and character-level NMT. </p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1297.pdf">Schmaltz et al.</a> find that character-based sequence-to-sequence models outperform word-based models and models with character convolutions for sentence correction.</li>
<li><a href="https://ryancotterell.github.io/">Ryan Cotterell</a> gave a great, movie-inspired tutorial on combining the best of FSTs (cowboys) and sequence-to-sequence models (aliens) for string-to-string transduction. While evaluated on morphological segmentation, the tutorial raised awareness in an entertaining way that often the best of both worlds, i.e. a combination of traditional and neural approaches performs best.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/ryan_cotterell_fsts_seq2seq.jpg" style="width: 60%; height: 60%" title="Combining FSTs and seq2seq models">
<figcaption>Figure 5: Ryan Cotterell on combining FSTs and seq2seq models for string-to-string transduction  
</figcaption>  
</figure>

<p><strong>Word embeddings</strong>   Research in word embeddings has matured and now mainly tries to 1) address deficits of word2vec, such as its ability of dealing with OOV words; 2) extend it to new settings, e.g. modelling the relations of words over time; and 3) understand the induced representations better:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1010.pdf">Pinter et al.</a> propose an approach for generating OOV word embeddings by training a character-based BiLSTM to generate embeddings that are close to pre-trained ones. This approach is promising as it provides us with a more sophisticated way to deal with out-of-vocabulary words than replacing them with an <unk> token. </unk></li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1030.pdf">Herbelot and Baroni</a> slightly modify word2vec to allow it to learn embeddings for OOV words from few data.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1122.pdf">Rosin et al.</a> propose a model for analyzing <em>when</em> two words relate to each other.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1194.pdf">Kutuzov et al.</a> propose another model that analyzes how two words relate to each other over time.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1033.pdf">Hasan and Curry</a> improve the performance of word embeddings on word similarity tasks by re-embedding them in a manifold.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1311.pdf">Yang et al.</a> introduce a simple approach to learning cross-domain word embeddings. Creating embeddings tuned on a small, in-domain corpus is still a challenge, so it is nice to see more approaches addressing this pain point.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1307.pdf">Mimno and Thompson</a> try to understand the geometry of word2vec better. They show that the learned word embeddings are positioned diametrically opposite of their context vectors in the embedding space.</li>
</ul>

<p><strong>Cross-lingual</strong>   An increasing number of papers evaluate their methods on multiple languages. In addition, there was an excellent <a href="http://emnlp2017.net/tutorials/day2/xling_word_rep.html">tutorial on cross-lingual word representations</a>, which summarized and tried to unify much of the existing literature. Slides of the tutorial are available <a href="http://people.ds.cam.ac.uk/iv250/tutorial/xlingrep-tutorial.pdf">here</a>.</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1267.pdf">Malaviya et al.</a> train a many-to-one NMT to translate 1017 languages into English and use this model to predict information missing from typological databases. </li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1268.pdf">Mayhew et al.</a> introduce a cheap translation method for cross-lingual NER that only requires a bilingual dictionary. They even perform a case study on Uyghur, a truly low-resource language.</li>
<li><a href="http://www.aclweb.org/anthology/D/D17/D17-1301.pdf">Kim et al.</a> present a cross-lingual transfer learning model for POS tagging without parallel data. Parallel data is expensive to create and rarely available for low-resource languages, so this approach fills an important need.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1269.pdf">Vulic et al.</a> propose a new cross-lingual transfer method for inducing VerbNets for different languages. The method leverages vector space specialisation, an effective word embedding post-processing technique similar to retro-fitting.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1257.pdf">Braud et al.</a> propose a robust, cross-lingual discourse segmentation model that only relies on POS tags. They show that dependency information is less useful than expected; it is important to evaluate our models on multiple languages, so we do not overfit to features that are specific to analytic languages, such as English.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/anders_sogaard_crosslingual.jpg" style="width: 60%; height: 60%" title="Anders Søgaard on cross-lingual embeddings">
<figcaption>Figure 6: Anders Søgaard demonstrating the similarities between different cross-lingual embedding models at the cross-lingual representations tutorial  
</figcaption>  
</figure>

<p><strong>Summarization</strong>   <a href="https://summarization2017.github.io/">The Workshop on New Frontiers of Summarization</a> brought researchers together to discuss key issues related to automatic summarization. Much of the research on summarization sought to develop new datasets and tasks:</p>

<ul>
<li><a href="https://research.google.com/pubs/author39008.html">Katja Filippova</a> gave an interesting talk on sentence compression and passage summarization for Q&amp;A. She described how they went from syntax-based methods to Deep Learning.</li>
<li><a href="http://www.aclweb.org/anthology/W/W17/W17-4508.pdf">Volkse et al.</a> created a new summarization corpus by looking for ‘TL;DR’ on Reddit. This is another example of a creative use of distant supervision, leveraging information that is already contained in the data in order to create a new corpus.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1322.pdf">Falke and Gurevych</a> won the best resource paper award for creating a new summary corpus that is based on concept maps rather than textual summaries. The concept map can be explored using a <a href="https://github.com/UKPLab/emnlp2017-graphdocexplore">graph-based document exploration system</a>, which is available as a <a href="http://cmaps.ukp.informatik.tu-darmstadt.de/graph-doc-explorer/#!/">demo</a>.</li>
<li><a href="http://www.aclweb.org/anthology/W17-4504">Pasunuru et al.</a> use multi-task learning to improve abstractive summarization by leveraging entailment generation.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1222.pdf">Isonuma et al.</a> also use multi-task learning with document classification in conjunction with curriculum learning.</li>
<li><a href="http://www.aclweb.org/anthology/W/W17/W17-4512.pdf">Li et al.</a> propose a new task, reader-aware multi-document summarization, which uses comments of articles, along with a dataset for this task.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1065.pdf">Naranyan et al.</a> propose another new task, split and rephrase, which aims to split a complex sentence into a sequence of shorter sentences with the same meaning, and also release a new dataset.</li>
<li><a href="http://www.aclweb.org/anthology/W/W17/W17-4511.pdf">Ghalandari</a> revisits the traditional centroid-based method and proposes a new strong baseline for multi-document summarization.</li>
</ul>

<p><strong>Bias</strong>   Data and model-inherent bias is an issue that is receiving more attention in the community. Some papers investigate and propose methods to address the bias in certain datasets and evaluations:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1110.pdf">Chaganty et al.</a> investigate bias in the evaluation of knowledge base population models and propose an importance sampling-based evaluation to mitigate the bias.</li>
<li>Dan Jurafsky gave a truly insightful <a href="http://emnlp2017.net/invited-speakers.html">keynote</a> about his three year-long study analyzing the body camera recordings his team obtained from the Oakland police department for racial bias. Besides describing the first contemporary linguistic study of officer-community member interaction, he also provided entertaining insights on the language of food (cheaper restaurants use terms related to addiction, more expensive venues use language related to indulgence) and the challenges of interdisciplinary publishing. The entire keynote can be viewed <a href="https://ku.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4f45fc4c-e7d6-4743-b843-0aae2ae8b17e&amp;utm_campaign=Revue%20newsletter&amp;utm_medium=Newsletter&amp;utm_source=NLP%20News">here</a>. </li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1119.pdf">Dubossarsky et al.</a> analyze the bias in word representation models and propose that recently proposed laws of semantic change must be revised. </li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1319.pdf">Zhao et al.</a> won the best paper award for an approach using Lagrangian relaxation to inject constraints based on corpus-level label statistics. An important finding of their work is bias amplification: While some bias is inherent in all datasets, they observed that models trained on the data amplified its bias. While a gendered dataset might only contain women in 30% of examples, the situation at prediction time might thus be even more dire.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/bias_amplification.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification">
<figcaption>Figure 7: Zhao et al.’s proposed method for reducing bias amplification  
</figcaption>  
</figure>

<p><strong>Argument mining &amp; debate analysis</strong>   Argument mining is closely related to summarization. In order to summarize argumentative texts, we have to understand claims and their justifications. This research area had the <a href="https://argmining2017.wordpress.com/">4th Workshop on Argument Mining</a> dedicated to it:</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/W/W17/W17-5102.pdf">Hidey et al.</a> analyse the semantic types of claims (e.g. agreement, interpretation) and premises (ethos, logos, pathos) in the Subreddit <a href="https://www.reddit.com/r/changemyview/">Change My View</a>. This is another creative use of reddit to create a dataset and analyze linguistic patterns.</li>
<li><a href="http://www.aclweb.org/anthology/W/W17/W17-5106.pdf">Wachsmut et al.</a> presented an argument web search engine, which can be queried <a href="http://www.args.me/">here</a>.</li>
<li><a href="http://aclweb.org/anthology/D17-1260">Potash and Rumshinsky</a> predict the winner of debates, based on audience favorability.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1166.pdf">Swamy et al.</a> also forecast winners for the Oscars, the US presidential primaries, and many other contests based on user predictions on Twitter. They create a dataset to test their approach.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1164.pdf">Zhang et al.</a> analyze the rhetorical role of questions in discourse.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1143.pdf">Liu et al.</a> show that argument-based features are also helpful for predicting review helpfulness.</li>
</ul>

<p><strong>Multi-agent communication</strong>   Multi-agent communication is a niche topic, which has nevertheless received some recent interest, notably in the representation learning community. Most papers deal with a scenario where two agents play a communicative referential game. The task is interesting, as the agents are required to cooperate and have been observed to develop a common pseudo-language in the process.</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1310.pdf">Andreas and Klein</a> investigate the structure encoded by RNN representations for messages in a communication game. They find that the mistakes are similar to the ones made by humans. In addition, they find that negation is encoded as a linear relationship in the vector space.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1320.pdf">Kottur et al.</a> show in their best short paper that language does not emerge naturally when two agents are cooperating, but that they can be coerced to develop compositional expressions.</li>
</ul>

<figure>  
      <img src="../content/images/2017/09/multi-agent_setup.jpg" style="width: 60%; height: 60%" title="Reducing bias amplification">
<figcaption>Figure 8: The multi-agent setup in the paper of Kottur et al.  
</figcaption>  
</figure>

<p><strong>Relation extraction</strong>   Extracting relations from documents is more compelling than simply extracting entities or concepts. Some papers improve upon existing approaches using better distant supervision or adversarial training:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D17-1189">Liu et al.</a> reduce the noise in distantly supervised relation extraction with a soft-label method.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1004.pdf">Zhang et al.</a> publish TACRED, a large supervised dataset knowledge base population, as well as a new model.</li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1187.pdf">Wu et al.</a> improve the precision of relation extraction with adversarial training.</li>
</ul>

<p><strong>Document and sentence representations</strong>   Learning better sentence representations is closely related to learning more general word representations. While word embeddings still have to be contextualized, sentence representations are promising as they can be directly applied to many different tasks:</p>

<ul>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1070.pdf">Mekala et al.</a> propose a novel technique for building document vectors from word embeddings, with good results for text classification. They use a combination of adding and concatenating word embeddings to represent multiple topics of a document, based on word clusters. </li>
<li><a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf">Conneau et al.</a> learn sentence representations from the SNLI dataset and evaluate them on 12 different tasks. </li>
</ul>

<p>These were my highlights. Naturally, I was not able to attend every session and see every paper. What were your highlights from the conference or which papers from the <a href="http://aclweb.org/anthology/D/D17/">proceedings</a> did you like most? Let me know in the comments below.</p>
    </div>

    <div class="post related">
        <a rel="prev" id="prev-btn" class="btn small square" href="../learning-select-data/">← Learning to select data for transfer learning</a>

        <a rel="next" id="next-btn" class="btn small square" href="../multi-task-learning-nlp/">Multi-Task Learning Objectives for Natural Language Processing →</a>
    </div>

    <footer class="post comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + window.disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</footer>

  </article>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  

        <footer>
  <span class="copyright">
    © 2018. All rights reserved. Built with <a href="https://ghost.org/" target="_blank">Ghost</a> and <a href="https://github.com/Kikobeats/uno-zen" target="_blank">Uno Zen</a> theme.
  </span>
</footer>
      </section>
    </main>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
    <script src="../assets/js/uno-zen.js?v=8de49cd7a6" type="text/javascript" charset="utf-8"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script>
  if (window.ga_id) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', window.ga_id, 'auto');
    ga('require', 'linkid', 'linkid.js');
    ga('send', 'pageview');
  }
</script>
  </body>
