
  <head>
    <title>An overview of proxy-label approaches for semi-supervised learning</title>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link rel="apple-touch-icon" sizes="57x57" href="../assets/img/apple-touch-icon-57x57.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="60x60" href="../assets/img/apple-touch-icon-60x60.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="72x72" href="../assets/img/apple-touch-icon-72x72.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="76x76" href="../assets/img/apple-touch-icon-76x76.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="114x114" href="../assets/img/apple-touch-icon-114x114.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="120x120" href="../assets/img/apple-touch-icon-120x120.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="144x144" href="../assets/img/apple-touch-icon-144x144.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="152x152" href="../assets/img/apple-touch-icon-152x152.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="180x180" href="../assets/img/apple-touch-icon-180x180.png?v=wAAv6Wqe6l">
<link rel="icon" type="image/png" href="../assets/img/favicon-32x32.png?v=wAAv6Wqe6l" sizes="32x32">
<link rel="icon" type="image/png" href="../assets/img/favicon-194x194.png?v=wAAv6Wqe6l" sizes="194x194">
<link rel="icon" type="image/png" href="../assets/img/favicon-96x96.png?v=wAAv6Wqe6l" sizes="96x96">
<link rel="icon" type="image/png" href="../assets/img/android-chrome-192x192.png?v=wAAv6Wqe6l" sizes="192x192">
<link rel="icon" type="image/png" href="../assets/img/favicon-16x16.png?v=wAAv6Wqe6l" sizes="16x16">
<link rel="manifest" href="../assets/img/manifest.json?v=wAAv6Wqe6l">
<link rel="shortcut icon" href="../assets/img/favicon.ico?v=wAAv6Wqe6l">
<meta name="msapplication-TileColor" content="#e74c3c">
<meta name="msapplication-TileImage" content="/assets/img/mstile-144x144.png?v=wAAv6Wqe6l">
<meta name="msapplication-config" content="/assets/img/browserconfig.xml?v=wAAv6Wqe6l">
<meta name="theme-color" content="#e74c3c">
    <link rel="stylesheet" type="text/css" href="../assets/css/uno-zen.css?v=988269e9e1">
    <link rel="canonical" href="http://ruder.io/an-overview-of-bootstrapping-approaches-for-semi-supervised-learning/">
    <meta name="referrer" content="origin">
    
    <meta property="og:site_name" content="Sebastian Ruder">
    <meta property="og:type" content="article">
    <meta property="og:title" content="An overview of proxy-label approaches for semi-supervised learning">
    <meta property="og:description" content="Table of contents: Semi-supervised learning is a truly vast field. For a (slightly outdated) overview, refer to Zhu (2005) [] and Chapelle et al. (2006) []. I will focus in this blog post on a particular class of semi-supervised learning algorithms that...">
    <meta property="og:url" content="u=http://ruder.io/an-overview-of-bootstrapping-approaches-for-semi-supervised-learning/">
    <meta property="article:published_time" content="2018-02-24T09:45:15.476Z">
    <meta property="article:modified_time" content="2018-03-03T20:38:52.897Z">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="An overview of proxy-label approaches for semi-supervised learning">
    <meta name="twitter:description" content="Table of contents: Semi-supervised learning is a truly vast field. For a (slightly outdated) overview, refer to Zhu (2005) [] and Chapelle et al. (2006) []. I will focus in this blog post on a particular class of semi-supervised learning algorithms that...">
    <meta name="twitter:url" content="u=http://ruder.io/an-overview-of-bootstrapping-approaches-for-semi-supervised-learning/">
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Sebastian Ruder",
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "url": "u=http://ruder.io/author/sebastian",
        "sameAs": null,
        "description": null
    },
    "headline": "An overview of proxy-label approaches for semi-supervised learning",
    "url": "u=http://ruder.io/an-overview-of-bootstrapping-approaches-for-semi-supervised-learning/",
    "datePublished": "2018-02-24T09:45:15.476Z",
    "dateModified": "2018-03-03T20:38:52.897Z",
    "description": "Table of contents: Semi-supervised learning is a truly vast field. For a (slightly outdated) overview, refer to Zhu (2005) [] and Chapelle et al. (2006) []. I will focus in this blog post on a particular class of semi-supervised learning algorithms that..."
}
    </script>

    <meta name="generator" content="Ghost 0.7">
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="http://ruder.io/rss/">
    <script>
var open_button = '.nav-blog > a'
</script>
<script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
  </head>
  <body class="post-template">
    <header id="menu-button" class="expanded">
      <a><i class="icon icon-list"></i></a>
    </header>
    <aside class="cover" style="background: url(../content/images/2017/05/imageedit_8_8459453433.jpg) center/cover no-repeat fixed">
  <div class="cover container">
    <div class="profile">
      <a id="avatar-link" title="link to homepage for Sebastian Ruder" href="http://ruder.io/#open">
        <img src="../content/images/2015/12/Seb_LinkedIn_Profile-.png" alt="Sebastian Ruder avatar" class="profile avatar rounded hvr-buzz-out">
        <h1 id="profile-title">Sebastian Ruder</h1>
        <h3 id="profile-resume"></h3>
      </a>

      <hr class="divider long">
      <p>I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</p>
      <hr class="divider short">
      <div class="navigation">
        <div class="profile contact">
          <nav class="navigation left">
  <ul class="links">
      <li class="nav-blog ">
        <a href="http://ruder.io/">Blog</a>
      </li>
      <li class="nav-about ">
        <a href="http://ruder.io/about/">About</a>
      </li>
      <li class="nav-papers ">
        <a href="http://ruder.io/publications/">Papers</a>
      </li>
      <li class="nav-news ">
        <a href="http://ruder.io/news">News</a>
      </li>
      <li class="nav-newsletter ">
        <a href="http://newsletter.ruder.io">Newsletter</a>
      </li>
      <li class="nav-faq ">
        <a href="http://ruder.io/faq">FAQ</a>
      </li>
  </ul>
</nav>

          
<nav class="navigation right">
  <ul class="social expanded">

  <!-- Twitter -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="http://twitter.com/seb_ruder" title="@seb_ruder on Twitter">
      <i class="icon icon-social-twitter"></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  <!-- Linkedin -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://www.linkedin.com/in/sebastianruder" title="sebastianruder on LinkedIn">
      <i class="icon icon-social-linkedin"></i>
      <span class="label">Linkedin</span>
    </a>
  </li>

  <!-- Github -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://github.com/sebastianruder" title="sebastianruder on Github">
      <i class="icon icon-social-github"></i>
      <span class="label">Github</span>
    </a>
  </li>

  <!-- E-mail -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="mailto:sebastian@ruder.io" title="send me an email">
      <i class="icon icon-mail"></i>
      <span class="label">Email</span>
    </a>
  </li>

  <!-- RSS -->
  <li class="social item hvr-grow-rotate">
    <a href="../rss/index.rss" title="Subscribe to RSS">
      <i class="icon icon-rss"></i>
      <span class="label">RSS</span>
    </a>
  </li>

  </ul>
</nav>
          <section class="icon icon-search" id="search-container">
  <hr class="divider short">
  <form id="search-form" action="https://www.google.com/#q=site:u=http://ruder.io">
    <input type="text" name="search" placeholder="Deep Learning, NLP, ..." id="search-field">
  </form>
</section>
        </div>
      </div>
    </div>
  </div>
</aside>
    <main>
      <section id="search-results"></section>
      <section class="content">
        

  <article class="post">
    <header>
      <div class="post meta">
        <time datetime="24 Feb 2018">24 Feb 2018</time>
        <span class="post tags"></span>


        <span class="post reading-time"> ~ <span></span> read.</span>
      </div>
      <a alt="Tweet 'An overview of proxy-label approaches for semi-supervised learning'" href="https://twitter.com/intent/tweet?text=An%20overview%20of%20proxy-label%20approaches%20for%20semi-supervised%20learning%20%C2%BB&amp;hashtags=&amp;url=http://ruder.io/an-overview-of-bootstrapping-approaches-for-semi-supervised-learning/">
        
        <h1 class="icon-reverse icon-social-twitter-post" id="post-title">An overview of proxy-label approaches for semi-supervised learning</h1>
      </a>
    </header>

    <div id="post-content" class="post">
      <p>Table of contents:</p>

<p>Semi-supervised learning is a truly vast field. For a (slightly outdated) overview, refer to Zhu (2005) [<sup id="fnref:1"><a href="index.html#fn:1" rel="footnote">1</a></sup>] and Chapelle et al. (2006) [<sup id="fnref:2"><a href="index.html#fn:2" rel="footnote">2</a></sup>]. I will focus in this blog post on a particular class of semi-supervised learning algorithms that produce proxy labels on unlabelled data. This class of models is of particular interest in my opinion, as a) deep neural networks have been shown to be good at dealing with noisy labels and b) these models have achieved state-of-the-art in semi-supervised learning for computer vision. Some of the following approaches have been referred to as <em>self-teaching</em> or <em>bootstrapping</em> algorithms; I am not aware of a term that captures all of them, so I will simply refer to them as <em>proxy-label</em> methods.</p>

<p>I will divide these methods in three groups, which I will discuss in the following: 1) self-training, which uses a model's own predictions as proxy labels; 2) multi-view learning, which uses the predictions of models trained on different views of the data; and 3) self-ensembling, which ensembles variations of a model's own predictions.</p>

<p>There are many interesting and equally important directions for semi-supervised learning that I will not cover in this post, e.g. graph-convolutional neural networks [<sup id="fnref:3"><a href="index.html#fn:3" rel="footnote">3</a></sup>]</p>

<p>Domain adaptation approaches that use unlabelled target domain data to bridge the domain gap. <br>
Look at papers of Learning with Limited Data Workshop at NIPS 2017</p>

<h2 id="selftraining">Self-training</h2>

<p>Self-training (Yarowsky, 1995; McClosky et al., 2006) [<sup id="fnref:4"><a href="index.html#fn:4" rel="footnote">4</a></sup>, <sup id="fnref:5"><a href="index.html#fn:5" rel="footnote">5</a></sup>] is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model's own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next.</p>

<p>Self-training trains a model \(m\) on a labeled training set \(L\) and an unlabeled data set \(U\). At each iteration, the model provides predictions \(m(x)\) in the form of a probability distribution over classes for all unlabeled examples \(x\) in \(U\). If the probability assigned to the most likely class is higher than a predetermined threshold \(\tau\), \(x\) is added to the labeled examples with \(\DeclareMathOperator*{\argmax}{argmax} p(x) = \argmax m(x)\) as pseudo-label. This instantiation is the most widely used and shown in Algorithm 1.</p>

<p><img src="../content/images/2018/03/self-training.png" alt=""></p>

<p>Classic self-training has shown mixed success. In parsing it proved successful with small datasets (Reichart, and Rappoport, 2007; Huang and Harper, 2009) [<sup id="fnref:6"><a href="index.html#fn:6" rel="footnote">6</a></sup>, <sup id="fnref:13"><a href="index.html#fn:13" rel="footnote">13</a></sup>] or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006; Suzuki and Isozaki , 2008) [<sup id="fnref:7"><a href="index.html#fn:7" rel="footnote">7</a></sup>]. Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012) [<sup id="fnref:8"><a href="index.html#fn:8" rel="footnote">8</a></sup>], while others report limited success on a variety of NLP tasks (He and Zhou, 2011; Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017) [<sup id="fnref:9"><a href="index.html#fn:9" rel="footnote">9</a></sup>, <sup id="fnref:10"><a href="index.html#fn:10" rel="footnote">10</a></sup>, <sup id="fnref:11"><a href="index.html#fn:11" rel="footnote">11</a></sup>, <sup id="fnref:12"><a href="index.html#fn:12" rel="footnote">12</a></sup>]. <br>
Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift.</p>

<h2 id="multiviewtraining"> Multi-view training</h2>

<p> 
<strong>Co-training</strong>   Co-training (Blum and Mitchell, 1998) [<sup id="fnref:17"><a href="index.html#fn:17" rel="footnote">17</a></sup>] assumes the data \(L\) is presented in two separate views \(L^1\) and \(L^2\). Two classifiers are then trained, one on each view. At each iteration, only inputs that are confident according to <em>exactly one</em> of the two classifiers are moved to the training set. One classifier thus provides the labels to the inputs on which the <em>other</em> classifier is uncertain. Co-training can be seen in Algorithm 2.</p>

<p><img src="../content/images/2018/03/co-training.png" alt=""></p>

<p>Chen et al. (2011) [<sup id="fnref:18"><a href="index.html#fn:18" rel="footnote">18</a></sup>] adapt Co-training to domain adaptation. They use pseudo-multiview regularization (Chen et al., 2011) [<sup id="fnref:19"><a href="index.html#fn:19" rel="footnote">19</a></sup>] in order to split the features into two mutually exclusive views so that co-training is effective.</p>

<p><strong>Tri-training</strong>   Tri-training (Zhou and Li, 2005) [<sup id="fnref:14"><a href="index.html#fn:14" rel="footnote">14</a></sup>] is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training as depicted in Algorithm 2 first trains three models \(m_1\), \(m_2\), and \(m_3\) on bootstrap samples of the labeled data \(L\). An unlabeled data point is added to the training set of a model \(m_i\) if the other two models \(m_j\) and \(m_k\) agree on its label. Training stops when the classifiers do not change anymore. </p>

<p><img src="../content/images/2018/03/tri-training.png" alt=""></p>

<p><strong>Tri-training with disagreement</strong>   Tri-training <em>with disagreement</em> (Søgaard, 2010) [<sup id="fnref:15"><a href="index.html#fn:15" rel="footnote">15</a></sup>] is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. In order to achieve this, it adds a simple modification to the original algorithm (altering line 8 in Algorithm 2), requiring that for an unlabeled data point on which \(m_j\) and \(m_k\) <em>agree</em>, the other model \(m_i\) <em>disagrees</em> on the prediction. Tri-training with disagreement is more data-efficient than tri-training and has achieved competitive results on part-of-speech tagging (Søgaard, 2010).</p>

<p><strong>Asymmetric tri-training</strong>   Asymmetic tri-training (Saito et al., 2017) [<sup id="fnref:16"><a href="index.html#fn:16" rel="footnote">16</a></sup>] is a recently proposed extension of tri-training that achieved state-of-the-art results for unsupervised domain adaptation in computer vision. For unsupervised domain adaptation, the text data and unlabeled data are from a different domain than the labelled examples. To adapt tri-training to this shift, Saito et al. train one of the models only on proxy labels on which the other two models agree (a change to line 10 in Algorithm 2) and use only this model to classify target domain examples at test time. In addition, all three models share the feature extractor.</p>

<h2 id="selfensembling"> Self-ensembling</h2>

<p><a href="https://thecuriousaicompany.com/mean-teacher/">https://thecuriousaicompany.com/mean-teacher/</a></p>

<p><strong>Ladder networks</strong>  </p>

<p>Semi-supervised learning with ladder networks blog post <br>
<a href="http://rinuboney.github.io/2016/01/19/ladder-network.html">http://rinuboney.github.io/2016/01/19/ladder-network.html</a></p>

<p><strong>Temporal Ensembling</strong>  </p>

<p><strong>Mean Teacher</strong>  </p>

<h2 id="relatedmethodsandareas"> Related methods and areas</h2>

<p><strong>Distillation</strong>  </p>

<p><strong>Learning from weak supervision</strong>  </p>

<p><strong>Data augmentation</strong>   </p>

<h2 id="conclusion"> Conclusion</h2>

<p>Many of the presented semi-supervised are quite old. I'm happy to correct any mistake I made.</p>

<h2 id="references"> References</h2>

<div class="footnotes"><ol><li class="footnote" id="fn:1"><p>Zhu, X. (2005). Semi-Supervised Learning Literature Survey. <a href="index.html#fnref:1" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:2"><p>Chapelle, O., Schölkopf, B., &amp; Zien, A. (2006). Semi-Supervised Learning. Interdisciplinary sciences computational life sciences (Vol. 1). <a href="http://doi.org/10.1007/s12539-009-0016-2">http://doi.org/10.1007/s12539-009-0016-2</a> <a href="index.html#fnref:2" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:3"><p>Kipf, T. N., &amp; Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. Proceedings of ICLR 2017. <a href="index.html#fnref:3" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:4"><p>Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics (pp. 189-196). Association for Computational Linguistics. <a href="index.html#fnref:4" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:5"><p>McClosky, D., Charniak, E., &amp; Johnson, M. (2006). Effective self-training for parsing. Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, 152–159. <a href="index.html#fnref:5" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:6"><p>Reichart, R., &amp; Rappoport, A. (2007). Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (pp. 616-623) <a href="index.html#fnref:6" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:7"><p>Suzuki, J., &amp; Isozaki, H. (2008). Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. Proceedings of ACL-08: HLT, 665-673. <a href="index.html#fnref:7" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:8"><p>Petrov, S., &amp; McDonald, R. (2012). Overview of the 2012 shared task on parsing the web. In Notes of the first workshop on syntactic analysis of non-canonical language (sancl) (Vol. 59). <a href="index.html#fnref:8" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:9"><p>He, Y., &amp; Zhou, D. (2011). Self-training from labeled features for sentiment analysis. Information Processing &amp; Management, 47(4), 606-616. <a href="index.html#fnref:9" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:10"><p>Plank, B. (2011). Domain adaptation for parsing. University Library Groniongen][Host]. <a href="index.html#fnref:10" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:11"><p>Van Asch, V., &amp; Daelemans, W. (2016). Predicting the Effectiveness of Self-Training: Application to Sentiment Classification. arXiv preprint arXiv:1601.03288. <a href="index.html#fnref:11" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:12"><p>van der Goot, R., Plank, B., &amp; Nissim, M. (2017). To normalize, or not to normalize: The impact of normalization on part-of-speech tagging. arXiv preprint arXiv:1707.05116. <a href="index.html#fnref:12" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:13"><p>Huang, Z., &amp; Harper, M. (2009). Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2 (pp. 832-841). Association for Computational Linguistics. <a href="index.html#fnref:13" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:14"><p>Zhou, Z.-H., &amp; Li, M. (2005). Tri-Training: Exploiting Unlabled Data Using Three Classifiers. IEEE Trans.Data Eng., 17(11), 1529–1541. <a href="http://doi.org/10.1109/TKDE.2005.186">http://doi.org/10.1109/TKDE.2005.186</a> <a href="index.html#fnref:14" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:15"><p>Søgaard, A. (2010). Simple semi-supervised training of part-of-speech taggers. Proceedings of the ACL 2010 Conference Short Papers. <a href="index.html#fnref:15" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:16"><p>Saito, K., Ushiku, Y., &amp; Harada, T. (2017). Asymmetric Tri-training for Unsupervised Domain Adaptation. In ICML 2017. Retrieved from <a href="http://arxiv.org/abs/1702.08400">http://arxiv.org/abs/1702.08400</a> <a href="index.html#fnref:16" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:17"><p>Blum, A., &amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory (pp. 92-100). ACM. <a href="index.html#fnref:17" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:18"><p>Chen, M., Weinberger, K. Q., &amp; Blitzer, J. C. (2011). Co-Training for Domain Adaptation. In Advances in Neural Information Processing Systems. <a href="index.html#fnref:18" title="return to article">↩</a></p></li>

<li class="footnote" id="fn:19"><p>Chen, M., Weinberger, K. Q., &amp; Chen, Y. (2011). Automatic Feature Decomposition for Single View Co-training. Proceedings of the 28th International Conference on Machine Learning (ICML-11), 953–960. <a href="index.html#fnref:19" title="return to article">↩</a></p></li></ol></div>
    </div>

    <div class="post related">
        <a rel="prev" id="prev-btn" class="btn small square" href="../deep-learning-optimization-2017/">← Optimization for Deep Learning Highlights in 2017</a>

        <a rel="next" id="next-btn" class="btn small square" href="../requests-for-research/">Requests for Research →</a>
    </div>

    <footer class="post comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + window.disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</footer>

  </article>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  

        <footer>
  <span class="copyright">
    © 2018. All rights reserved. Built with <a href="https://ghost.org/" target="_blank">Ghost</a> and <a href="https://github.com/Kikobeats/uno-zen" target="_blank">Uno Zen</a> theme.
  </span>
</footer>
      </section>
    </main>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
    <script src="../assets/js/uno-zen.js?v=988269e9e1" type="text/javascript" charset="utf-8"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script>
  if (window.ga_id) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', window.ga_id, 'auto');
    ga('require', 'linkid', 'linkid.js');
    ga('send', 'pageview');
  }
</script>
  </body>
