
  <head>
    <title>A Review of the Recent History of Natural Language Processing</title>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link rel="apple-touch-icon" sizes="57x57" href="../assets/img/apple-touch-icon-57x57.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="60x60" href="../assets/img/apple-touch-icon-60x60.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="72x72" href="../assets/img/apple-touch-icon-72x72.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="76x76" href="../assets/img/apple-touch-icon-76x76.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="114x114" href="../assets/img/apple-touch-icon-114x114.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="120x120" href="../assets/img/apple-touch-icon-120x120.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="144x144" href="../assets/img/apple-touch-icon-144x144.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="152x152" href="../assets/img/apple-touch-icon-152x152.png?v=wAAv6Wqe6l">
<link rel="apple-touch-icon" sizes="180x180" href="../assets/img/apple-touch-icon-180x180.png?v=wAAv6Wqe6l">
<link rel="icon" type="image/png" href="../assets/img/favicon-32x32.png?v=wAAv6Wqe6l" sizes="32x32">
<link rel="icon" type="image/png" href="../assets/img/favicon-194x194.png?v=wAAv6Wqe6l" sizes="194x194">
<link rel="icon" type="image/png" href="../assets/img/favicon-96x96.png?v=wAAv6Wqe6l" sizes="96x96">
<link rel="icon" type="image/png" href="../assets/img/android-chrome-192x192.png?v=wAAv6Wqe6l" sizes="192x192">
<link rel="icon" type="image/png" href="../assets/img/favicon-16x16.png?v=wAAv6Wqe6l" sizes="16x16">
<link rel="manifest" href="../assets/img/manifest.json?v=wAAv6Wqe6l">
<link rel="shortcut icon" href="../assets/img/favicon.ico?v=wAAv6Wqe6l">
<meta name="msapplication-TileColor" content="#e74c3c">
<meta name="msapplication-TileImage" content="/assets/img/mstile-144x144.png?v=wAAv6Wqe6l">
<meta name="msapplication-config" content="/assets/img/browserconfig.xml?v=wAAv6Wqe6l">
<meta name="theme-color" content="#e74c3c">
    <link rel="stylesheet" type="text/css" href="../assets/css/uno-zen.css?v=913ce5f5b3">
    <link rel="canonical" href="http://ruder.io/a-review-of-the-recent-history-of-nlp/">
    <meta name="referrer" content="origin">
    
    <meta property="og:site_name" content="Sebastian Ruder">
    <meta property="og:type" content="article">
    <meta property="og:title" content="A Review of the Recent History of Natural Language Processing">
    <meta property="og:description" content="This blog post discusses the eight biggest milestones in the last ~15 years of Natural Language Processing.">
    <meta property="og:url" content="u=http://ruder.io/a-review-of-the-recent-history-of-nlp/">
    <meta property="og:image" content="u=http://ruder.io/content/images/2018/10/review_recent_history_image.png">
    <meta property="article:published_time" content="2018-10-01T12:50:00.000Z">
    <meta property="article:modified_time" content="2018-10-01T17:32:53.205Z">
    <meta property="article:tag" content="natural language processing">
    <meta property="article:tag" content="nlp">
    <meta property="article:tag" content="deep learning">
    <meta property="article:tag" content="word embeddings">
    <meta property="article:tag" content="multi-task learning">
    <meta property="article:tag" content="transfer learning">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="A Review of the Recent History of Natural Language Processing">
    <meta name="twitter:description" content="This blog post discusses the eight biggest milestones in the last ~15 years of Natural Language Processing.">
    <meta name="twitter:url" content="u=http://ruder.io/a-review-of-the-recent-history-of-nlp/">
    <meta name="twitter:image:src" content="u=http://ruder.io/content/images/2018/10/review_recent_history_image.png">
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Sebastian Ruder",
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "url": "u=http://ruder.io/author/sebastian",
        "sameAs": null,
        "description": null
    },
    "headline": "A Review of the Recent History of Natural Language Processing",
    "url": "u=http://ruder.io/a-review-of-the-recent-history-of-nlp/",
    "datePublished": "2018-10-01T12:50:00.000Z",
    "dateModified": "2018-10-01T17:32:53.205Z",
    "image": "u=http://ruder.io/content/images/2018/10/review_recent_history_image.png",
    "keywords": "natural language processing, nlp, deep learning, word embeddings, multi-task learning, transfer learning",
    "description": "This blog post discusses the eight biggest milestones in the last ~15 years of Natural Language Processing."
}
    </script>

    <meta name="generator" content="Ghost 0.7">
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="http://ruder.io/rss/">
    <script>
var open_button = '.nav-blog > a'
</script>
<script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
  </head>
  <body class="post-template tag-natural-language-processing tag-nlp tag-deep-learning tag-word-embeddings tag-multi-task-learning tag-transfer-learning">
    <header id="menu-button" class="expanded">
      <a><i class="icon icon-list"></i></a>
    </header>
    <aside class="cover" style="background: url(../content/images/2017/05/imageedit_8_8459453433.jpg) center/cover no-repeat fixed">
  <div class="cover container">
    <div class="profile">
      <a id="avatar-link" title="link to homepage for Sebastian Ruder" href="http://ruder.io/#open">
        <img src="../content/images/2015/12/Seb_LinkedIn_Profile-.png" alt="Sebastian Ruder avatar" class="profile avatar rounded hvr-buzz-out">
        <h1 id="profile-title">Sebastian Ruder</h1>
        <h3 id="profile-resume"></h3>
      </a>

      <hr class="divider long">
      <p>I'm a PhD student in Natural Language Processing and a research scientist at AYLIEN. I blog about Machine Learning, Deep Learning, NLP, and startups.</p>
      <hr class="divider short">
      <div class="navigation">
        <div class="profile contact">
          <nav class="navigation left">
  <ul class="links">
      <li class="nav-blog ">
        <a href="http://ruder.io/">Blog</a>
      </li>
      <li class="nav-about ">
        <a href="http://ruder.io/about/">About</a>
      </li>
      <li class="nav-papers ">
        <a href="http://ruder.io/publications/">Papers</a>
      </li>
      <li class="nav-news ">
        <a href="http://ruder.io/news">News</a>
      </li>
      <li class="nav-newsletter ">
        <a href="http://newsletter.ruder.io">Newsletter</a>
      </li>
      <li class="nav-faq ">
        <a href="http://ruder.io/faq">FAQ</a>
      </li>
      <li class="nav-progress ">
        <a href="https://nlpprogress.com/">Progress</a>
      </li>
  </ul>
</nav>

          
<nav class="navigation right">
  <ul class="social expanded">

  <!-- Twitter -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="http://twitter.com/seb_ruder" title="@seb_ruder on Twitter">
      <i class="icon icon-social-twitter"></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  <!-- Linkedin -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://www.linkedin.com/in/sebastianruder" title="sebastianruder on LinkedIn">
      <i class="icon icon-social-linkedin"></i>
      <span class="label">Linkedin</span>
    </a>
  </li>

  <!-- Github -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="https://github.com/sebastianruder" title="sebastianruder on Github">
      <i class="icon icon-social-github"></i>
      <span class="label">Github</span>
    </a>
  </li>

  <!-- E-mail -->
  <li class="social item hvr-grow-rotate">
    <a rel="me" target="blank" href="mailto:sebastian@ruder.io" title="send me an email">
      <i class="icon icon-mail"></i>
      <span class="label">Email</span>
    </a>
  </li>

  <!-- RSS -->
  <li class="social item hvr-grow-rotate">
    <a href="../rss/index.rss" title="Subscribe to RSS">
      <i class="icon icon-rss"></i>
      <span class="label">RSS</span>
    </a>
  </li>

  </ul>
</nav>
          <section class="icon icon-search" id="search-container">
  <hr class="divider short">
  <form id="search-form" action="https://www.google.com/#q=site:u=http://ruder.io">
    <input type="text" name="search" placeholder="Deep Learning, NLP, ..." id="search-field">
  </form>
</section>
        </div>
      </div>
    </div>
  </div>
</aside>
    <main>
      <section id="search-results"></section>
      <section class="content">
        

  <article class="post tag-natural-language-processing tag-nlp tag-deep-learning tag-word-embeddings tag-multi-task-learning tag-transfer-learning">
    <header>
      <div class="post meta">
        <time datetime="01 Oct 2018">01 Oct 2018</time>
        <span class="post tags">in <a href="../tag/natural-language-processing/">natural language processing</a> <a href="../tag/nlp/">nlp</a> <a href="../tag/deep-learning/">deep learning</a> <a href="../tag/word-embeddings/">word embeddings</a> <a href="../tag/multi-task-learning/">multi-task learning</a> <a href="../tag/transfer-learning/">transfer learning</a></span>


        <span class="post reading-time"> ~ <span></span> read.</span>
      </div>
      <a alt="Tweet 'A Review of the Recent History of Natural Language Processing'" href="https://twitter.com/intent/tweet?text=A%20Review%20of%20the%20Recent%20History%20of%20Natural%20Language%20Processing%20%C2%BB&amp;hashtags=natural%20language%20processing,nlp,deep%20learning,word%20embeddings,multi-task%20learning,transfer%20learning&amp;url=http://ruder.io/a-review-of-the-recent-history-of-nlp/">
        <img id="post-image" src="../content/images/2018/10/review_recent_history_image.png" alt="A Review of the Recent History of Natural Language Processing">
        <h1 class="icon-reverse icon-social-twitter-post" id="post-title">A Review of the Recent History of Natural Language Processing</h1>
      </a>
    </header>

    <div id="post-content" class="post tag-natural-language-processing tag-nlp tag-deep-learning tag-word-embeddings tag-multi-task-learning tag-transfer-learning">
      <p><link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot-number.min.css">  </p>

<script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/bigfoot/2.1.4/bigfoot.min.js"></script>  
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"></script>  
<script type="text/javascript">  
    $.bigfoot({useFootnoteOnlyOnce: false});
</script>  

<p><em>This post originally appeared at the <a href="http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/">AYLIEN blog</a>.</em></p>

<p>This is the first blog post in a two-part series. The series expands on the Frontiers of Natural Language Processing session organized by <a href="http://www.kamperh.com/">Herman Kamper</a> and me at the <a href="http://www.deeplearningindaba.com/">Deep Learning Indaba 2018</a>. Slides of the entire session can be found <a href="https://www.slideshare.net/SebastianRuder/frontiers-of-natural-language-processing">here</a>. This post will discuss major recent advances in NLP. The second post will discuss open problems in NLP.</p>

<p>Table of contents:</p>

<ul>  
<li><a href="index.html#2001neurallanguagemodels">2001 - Neural language models</a></li>  
<li><a href="index.html#20018multitasklearning">2008 - Multi-task learning</a></li>  
<li><a href="index.html#2013wordembeddings">2013 - Word embeddings</a></li>  
<li><a href="index.html#2013neuralnetworksfornlp">2013 - Neural networks for NLP</a></li>  
<li><a href="index.html#2014sequencetosequencemodels">2014 - Sequence-to-sequence models</a></li>  
<li><a href="index.html#2015memorybasednetworks">2015 - Memory-based networks</a></li>  
<li><a href="index.html#2018pretrainedlanguagemodels">2018 - Pretrained language models</a></li>  
<li><a href="index.html#othermilestones">Other milestones</a></li>  
</ul>

<h1 id="2001neurallanguagemodels">2001 - Neural language models</h1>

<p>Language modelling is the task of predicting the next word in a text given the previous words. It is probably the simplest language processing task with concrete practical applications such as <a href="https://en.wikipedia.org/wiki/SwiftKey">intelligent keyboards</a> and email response suggestion (Kannan et al., 2016)<a href="index.html#fn2"></a>. Unsurprisingly, language modelling has a rich history. Classic approaches are based on <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> and employ smoothing to deal with unseen n-grams (Kneser &amp; Ney, 1995)<a href="index.html#fn1"></a>. <br>
The first neural language model, a feed-forward neural network was proposed in 2001 by Bengio et al.<a href="index.html#fn3"></a>, shown in Figure 1 below.  </p>

<figure>  
    <center>
<img src="../content/images/2018/09/lm_bengio_2003.png" width="450">  
    <figcaption>Figure 1: A feed-forward neural network language model (Bengio et al., 2001; 2003)</figcaption>
    </center>
</figure>  

<p>This model takes as input vector representations of the \(n\) previous words, which are looked up in a table \(C\). Nowadays, such vectors are known as word embeddings. These word embeddings are concatenated and fed into a hidden layer, whose output is then provided to a softmax layer. For more information about the model, have a look at <a href="http://ruder.io/word-embeddings-1/index.html#classicneurallanguagemodel">this post</a>. <br>
More recently, feed-forward neural networks have been replaced with recurrent neural networks (RNNs; Mikolov et al., 2010)<a href="index.html#fn4"></a> and long short-term memory networks (LSTMs; Graves, 2013)<a href="index.html#fn5"></a> for language modelling. Many new language models that extend the classic LSTM have been proposed in recent years (have a look at <a href="http://nlpprogress.com/language_modeling.html">this page</a> for an overview). Despite these developments, the classic LSTM remains a strong baseline (Melis et al., 2018)<a href="index.html#fn6"></a>. Even Bengio et al.'s classic feed-forward neural network is in some settings competitive with more sophisticated models as these typically only learn to consider the most recent words (Daniluk et al., 2017)<a href="index.html#fn7"></a>. Understanding better what information such language models capture consequently is an active research area (Kuncoro et al., 2018; Blevins et al., 2018)<a href="index.html#fn87"></a><a href="index.html#fn88"></a>.</p>

<p>Language modelling is typically the training ground of choice when applying RNNs and has succeeded at capturing the imagination, with many getting their first exposure via <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej's blog post</a>. Language modelling is a form of unsupervised learning, which Yann LeCun also calls predictive learning and cites as a prerequisite to acquiring common sense (see <a href="http://ruder.io/highlights-nips-2016/#generalartificialintelligence">here</a> for his Cake slide from NIPS 2016). Probably the most remarkable aspect about language modelling is that despite its simplicity, it is core to many of the later advances discussed in this post:</p>

<ul>  
<li>Word embeddings: The objective of word2vec is a simplification of language modelling.</li>  
<li>Sequence-to-sequence models: Such models generate an output sequence by predicting one word at a time.</li>  
<li>Pretrained language models: These methods use representations from language models for transfer learning.</li>  
</ul>

<p>This conversely means that many of the most important recent advances in NLP reduce to a form of language modelling. In order to do "real" natural language understanding, just learning from the raw form of text likely will not be enough and we will need new methods and models.</p>

<h1 id="2008multitasklearning">2008 - Multi-task learning</h1>

<p>Multi-task learning is a general method for sharing parameters between models that are trained on multiple tasks. In neural networks, this can be done easily by tying the weights of different layers. The idea of multi-task learning was first proposed in 1993 by Rich Caruana<a href="index.html#fn8"></a> and was applied to road-following and pneumonia prediction (Caruana, 1998)<a href="index.html#fn10"></a>. Intuitively, multi-task learning encourages the models to learn representations that are useful for many tasks. This is particularly useful for learning general, low-level representations, to focus a model's attention or in settings with limited amounts of training data. For a more comprehensive overview of multi-task learning, have a look at <a href="http://ruder.io/multi-task/">this post</a>.</p>

<p>Multi-task learning was first applied to neural networks for NLP in 2008 by Collobert and Weston<a href="index.html#fn9"></a><a href="index.html#fn11"></a>. In their model, the look-up tables (or word embedding matrices) are shared between two models trained on different tasks, as depicted in Figure 2 below.  </p>

<figure>  
    <center>
<img src="../content/images/2018/09/collobert_icml2008.png" width="450">  
    <figcaption>Figure 2: Sharing of word embedding matrices (Collobert &amp; Weston, 2008; Collobert et al., 2011)</figcaption>
    </center>
</figure>  

<p>Sharing the word embeddings enables the models to collaborate and share general low-level information in the word embedding matrix, which typically makes up the largest number of parameters in a model. The 2008 paper by Collobert and Weston proved influential beyond its use of multi-task learning. It spearheaded ideas such as pretraining word embeddings and using convolutional neural networks (CNNs) for text that have only been widely adopted in the last years. It won the <a href="https://research.fb.com/facebook-researchers-win-test-of-time-award-at-icml-2018/">test-of-time award at ICML 2018</a> (see the test-of-time award talk contextualizing the paper <a href="https://www.facebook.com/722677142/posts/10155393881507143/">here</a>).</p>

<p>Multi-task learning is now used across a wide range of NLP tasks and leveraging existing or "artificial" tasks has become a useful tool in the NLP repertoire. For an overview of different auxiliary tasks, have a look at <a href="http://ruder.io/multi-task-learning-nlp/">this post</a>. While the sharing of parameters is typically predefined, different sharing patterns can also be learned during the optimization process (Ruder et al., 2017)<a href="index.html#fn12"></a>. As models are being increasingly evaluated on multiple tasks to gauge their generalization ability, multi-task learning is gaining in importance and dedicated benchmarks for multi-task learning have been proposed recently (Wang et al., 2018; McCann et al., 2018)<a href="index.html#fn89"></a><a href="index.html#fn90"></a>.</p>

<h1 id="2013wordembeddings">2013 - Word embeddings</h1>

<p>Sparse vector representations of text, the so-called <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words model</a> have a long history in NLP. Dense vector representations of words or word embeddings have been used as early as 2001 as we have seen above. The main innovation that was proposed in 2013 by Mikolov et al.<a href="index.html#fn14"></a><a href="index.html#fn13"></a> was to make the training of these word embeddings more efficient by removing the hidden layer and approximating the objective. While these changes were simple in nature, they enabled---together with the efficient word2vec implementation---large-scale training of word embeddings.</p>

<p>Word2vec comes in two flavours that can be seen in Figure 3 below: continuous bag-of-words (CBOW) and skip-gram. They differ in their objective: one predicts the centre word based based on the surrounding words, while the other does the opposite.  </p>

<figure>  
    <center>
<img src="../content/images/2018/09/cbow_skipgram_mikolov_2013.png" width="450">  
    <figcaption>Figure 3: Continuous bag-of-words and skip-gram architectures (Mikolov et al., 2013a; 2013b)</figcaption>
    </center>
</figure>  

<p>While these embeddings are no different conceptually than the ones learned with a feed-forward neural network, training on a very large corpus enables them to capture certain relation between words such as gender, verb tense, and country-capital relations, which can be seen in Figure 4 below.  </p>

<figure>  
    <center>
<img src="../content/images/2018/09/word2vec_relations.png" width="450">  
    <figcaption>Figure 4: Relations captured by word2vec (Mikolov et al., 2013a; 2013b)</figcaption>
    </center>
</figure>  

<p>These relations and the meaning behind them sparked initial interest in word embeddings and many studies have investigated the origin of these linear relationships (Arora et al., 2016; Mimno &amp; Thompson, 2017; Antoniak &amp; Mimno, 2018; Wendlandt et al., 2018)<a href="index.html#fn15"></a><a href="index.html#fn16"></a><a href="index.html#fn17"></a><a href="index.html#fn18"></a>. What cemented word embeddings as a mainstay in current NLP, however, was that using pretrained embeddings as initialization was shown to improve performance across a wide range of downstream tasks<a href="index.html#fn19"></a>.</p>

<p>While the relations word2vec captured had an intuitive and almost magical quality to them, later studies showed that there is nothing inherently special about word2vec: Word embeddings can also be learned via matrix factorization (Pennington et al, 2014; Levy &amp; Goldberg, 2014)<a href="index.html#fn20"></a><a href="index.html#fn21"></a> and with proper tuning, classic matrix factorization approaches like SVD and LSA achieve similar results (Levy et al., 2015)<a href="index.html#fn22"></a>. </p>

<p>Since then, a lot of work has gone into exploring different facets of word embeddings (as indicated by the <a href="https://scholar.google.de/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=distributed+representations+of+words+and+phrases&amp;btnG=">staggering number of citations of the original paper</a>). Have a look at <a href="http://ruder.io/word-embeddings-2017/">this post</a> for some trends and future directions. Despite many developments, word2vec is still a popular choice and widely used today. Word2vec's reach has even extended beyond the word level: skip-gram with negative sampling, a convenient objective for learning embeddings based on local context, has been applied to learn representations for sentences (Mikolov &amp; Le, 2014; Kiros et al., 2015)<a href="index.html#fn23"></a><a href="index.html#fn24"></a>---and even going beyond NLP---to networks (Grover &amp; Leskovec, 2016)<a href="index.html#fn25"></a> and biological sequences (Asgari &amp; Mofrad, 2015)<a href="index.html#fn26"></a>, among others.</p>

<p>One particularly exciting direction is to project word embeddings of different languages into the same space to enable (zero-shot) cross-lingual transfer. It is becoming increasingly possible to learn a good projection in a completely unsupervised way (at least for similar languages) (Conneau et al., 2018; Artetxe et al., 2018; Søgaard et al., 2018)<a href="index.html#fn27"></a><a href="index.html#fn28"></a><a href="index.html#fn29"></a>, which opens applications for low-resource languages and unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018)<a href="index.html#fn91"></a><a href="index.html#fn92"></a>. Have a look at (Ruder et al., 2018)<a href="index.html#fn30"></a> for an overview.</p>

<h1 id="2013neuralnetworksfornlp">2013 - Neural networks for NLP</h1>

<p>2013 and 2014 marked the time when neural network models started to get adopted in NLP. Three main types of neural networks became the most widely used: recurrent neural networks, convolutional neural networks, and recursive neural networks.</p>

<p><strong>Recurrent neural networks</strong>   Recurrent neural networks (RNNs) are an obvious choice to deal with the dynamic input sequences ubiquitous in NLP. Vanilla RNNs (Elman, 1990)<a href="index.html#fn31"></a> were quickly replaced with the classic long-short term memory networks (Hochreiter &amp; Schmidhuber, 1997)<a href="index.html#fn32"></a>, which proved more resilient to the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing and exploding gradient problem</a>. Before 2013, RNNs were still thought to be difficult to train; <a href="https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Ilya Sutskever's PhD thesis</a> was a key example on the way to changing this reputation. A visualization of an LSTM cell can be seen in Figure 5 below. A bidirectional LSTM (Graves et al., 2013)<a href="index.html#fn93"></a> is typically used to deal with both left and right context.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/lstm_colah_2015.png" width="450">  
    <figcaption>Figure 5: An LSTM network (Source: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Chris Olah</a>)</figcaption>
    </center>
</figure>

<p><strong>Convolutional neural networks</strong>   With convolutional neural networks (CNNs) being widely used in computer vision, they also started to get applied to language (Kalchbrenner et al., 2014; Kim et al., 2014)<a href="index.html#fn33"></a><a href="index.html#fn34"></a>. A convolutional neural network for text only operates in two dimensions, with the filters only needing to be moved along the temporal dimension. Figure 6 below shows a typical CNN as used in NLP. </p>

<figure>  
    <center>
<img src="../content/images/2018/09/kim_emnlp2014.png" width="450">  
    <figcaption>Figure 6: A convolutional neural network for text (Kim, 2014)</figcaption>
    </center>
</figure>

<p>An advantage of convolutional neural networks is that they are more parallelizable than RNNs, as the state at every timestep only depends on the local context (via the convolution operation) rather than all past states as in the RNN. CNNs can be extended with wider receptive fields using dilated convolutions to capture a wider context (Kalchbrenner et al., 2016)<a href="index.html#fn35"></a>. CNNs and LSTMs can also be combined and stacked (Wang et al., 2016)<a href="index.html#fn36"></a> and convolutions can be used to speed up an LSTM (Bradbury et al., 2017)<a href="index.html#fn37"></a>.</p>

<p><strong>Recursive neural networks</strong>   RNNs and CNNs both treat the language as a sequence. From a linguistic perspective, however, language is <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">inherently hierarchical</a>: Words are composed into higher-order phrases and clauses, which can themselves be recursively combined according to a set of production rules. The linguistically inspired idea of treating sentences as trees rather than as a sequence gives rise to recursive neural networks (Socher et al., 2013)<a href="index.html#fn38"></a>, which can be seen in Figure 7 below. </p>

<figure>  
    <center>
<img src="../content/images/2018/09/socher_emnlp2013.png" width="450">  
    <figcaption>Figure 7: A recursive neural network (Socher et al., 2013)</figcaption>
    </center>
</figure>

<p>Recursive neural networks build the representation of a sequence from the bottom up in contrast to RNNs who process the sentence left-to-right or right-to-left. At every node of the tree, a new representation is computed by composing the representations of the child nodes. As a tree can also be seen as imposing a different processing order on an RNN, LSTMs have naturally been extended to trees (Tai et al., 2015)<a href="index.html#fn39"></a>.</p>

<p>Not only RNNs and LSTMs can be extended to work with hierarchical structures. Word embeddings can be learned based not only on local but on grammatical context (Levy &amp; Goldberg, 2014)<a href="index.html#fn40"></a>; language models can generate words based on a syntactic stack (Dyer et al., 2016)<a href="index.html#fn41"></a>; and graph-convolutional neural networks can operate over a tree (Bastings et al., 2017)<a href="index.html#fn42"></a>.</p>

<h1 id="2014sequencetosequencemodels">2014 - Sequence-to-sequence models</h1>

<p>In 2014, Sutskever et al.<a href="index.html#fn63"></a> proposed sequence-to-sequence learning, a general framework for mapping one sequence to another one using a neural network. In the framework, an encoder neural network processes a sentence symbol by symbol and compresses it into a vector representation; a decoder neural network then predicts the output symbol by symbol based on the encoder state, taking as input at every step the previously predicted symbol as can be seen in Figure 8 below.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/sutskever_nips2014.png" width="450">  
    <figcaption>Figure 8: A sequence-to-sequence model (Sutskever et al., 2014)</figcaption>
    </center>
</figure>

<p>Machine translation turned out to be the killer application of this framework. In 2016, Google announced that it was starting to replace its monolithic phrase-based MT models with neural MT models (Wu et al., 2016)<a href="index.html#fn48"></a>. <a href="https://www.oreilly.com/ideas/what-machine-learning-means-for-software-development">According to Jeff Dean</a>, this meant replacing 500,000 lines of phrase-based MT code with a 500-line neural network model.</p>

<p>This framework due to its flexibility is now the go-to framework for natural language generation tasks, with different models taking on the role of the encoder and the decoder. Importantly, the decoder model can not only be conditioned on a sequence, but on arbitrary representations. This enables for instance generating a caption based on an image (Vinyals et al., 2015)<a href="index.html#fn43"></a> (as can be seen in Figure 9 below), text based on a table (Lebret et al., 2016)<a href="index.html#fn44"></a>, and a description based on source code changes (Loyola et al., 2017)<a href="index.html#fn45"></a>, among many other applications.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/vinyals_cvpr2015.png" width="450">  
    <figcaption>Figure 9: Generating a caption based on an image (Vinyals et al., 2015)</figcaption>
    </center>
</figure>

<p>Sequence-to-sequence learning can even be applied to structured prediction tasks common in NLP where the output has a particular structure. For simplicity, the output is linearized as can be seen for constituency parsing in Figure 10 below. Neural networks have demonstrated the ability to directly learn to produce such a linearized output given sufficient amount of training data for constituency parsing (Vinyals et al, 2015)<a href="index.html#fn46"></a>, and named entity recognition (Gillick et al., 2016)<a href="index.html#fn47"></a>, among others.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/vinyals_nips2015.png" width="450">  
    <figcaption>Figure 10: Linearizing a constituency parse tree (Vinyals et al., 2015)</figcaption>
    </center>
</figure>

<p>Encoders for sequences and decoders are typically based on RNNs but other model types can be used. New architectures mainly emerge from work in MT, which acts as a Petri dish for sequence-to-sequence architectures. Recent models are deep LSTMs (Wu et al., 2016)<a href="index.html#fn48"></a>, convolutional encoders (Kalchbrenner et al., 2016; Gehring et al., 2017)<a href="index.html#fn35"></a><a href="index.html#fn49"></a>, the Transformer (Vaswani et al., 2017)<a href="index.html#fn50"></a>, which will be discussed in the next section, and a combination of an LSTM and a Transformer (Chen et al., 2018)<a href="index.html#fn51"></a>.</p>

<h1 id="2015attention">2015 - Attention</h1>

<p>Attention (Bahdanau et al., 2015)<a href="index.html#fn52"></a> is one of the core innovations in neural MT (NMT) and the key idea that enabled NMT models to outperform classic phrase-based MT systems. The main bottleneck of sequence-to-sequence learning is that it requires to compress the entire content of the source sequence into a fixed-size vector. Attention alleviates this by allowing the decoder to look back at the source sequence hidden states, which are then provided as a weighted average as additional input to the decoder as can be seen in Figure 11 below.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/attention_bahdanau_iclr2015.png" width="250">  
    <figcaption>Figure 11: Attention (Bahdanau et al., 2015)</figcaption>
    </center>
</figure>

<p>Different forms of attention are available (Luong et al., 2015)<a href="index.html#fn53"></a>. Have a look <a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#attention">here</a> for a brief overview. Attention is widely applicable and potentially useful for any task that requires making decisions based on certain parts of the input. It has been applied to consituency parsing (Vinyals et al., 2015)<a href="index.html#fn46"></a>, reading comprehension (Hermann et al., 2015)<a href="index.html#fn54"></a>, and one-shot learning (Vinyals et al., 2016)<a href="index.html#fn56"></a>, among many others. The input does not even need to be a sequence, but can consist of other representations as in the case of image captioning (Xu et al., 2015)<a href="index.html#fn55"></a>, which can be seen in Figure 12 below. A useful side-effect of attention is that it provides a rare---if only superficial---glimpse into the inner workings of the model by inspecting which parts of the input are relevant for a particular output based on the attention weights.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/xu_icml2015.png" width="450">  
    <figcaption>Figure 12: Visual attention in an image captioning model indicating what the model is attending to when generating the word "frisbee". (Xu et al., 2015)</figcaption>
    </center>
</figure>

<p>Attention is also not restricted to just looking at the input sequence; self-attention can be used to look at the surrounding words in a sentence or document to obtain more contextually sensitive word representations. Multiple layers of self-attention are at the core of the Transformer architecture (Vaswani et al., 2017)<a href="index.html#fn50"></a>, the current state-of-the-art model for NMT.</p>

<h1 id="2015memorybasednetworks">2015 - Memory-based networks</h1>

<p>Attention can be seen as a form of fuzzy memory where the memory consists of the past hidden states of the model, with the model choosing what to retrieve from memory. For a more detailed overview of attention and its connection to memory, have a look at <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">this post</a>. Many models with a more explicit memory have been proposed. They come in different variants such as Neural Turing Machines (Graves et al., 2014)<a href="index.html#fn57"></a>, Memory Networks (Weston et al., 2015)<a href="index.html#fn58"></a> and End-to-end Memory Networks (Sukhbaatar et al., 2015)<a href="index.html#fn59"></a>, Dynamic Memory Networks (Kumar et al., 2015)<a href="index.html#fn60"></a>, the Neural Differentiable Computer (Graves et al., 2016)<a href="index.html#fn61"></a>, and the Recurrent Entity Network (Henaff et al., 2017)<a href="index.html#fn62"></a>.</p>

<p>Memory is often accessed based on similarity to the current state similar to attention and can typically be written to and read from. Models differ in how they implement and leverage the memory. For instance, End-to-end Memory Networks process the input multiple times and update the memory to enable multiple steps of inference. Neural Turing Machines also have a location-based addressing, which allows them to learn simple computer programs like sorting. Memory-based models are typically applied to tasks, where retaining information over longer time spans should be useful such as language modelling and reading comprehension. The concept of a memory is very versatile: A knowledge base or table can function as a memory, while a memory can also be populated based on the entire input or particular parts of it.</p>

<h1 id="2018pretrainedlanguagemodels">2018 - Pretrained language models</h1>

<p>Pretrained word embeddings are context-agnostic and only used to initialize the first layer in our models. In recent months, a range of supervised tasks has been used to pretrain neural networks (Conneau et al., 2017; McCann et al., 2017; Subramanian et al., 2018)<a href="index.html#fn65"></a><a href="index.html#fn64"></a><a href="index.html#fn66"></a>. In contrast, language models only require unlabelled text; training can thus scale to billions of tokens, new domains, and new languages. Pretrained language models were first proposed in 2015 (Dai &amp; Le, 2015)<a href="index.html#fn67"></a>; only recently were they shown to be beneficial across a diverse range of tasks. Language model embeddings can be used as features in a target model (Peters et al., 2018)<a href="index.html#fn68"></a> or a language model can be fine-tuned on target task data (Howard &amp; Ruder et al., 2018)<a href="index.html#fn69"></a>. Adding language model embeddings gives a large improvement over the state-of-the-art across many different tasks as can be seen in Figure 13 below.</p>

<figure>  
    <center>
<img src="../content/images/2018/09/elmo_peters_2018.png" width="450">  
    <figcaption>Figure 13: Improvements with language model embeddings over the state-of-the-art (Peters et al., 2018)</figcaption>
    </center>
</figure>

<p>Pretrained language models have been shown enable learning with significantly less data. As language models only require unlabelled data, they are particularly beneficial for low-resource languages where labelled data is scarce. For more information about the potential of pretrained language models, refer to <a href="https://thegradient.pub/nlp-imagenet/">this article</a>.</p>

<h1 id="othermilestones">Other milestones</h1>

<p>Some other developments are less pervasive than the ones mentioned above, but still have wide-ranging impact.</p>

<p><strong>Character-based representations</strong>   Using a CNN or an LSTM over characters to obtain a character-based word representation is now fairly common, particularly for morphologically rich languages and tasks where morphological information is important or that have many unknown words. As far as I am aware, character-based representations were first used for sequence labelling (Lample et al., 2016; Plank et al., 2016)<a href="index.html#fn70"></a><a href="index.html#fn71"></a>. Character-based representations alleviate the need of having to deal with a fixed vocabulary at increased computational cost and enable applications such as fully character-based NMT (Ling et al., 2016; Lee et al., 2017)<a href="index.html#fn72"></a><a href="index.html#fn73"></a>.</p>

<p><strong>Adversarial learning</strong>   Adversarial methods have taken the field of ML by storm and have also been used in different forms in NLP. Adversarial examples are becoming increasingly widely used not only as a tool to probe models and understand their failure cases, but also to make them more robust (Jia &amp; Liang, 2017)<a href="index.html#fn74"></a>. (Virtual) adversarial training, that is, worst-case perturbations (Miyato et al., 2017; Yasunaga et al., 2018)<a href="index.html#fn75"></a><a href="index.html#fn76"></a> and domain-adversarial losses (Ganin et al., 2016; Kim et al., 2017)<a href="index.html#fn77"></a><a href="index.html#fn78"></a> are useful forms of regularization that can equally make models more robust. Generative adversarial networks (GANs) are not yet too effective for natural language generation (Semeniuta et al., 2018)<a href="index.html#fn79"></a>, but are useful for instance when matching distributions (Conneau et al., 2018)<a href="index.html#fn27"></a>.</p>

<p><strong>Reinforcement learning</strong>   Reinforcement learning has been shown to be useful for tasks with a temporal dependency  such as selecting data during training (Fang et al., 2017; Wu et al., 2018)<a href="index.html#fn80"></a><a href="index.html#fn81"></a> and modelling dialogue (Liu et al., 2018)<a href="index.html#fn86"></a>. RL is also effective for directly optimizing a non-differentiable end metric such as ROUGE or BLEU instead of optimizing a surrogate loss such as cross-entropy in summarization (Paulus et al, 2018; Celikyilmaz et al., 2018)<a href="index.html#fn82"></a><a href="index.html#fn83"></a> and machine translation (Ranzato et al., 2016)<a href="index.html#fn84"></a>. Similarly, inverse reinforcement learning can be useful in settings where the reward is too complex to be specified such as visual storytelling (Wang et al., 2018)<a href="index.html#fn85"></a>. </p>

<ol class="footnotes-list">  
<li id="fn1" class="footnote-item">Kneser, R., &amp; Ney, H. (1995, May). Improved backing-off for m-gram language modeling. In icassp (Vol. 1, p. 181e4).</li>  
<li id="fn2" class="footnote-item">Kannan, A., Kurach, K., Ravi, S., Kaufmann, T., Tomkins, A., Miklos, B., ... &amp; Ramavajjala, V. (2016, August). Smart reply: Automated response suggestion for email. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 955-964). ACM.</li>  
<li id="fn3" class="footnote-item">Bengio, Y., Ducharme, R., &amp; Vincent, P. (2001). Proceedings of NIPS.</li>  
<li id="fn4" class="footnote-item">Mikolov, T., Karafiát, M., Burget, L., Černocký, J., &amp; Khudanpur, S. (2010). Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association.</li>  
<li id="fn5" class="footnote-item">Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</li>  
<li id="fn6" class="footnote-item">Melis, G., Dyer, C., &amp; Blunsom, P. (2018). On the State of the Art of Evaluation in Neural Language Models. In Proceedings of ICLR 2018.</li>  
<li id="fn7" class="footnote-item">Daniluk, M., Rocktäschel, T., Weibl, J., &amp; Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. In Proceedings of ICLR 2017.</li>  
<li id="fn8" class="footnote-item">Caruana, R. (1993). Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning.</li>  
<li id="fn9" class="footnote-item">Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. In Proceedings of the 25th International Conference on Machine Learning (pp. 160–167).</li>  
<li id="fn10" class="footnote-item">Caruana, R. (1998). Multitask Learning. Autonomous Agents and Multi-Agent Systems, 27(1), 95–133.</li>  
<li id="fn11" class="footnote-item">Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537. Retrieved from <a href="http://arxiv.org/abs/1103.0398">http://arxiv.org/abs/1103.0398</a>.</li>  
<li id="fn12" class="footnote-item">Ruder, S., Bingel, J., Augenstein, I., &amp; Søgaard, A. (2017). Learning what to share between loosely related tasks. ArXiv Preprint ArXiv:1705.08142. Retrieved from <a href="http://arxiv.org/abs/1705.08142">http://arxiv.org/abs/1705.08142</a></li>  
<li id="fn13" class="footnote-item">Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems.</li>  
<li id="fn14" class="footnote-item">Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013).</li>  
<li id="fn15" class="footnote-item">Arora, S., Li, Y., Liang, Y., Ma, T., &amp; Risteski, A. (2016). A Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4, 385–399.</li>  
<li id="fn16" class="footnote-item">Mimno, D., &amp; Thompson, L. (2017). The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2863–2868).</li>  
<li id="fn17" class="footnote-item">Antoniak, M., &amp; Mimno, D. (2018). Evaluating the Stability of Embedding-based Word Similarities. Transactions of the Association for Computational Linguistics, 6, 107–119.</li>  
<li id="fn18" class="footnote-item">Wendlandt, L., Kummerfeld, J. K., &amp; Mihalcea, R. (2018). Factors Influencing the Surprising Instability of Word Embeddings. In Proceedings of NAACL-HLT 2018.</li>  
<li id="fn19" class="footnote-item">Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from <a href="http://arxiv.org/abs/1408.5882">http://arxiv.org/abs/1408.5882</a></li>  
<li id="fn20" class="footnote-item">Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543.</li>  
<li id="fn21" class="footnote-item">Levy, O., &amp; Goldberg, Y. (2014). Neural Word Embedding as Implicit Matrix Factorization. Advances in Neural Information Processing Systems (NIPS), 2177–2185. Retrieved from <a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization">http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization</a></li>  
<li id="fn22" class="footnote-item">Levy, O., Goldberg, Y., &amp; Dagan, I. (2015). Improving Distributional Similarity with Lessons Learned from Word Embeddings. Transactions of the Association for Computational Linguistics, 3, 211–225. Retrieved from <a href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570">https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570</a></li>  
<li id="fn23" class="footnote-item">Le, Q. V., &amp; Mikolov, T. (2014). Distributed Representations of Sentences and Documents. International Conference on Machine Learning - ICML 2014, 32, 1188–1196. Retrieved from <a href="http://arxiv.org/abs/1405.4053">http://arxiv.org/abs/1405.4053</a></li>  
<li id="fn24" class="footnote-item">Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp; Fidler, S. (2015). Skip-Thought Vectors. In Proceedings of NIPS 2015. Retrieved from <a href="http://arxiv.org/abs/1506.06726">http://arxiv.org/abs/1506.06726</a></li>  
<li id="fn25" class="footnote-item">Grover, A., &amp; Leskovec, J. (2016, August). node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 855-864). ACM.</li>  
<li id="fn26" class="footnote-item">Asgari, E., &amp; Mofrad, M. R. (2015). Continuous distributed representation of biological sequences for deep proteomics and genomics. PloS one, 10(11), e0141287.</li>  
<li id="fn27" class="footnote-item">Conneau, A., Lample, G., Ranzato, M., Denoyer, L., &amp; Jégou, H. (2018). Word Translation Without Parallel Data. In Proceedings of ICLR 2018. Retrieved from <a href="http://arxiv.org/abs/1710.04087">http://arxiv.org/abs/1710.04087</a></li>  
<li id="fn28" class="footnote-item">Artetxe, M., Labaka, G., &amp; Agirre, E. (2018). A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of ACL 2018.</li>  
<li id="fn29" class="footnote-item">Søgaard, A., Ruder, S., &amp; Vulić, I. (2018). On the Limitations of Unsupervised Bilingual Dictionary Induction. In Proceedings of ACL 2018.</li>  
<li id="fn30" class="footnote-item">Ruder, S., Vulić, I., &amp; Søgaard, A. (2018). A Survey of Cross-lingual Word Embedding Models. To be published in Journal of Artificial Intelligence Research. Retrieved from <a href="http://arxiv.org/abs/1706.04902">http://arxiv.org/abs/1706.04902</a></li>  
<li id="fn31" class="footnote-item">Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2), 179-211.</li>  
<li id="fn32" class="footnote-item">Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</li>  
<li id="fn33" class="footnote-item">Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (pp. 655–665). Retrieved from <a href="http://arxiv.org/abs/1404.2188">http://arxiv.org/abs/1404.2188</a></li>  
<li id="fn34" class="footnote-item">Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 1746–1751. Retrieved from <a href="http://arxiv.org/abs/1408.5882">http://arxiv.org/abs/1408.5882</a></li>  
<li id="fn35" class="footnote-item">Kalchbrenner, N., Espeholt, L., Simonyan, K., Oord, A. van den, Graves, A., &amp; Kavukcuoglu, K. (2016). Neural Machine Translation in Linear Time. ArXiv Preprint ArXiv: Retrieved from <a href="http://arxiv.org/abs/1610.10099">http://arxiv.org/abs/1610.10099</a></li>  
<li id="fn36" class="footnote-item">Wang, J., Yu, L., Lai, K. R., &amp; Zhang, X. (2016). Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), 225–230.</li>  
<li id="fn37" class="footnote-item">Bradbury, J., Merity, S., Xiong, C., &amp; Socher, R. (2017). Quasi-Recurrent Neural Networks. In ICLR 2017. Retrieved from <a href="http://arxiv.org/abs/1611.01576">http://arxiv.org/abs/1611.01576</a></li>  
<li id="fn38" class="footnote-item">Socher, R., Perelygin, A., &amp; Wu, J. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–1642.</li>  
<li id="fn39" class="footnote-item">Tai, K. S., Socher, R., &amp; Manning, C. D. (2015). Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. Acl-2015, 1556–1566.</li>  
<li id="fn40" class="footnote-item">Levy, O., &amp; Goldberg, Y. (2014). Dependency-Based Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) (pp. 302–308). <a href="https://doi.org/10.3115/v1/P14-2050">https://doi.org/10.3115/v1/P14-2050</a></li>  
<li id="fn41" class="footnote-item">Dyer, C., Kuncoro, A., Ballesteros, M., &amp; Smith, N. A. (2016). Recurrent Neural Network Grammars. In NAACL. Retrieved from <a href="http://arxiv.org/abs/1602.07776">http://arxiv.org/abs/1602.07776</a></li>  
<li id="fn42" class="footnote-item">Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., &amp; Sima’an, K. (2017). Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</li>  
<li id="fn43" class="footnote-item">Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).</li>  
<li id="fn44" class="footnote-item">Lebret, R., Grangier, D., &amp; Auli, M. (2016). Generating Text from Structured Data with Application to the Biography Domain. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1603.07771">http://arxiv.org/abs/1603.07771</a></li>  
<li id="fn45" class="footnote-item">Loyola, P., Marrese-Taylor, E., &amp; Matsuo, Y. (2017). A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes. In ACL 2017. Retrieved from <a href="http://arxiv.org/abs/1704.04856">http://arxiv.org/abs/1704.04856</a></li>  
<li id="fn46" class="footnote-item">Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., &amp; Hinton, G. (2015). Grammar as a Foreign Language. Advances in Neural Information Processing Systems.</li>  
<li id="fn47" class="footnote-item">Gillick, D., Brunk, C., Vinyals, O., &amp; Subramanya, A. (2016). Multilingual Language Processing From Bytes. In NAACL (pp. 1296–1306). Retrieved from <a href="http://arxiv.org/abs/1512.00103">http://arxiv.org/abs/1512.00103</a></li>  
<li id="fn48" class="footnote-item">Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. ArXiv Preprint ArXiv:1609.08144.</li>  
<li id="fn49" class="footnote-item">Gehring, J., Auli, M., Grangier, D., Yarats, D., &amp; Dauphin, Y. N. (2017). Convolutional Sequence to Sequence Learning. ArXiv Preprint ArXiv:1705.03122. Retrieved from <a href="http://arxiv.org/abs/1705.03122">http://arxiv.org/abs/1705.03122</a></li>  
<li id="fn50" class="footnote-item">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems.</li>  
<li id="fn51" class="footnote-item">Chen, M. X., Foster, G., &amp; Parmar, N. (2018). The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. In Proceedings of ACL 2018.</li>  
<li id="fn52" class="footnote-item">Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR 2015.</li>  
<li id="fn53" class="footnote-item">Luong, M.-T., Pham, H., &amp; Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. In Proceedings of EMNLP 2015. Retrieved from <a href="http://arxiv.org/abs/1508.04025">http://arxiv.org/abs/1508.04025</a></li>  
<li id="fn54" class="footnote-item">Hermann, K. M., Kočiský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp; Blunsom, P. (2015). Teaching Machines to Read and Comprehend. Advances in Neural Information Processing Systems. Retrieved from <a href="http://arxiv.org/abs/1506.03340v1">http://arxiv.org/abs/1506.03340v1</a></li>  
<li id="fn55" class="footnote-item">Xu, K., Courville, A., Zemel, R. S., &amp; Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In Proceedings of ICML 2015.</li>  
<li id="fn56" class="footnote-item">Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems 29 (NIPS 2016). Retrieved from <a href="http://arxiv.org/abs/1606.04080">http://arxiv.org/abs/1606.04080</a></li>  
<li id="fn57" class="footnote-item">Graves, A., Wayne, G., &amp; Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.</li>  
<li id="fn58" class="footnote-item">Weston, J., Chopra, S., &amp; Bordes, A. (2015). Memory Networks. In Proceedings of ICLR 2015.</li>  
<li id="fn59" class="footnote-item">Sukhbaatar, S., Szlam, A., Weston, J., &amp; Fergus, R. (2015). End-To-End Memory Networks. In Proceedings of NIPS 2015. Retrieved from <a href="http://arxiv.org/abs/1503.08895">http://arxiv.org/abs/1503.08895</a></li>  
<li id="fn60" class="footnote-item">Kumar, A., Irsoy, O., Ondruska, P., Iyyer, M., Bradbury, J., Gulrajani, I., ... &amp; Socher, R. (2016, June). Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning (pp. 1378-1387).</li>  
<li id="fn61" class="footnote-item">Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., … Hassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory. Nature.</li>  
<li id="fn62" class="footnote-item">Henaff, M., Weston, J., Szlam, A., Bordes, A., &amp; LeCun, Y. (2017). Tracking the World State with Recurrent Entity Networks. In Proceedings of ICLR 2017.</li>  
<li id="fn63" class="footnote-item">Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems.</li>  
<li id="fn64" class="footnote-item">McCann, B., Bradbury, J., Xiong, C., &amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. In Advances in Neural Information Processing Systems.</li>  
<li id="fn65" class="footnote-item">Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</li>  
<li id="fn66" class="footnote-item">Subramanian, S., Trischler, A., Bengio, Y., &amp; Pal, C. J. (2018). Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Proceedings of ICLR 2018.</li>  
<li id="fn67" class="footnote-item">Dai, A. M., &amp; Le, Q. V. (2015). Semi-supervised Sequence Learning. Advances in Neural Information Processing Systems (NIPS ’15). Retrieved from <a href="http://arxiv.org/abs/1511.01432">http://arxiv.org/abs/1511.01432</a></li>  
<li id="fn68" class="footnote-item">Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of NAACL-HLT 2018.</li>  
<li id="fn69" class="footnote-item">Howard, J., &amp; Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of ACL 2018. Retrieved from <a href="http://arxiv.org/abs/1801.06146">http://arxiv.org/abs/1801.06146</a></li>  
<li id="fn70" class="footnote-item">Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., &amp; Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In NAACL-HLT 2016.</li>  
<li id="fn71" class="footnote-item">Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.</li>  
<li id="fn72" class="footnote-item">Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. (2016). Character-based Neural Machine Translation. In ICLR. Retrieved from <a href="http://arxiv.org/abs/1511.04586">http://arxiv.org/abs/1511.04586</a></li>  
<li id="fn73" class="footnote-item">Lee, J., Cho, K., &amp; Bengio, Y. (2017). Fully Character-Level Neural Machine Translation without Explicit Segmentation. In Transactions of the Association for Computational Linguistics.</li>  
<li id="fn74" class="footnote-item">Jia, R., &amp; Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</li>  
<li id="fn75" class="footnote-item">Miyato, T., Dai, A. M., &amp; Goodfellow, I. (2017). Adversarial Training Methods for Semi-supervised Text Classification. In Proceedings of ICLR 2017.</li>  
<li id="fn76" class="footnote-item">Yasunaga, M., Kasai, J., &amp; Radev, D. (2018). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from <a href="http://arxiv.org/abs/1711.04903">http://arxiv.org/abs/1711.04903</a></li>  
<li id="fn77" class="footnote-item">Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17.</li>  
<li id="fn78" class="footnote-item">Kim, Y., Stratos, K., &amp; Kim, D. (2017). Adversarial Adaptation of Synthetic or Stale Data. In Proceedings of ACL (pp. 1297–1307).</li>  
<li id="fn79" class="footnote-item">Semeniuta, S., Severyn, A., &amp; Gelly, S. (2018). On Accurate Evaluation of GANs for Language Generation. Retrieved from <a href="http://arxiv.org/abs/1806.04936">http://arxiv.org/abs/1806.04936</a></li>  
<li id="fn80" class="footnote-item"><p>Fang, M., Li, Y., &amp; Cohn, T. (2017). Learning how to Active Learn: A Deep Reinforcement Learning Approach. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="https://arxiv.org/pdf/1708.02383v1.pdf">https://arxiv.org/pdf/1708.02383v1.pdf</a></p>  
</li>  
<li id="fn81" class="footnote-item"><p>Wu, J., Li, L., &amp; Wang, W. Y. (2018). Reinforced Co-Training. In Proceedings of NAACL-HLT 2018.</p>  
</li>  
<li id="fn82" class="footnote-item"><p>Paulus, R., Xiong, C., &amp; Socher, R. (2018). A deep reinforced model for abstractive summarization. In Proceedings of ICLR 2018.</p></li>  
<li id="fn83" class="footnote-item"><p>Celikyilmaz, A., Bosselut, A., He, X., &amp; Choi, Y. (2018). Deep communicating agents for abstractive summarization. In Proceedings of NAACL-HLT 2018.</p></li>  
<li id="fn84" class="footnote-item"><p>Ranzato, M. A., Chopra, S., Auli, M., &amp; Zaremba, W. (2016). Sequence level training with recurrent neural networks. In Proceedings of ICLR 2016.</p></li>  
<li id="fn85" class="footnote-item"><p>Wang, X., Chen, W., Wang, Y.-F., &amp; Wang, W. Y. (2018). No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling. In Proceedings of ACL 2018. Retrieved from <a href="http://arxiv.org/abs/1804.09160">http://arxiv.org/abs/1804.09160</a></p></li>  
<li id="fn86" class="footnote-item">Liu, B., Tür, G., Hakkani-Tür, D., Shah, P., &amp; Heck, L. (2018). Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems. In Proceedings of NAACL-HLT 2018.</li>  
<li id="fn87" class="footnote-item">Kuncoro, A., Dyer, C., Hale, J., Yogatama, D., Clark, S., &amp; Blunsom, P. (2018). LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better. In Proceedings of ACL 2018 (pp. 1–11). Retrieved from <a href="http://aclweb.org/anthology/P18-1132">http://aclweb.org/anthology/P18-1132</a></li>  
<li id="fn88" class="footnote-item">Blevins, T., Levy, O., &amp; Zettlemoyer, L. (2018). Deep RNNs Encode Soft Hierarchical Syntax. In Proceedings of ACL 2018. Retrieved from <a href="http://arxiv.org/abs/1805.04218">http://arxiv.org/abs/1805.04218</a></li>  
<li id="fn89" class="footnote-item">Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., &amp; Bowman, S. R. (2018). GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.</li>  
<li id="fn90" class="footnote-item">McCann, B., Keskar, N. S., Xiong, C., &amp; Socher, R. (2018). The Natural Language Decathlon: Multitask Learning as Question Answering.</li>  
<li id="fn91" class="footnote-item">Lample, G., Denoyer, L., &amp; Ranzato, M. (2018). Unsupervised Machine Translation Using Monolingual Corpora Only. In Proceedings of ICLR 2018.</li>  
<li id="fn92" class="footnote-item">Artetxe, M., Labaka, G., Agirre, E., &amp; Cho, K. (2018). Unsupervised Neural Machine Translation. In Proceedings of ICLR 2018. Retrieved from <a href="http://arxiv.org/abs/1710.11041">http://arxiv.org/abs/1710.11041</a></li>  
<li id="fn93" class="footnote-item">Graves, A., Jaitly, N., &amp; Mohamed, A. R. (2013, December). Hybrid speech recognition with deep bidirectional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on (pp. 273-278). IEEE.</li>  
</ol>
    </div>

    <div class="post related">
        <a rel="prev" id="prev-btn" class="btn small square" href="../acl-2018-highlights/">← ACL 2018 Highlights: Understanding Representations and Evaluation in More Challenging Settings</a>

    </div>

    <footer class="post comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  (function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + window.disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</footer>

  </article>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  

        <footer>
  <span class="copyright">
    © 2018. All rights reserved. Built with <a href="https://ghost.org/" target="_blank">Ghost</a> and <a href="https://github.com/Kikobeats/uno-zen" target="_blank">Uno Zen</a> theme.
  </span>
</footer>
      </section>
    </main>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
    <script src="../assets/js/uno-zen.js?v=913ce5f5b3" type="text/javascript" charset="utf-8"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script>
  if (window.ga_id) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', window.ga_id, 'auto');
    ga('require', 'linkid', 'linkid.js');
    ga('send', 'pageview');
  }
</script>
  </body>
